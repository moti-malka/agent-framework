{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321ecba2",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">üëã Welcome to the Microsoft Agent Framework Demo</h1>\n",
    "\n",
    "<h2 align=\"center\">üöÄ Microsoft Agent Framework ‚Äî Support Email Copilot (Capstone Demo)</h2>\n",
    "\n",
    "<p align=\"center\">In this notebook, we‚Äôll build a futuristic multi-agent Support Email Copilot using the Microsoft Agent Framework (Python SDK).</p>\n",
    "\n",
    "![main.png](images/main.png)\n",
    "\n",
    "\n",
    "<h1 align=\"center\">Microsoft Agent Framework</h1>\n",
    "\n",
    "![Microsoft Agent Framework](images/maf.png)\n",
    "\n",
    "The **Microsoft Agent Framework** is a Python SDK for building AI agents and multi-agent workflows. It provides a unified interface for creating intelligent systems that can reason, take actions, and collaborate.\n",
    "\n",
    "**Key Capabilities:**\n",
    "- Single and multi-agent orchestration\n",
    "- Tool integration and function calling\n",
    "- Memory and context management\n",
    "- Workflow patterns (sequential, parallel, branching)\n",
    "- Built-in observability and middleware\n",
    "\n",
    "---\n",
    "\n",
    "## What is an Agent?\n",
    "\n",
    "![What is an Agent](images/what-is-agent.png)\n",
    "\n",
    "Unlike traditional LLM deployments that simply respond to prompts, agents follow the **ReAct pattern** (Reasoning + Acting):\n",
    "\n",
    "| Traditional LLM | Agent (ReAct) |\n",
    "|-----------------|---------------|\n",
    "| Input ‚Üí Output | Input ‚Üí Reason ‚Üí Act ‚Üí Observe ‚Üí Repeat |\n",
    "| Single response | Multi-step execution |\n",
    "| No tool access | Tool integration |\n",
    "| Stateless | Memory & context |\n",
    "\n",
    "Agents autonomously decide *what* to do, *which* tools to use, and *when* to stop.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflows & Multi-Agent Orchestration\n",
    "\n",
    "![Workflow Example](images/workflow-example.png)\n",
    "\n",
    "Complex tasks require coordination between multiple specialized agents. The Agent Framework provides workflow primitives:\n",
    "\n",
    "- **Sequential** ‚Äî Agents execute in order (A ‚Üí B ‚Üí C)\n",
    "- **Parallel (Fan-out/Fan-in)** ‚Äî Concurrent execution with result aggregation\n",
    "- **Branching** ‚Äî Conditional routing based on outputs\n",
    "- **Group Chat** ‚Äî Collaborative multi-agent discussions\n",
    "\n",
    "---\n",
    "\n",
    "## Demo Overview\n",
    "\n",
    "We'll build a **Support Email Copilot** that demonstrates core framework concepts:\n",
    "\n",
    "| Section | Concept |\n",
    "|---------|---------|\n",
    "| 1-2 | Agent basics & streaming |\n",
    "| 3-4 | Conversations & function tools |\n",
    "| 5-7 | Approvals, middleware, memory |\n",
    "| 8-10 | Workflows: sequential, branching, parallel |\n",
    "| 11-12 | Multi-agent collaboration & capstone |\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure subscription with Azure OpenAI access\n",
    "- Azure OpenAI resource with deployed model (e.g., `gpt-4o-mini`)\n",
    "- Azure CLI installed and authenticated (`az login`)\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d841bbb",
   "metadata": {},
   "source": [
    "# 0. Environment Setup\n",
    "\n",
    "## Create Virtual Environment\n",
    "\n",
    "Run the following in your terminal to set up the environment:\n",
    "\n",
    "```bash\n",
    "python3.10 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Or run the cell below to install dependencies directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the virtual environment (run once)\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def find_python():\n",
    "    \"\"\"Find a Python 3.10+ interpreter on the system.\"\"\"\n",
    "    # Check common Python commands in order of preference\n",
    "    candidates = [\n",
    "        \"python3.13\", \"python3.12\", \"python3.11\", \"python3.10\",\n",
    "        \"python3\", \"python\"\n",
    "    ]\n",
    "    \n",
    "    for cmd in candidates:\n",
    "        path = shutil.which(cmd)\n",
    "        if path:\n",
    "            # Verify version is 3.10+\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [path, \"-c\", \"import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')\"],\n",
    "                    capture_output=True, text=True\n",
    "                )\n",
    "                version = result.stdout.strip()\n",
    "                major, minor = map(int, version.split('.'))\n",
    "                if major >= 3 and minor >= 10:\n",
    "                    return path, version\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    raise RuntimeError(\"No Python 3.10+ found. Please install Python 3.10 or higher.\")\n",
    "\n",
    "# Find suitable Python\n",
    "python_path, python_version = find_python()\n",
    "print(f\"‚úÖ Found Python {python_version}: {python_path}\")\n",
    "\n",
    "# Create .venv\n",
    "subprocess.run([python_path, \"-m\", \"venv\", \".venv\"])\n",
    "\n",
    "# Install requirements with pre-release flag\n",
    "subprocess.run([\".venv/bin/pip\", \"install\", \"-r\", \"requirements.txt\", \"--pre\"])\n",
    "\n",
    "print(\"\\n‚úÖ Virtual environment created at .venv\")\n",
    "print(\"   Activate with: source .venv/bin/activate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e423b",
   "metadata": {},
   "source": [
    "## Initialize Chat Client\n",
    "\n",
    "Load environment variables and create the Azure OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d946629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import AzureCliCredential\n",
    "from agent_framework.azure import AzureOpenAIChatClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create ONE chat client - reused throughout the notebook\n",
    "chat_client = AzureOpenAIChatClient(credential=AzureCliCredential())\n",
    "\n",
    "print(\"‚úÖ Environment loaded and chat_client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10492593",
   "metadata": {},
   "source": [
    "## Data Models\n",
    "\n",
    "Pydantic models for structured input/output throughout the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# === Input Model ===\n",
    "class EmailInput(BaseModel):\n",
    "    \"\"\"Incoming support email.\"\"\"\n",
    "    sender: str = Field(description=\"Email sender address\")\n",
    "    subject: str = Field(description=\"Email subject line\")\n",
    "    body: str = Field(description=\"Email body content\")\n",
    "    customer_id: str | None = Field(default=None, description=\"Customer ID if known\")\n",
    "    ticket_id: str | None = Field(default=None, description=\"Related ticket ID if any\")\n",
    "\n",
    "# === Classification Model ===\n",
    "class ClassificationResult(BaseModel):\n",
    "    \"\"\"Result of email classification.\"\"\"\n",
    "    category: Literal[\"spam\", \"not_spam\", \"uncertain\"] = Field(description=\"Email category\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score 0-1\")\n",
    "    reason: str = Field(description=\"Brief explanation of classification\")\n",
    "\n",
    "# === Draft Response Model ===\n",
    "class DraftResponse(BaseModel):\n",
    "    \"\"\"Draft reply to customer email.\"\"\"\n",
    "    subject: str = Field(description=\"Reply subject line\")\n",
    "    body: str = Field(description=\"Reply body\")\n",
    "    tone: Literal[\"formal\", \"friendly\", \"apologetic\"] = Field(description=\"Tone used\")\n",
    "    needs_review: bool = Field(default=False, description=\"Flag if needs human review\")\n",
    "\n",
    "# === Final Response Model ===\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"Final approved response.\"\"\"\n",
    "    classification: ClassificationResult\n",
    "    draft: DraftResponse | None = Field(default=None, description=\"Draft if not spam\")\n",
    "    review_notes: str | None = Field(default=None, description=\"Reviewer comments\")\n",
    "    approved: bool = Field(default=False, description=\"Whether approved to send\")\n",
    "\n",
    "print(\"‚úÖ Shared models defined: EmailInput, ClassificationResult, DraftResponse, FinalResponse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca66c9",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Test emails used throughout the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d68e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LEGITIMATE EMAIL ===\n",
    "LEGIT_EMAIL = EmailInput(\n",
    "    sender=\"sarah.chen@acmecorp.com\",\n",
    "    subject=\"Order #12345 - Delivery Issue\",\n",
    "    body=\"\"\"Hi Support Team,\n",
    "\n",
    "I placed order #12345 last week and the tracking shows it was delivered, \n",
    "but I never received the package. I've checked with my neighbors and the building \n",
    "concierge, but no one has seen it.\n",
    "\n",
    "This is urgent as the items were needed for a client presentation on Friday.\n",
    "Can you please help me locate the package or arrange a replacement?\n",
    "\n",
    "Thank you,\n",
    "Sarah Chen\n",
    "Account: ACME-7891\n",
    "\"\"\",\n",
    "    customer_id=\"CUST-7891\",\n",
    "    ticket_id=\"TKT-2024-001\"\n",
    ")\n",
    "\n",
    "# === SPAM EMAIL ===\n",
    "SPAM_EMAIL = EmailInput(\n",
    "    sender=\"winner@prize-notifications.biz\",\n",
    "    subject=\"üéâ CONGRATULATIONS! You've WON $1,000,000!!!\",\n",
    "    body=\"\"\"URGENT NOTIFICATION!!!\n",
    "\n",
    "You have been selected as the WINNER of our international lottery!\n",
    "To claim your $1,000,000 prize, simply send your bank details and \n",
    "a processing fee of $500 to unlock your winnings.\n",
    "\n",
    "ACT NOW - This offer expires in 24 HOURS!!!\n",
    "\n",
    "Click here to claim: http://totally-legit-prize.com/claim\n",
    "\"\"\",\n",
    "    customer_id=None,\n",
    "    ticket_id=None\n",
    ")\n",
    "\n",
    "# === AMBIGUOUS EMAIL ===\n",
    "AMBIGUOUS_EMAIL = EmailInput(\n",
    "    sender=\"j.smith@unknown-domain.net\",\n",
    "    subject=\"Partnership Opportunity\",\n",
    "    body=\"\"\"Hello,\n",
    "\n",
    "I found your company online and I'm interested in discussing a potential \n",
    "business partnership. We have a new product line that might complement your services.\n",
    "\n",
    "Can we schedule a call this week?\n",
    "\n",
    "Best,\n",
    "J. Smith\n",
    "\"\"\",\n",
    "    customer_id=None,\n",
    "    ticket_id=None\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sample emails defined: LEGIT_EMAIL, SPAM_EMAIL, AMBIGUOUS_EMAIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26154c1d",
   "metadata": {},
   "source": [
    "# 1. Basic Agent\n",
    "\n",
    "![Agent Components](images/agent-components.png)\n",
    "\n",
    "Create a support agent using `chat_client.as_agent()` with instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the core Support Agent - we'll enhance this throughout the notebook\n",
    "support_agent = chat_client.as_agent(\n",
    "    name=\"SupportAgent\",\n",
    "    instructions=\"\"\"You are a helpful customer support agent for an e-commerce company.\n",
    "Your job is to:\n",
    "1. Understand customer issues from their emails\n",
    "2. Draft professional, empathetic responses\n",
    "3. Provide clear next steps when possible\n",
    "\n",
    "Always be polite, acknowledge the customer's frustration, and offer concrete solutions.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ support_agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f5e7f",
   "metadata": {},
   "source": [
    "## Run the Agent\n",
    "\n",
    "Execute the agent with `agent.run()`. Returns an `AgentResponse` with `.text` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0324c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the support agent on our legitimate email\n",
    "async def run_basic_agent():\n",
    "    prompt = f\"\"\"Please draft a response to this customer email:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "    result = await support_agent.run(prompt)\n",
    "    print(\"üìß Draft Response:\\n\")\n",
    "    print(result.text)\n",
    "\n",
    "asyncio.run(run_basic_agent())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9e31c",
   "metadata": {},
   "source": [
    "# 2. Streaming Responses\n",
    "\n",
    "Stream responses token-by-token using `agent.run_stream()` for real-time output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stream the response token by token using the SAME support_agent\n",
    "async def stream_support_response():\n",
    "    prompt = f\"\"\"Please draft a response to this customer email:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "    print(\"üìß Streaming Draft Response:\\n\")\n",
    "    async for update in support_agent.run_stream(prompt):\n",
    "        if update.text:\n",
    "            print(update.text, end=\"\", flush=True)\n",
    "    print()  # New line after streaming\n",
    "\n",
    "asyncio.run(stream_support_response())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54494e5c",
   "metadata": {},
   "source": [
    "# 3. Multi-Turn Conversations\n",
    "\n",
    "![Threads and Memory](images/threads-and-memory.png)\n",
    "\n",
    "Agents are stateless by default. Use **Threads** to maintain conversation context across turns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4aa94",
   "metadata": {},
   "source": [
    "## Using Threads\n",
    "\n",
    "Create a thread with `agent.get_new_thread()` and pass it to each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread for multi-turn conversation\n",
    "thread = support_agent.get_new_thread()\n",
    "\n",
    "# Turn 1: Summarize the customer issue\n",
    "print(\"Turn 1: Summarize the issue\")\n",
    "print(\"-\" * 50)\n",
    "result1 = await support_agent.run(\n",
    "    f\"Summarize the key issues in this email in 2-3 bullet points:\\n\\n{LEGIT_EMAIL.body}\", \n",
    "    thread=thread\n",
    ")\n",
    "print(result1.text)\n",
    "print()\n",
    "\n",
    "# Turn 2: Draft a response (agent remembers the summary from Turn 1)\n",
    "print(\"Turn 2: Draft response with professional tone\")\n",
    "print(\"-\" * 50)\n",
    "result2 = await support_agent.run(\n",
    "    \"Now draft a professional response addressing each of those issues. Use a formal but empathetic tone.\",\n",
    "    thread=thread\n",
    ")\n",
    "print(result2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d5c10",
   "metadata": {},
   "source": [
    "# 4. Function Tools\n",
    "\n",
    "Extend agent capabilities by registering Python functions as tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d70fa",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "Use the `@tool` decorator to expose functions to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963debc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import tool\n",
    "# Simulated database of customer SLAs\n",
    "CUSTOMER_SLAS = {\n",
    "    \"CUST-7891\": {\"tier\": \"Premium\", \"response_time\": \"4 hours\", \"replacement_policy\": \"Free expedited replacement\"},\n",
    "    \"CUST-1234\": {\"tier\": \"Standard\", \"response_time\": \"24 hours\", \"replacement_policy\": \"Standard replacement\"},\n",
    "}\n",
    "\n",
    "# Simulated ticket database\n",
    "TICKET_STATUSES = {\n",
    "    \"TKT-2024-001\": {\"status\": \"Open\", \"priority\": \"High\", \"assigned_to\": \"Support Team\", \"last_update\": \"2024-01-15\"},\n",
    "    \"TKT-2024-002\": {\"status\": \"Resolved\", \"priority\": \"Low\", \"assigned_to\": \"Bot\", \"last_update\": \"2024-01-10\"},\n",
    "}\n",
    "\n",
    "@tool(name=\"lookup_customer_sla\", description=\"Look up a customer's SLA tier and policies\")\n",
    "def lookup_customer_sla(\n",
    "    customer_id: Annotated[str, Field(description=\"The customer ID to look up (e.g., CUST-7891)\")]\n",
    ") -> str:\n",
    "    \"\"\"Look up customer SLA information.\"\"\"\n",
    "    if customer_id in CUSTOMER_SLAS:\n",
    "        sla = CUSTOMER_SLAS[customer_id]\n",
    "        return f\"Customer {customer_id}: {sla['tier']} tier, {sla['response_time']} response time, {sla['replacement_policy']}\"\n",
    "    return f\"Customer {customer_id} not found in system.\"\n",
    "\n",
    "@tool(name=\"get_incident_status\", description=\"Get the current status of a support ticket\")\n",
    "def get_incident_status(\n",
    "    ticket_id: Annotated[str, Field(description=\"The ticket ID to check (e.g., TKT-2024-001)\")]\n",
    ") -> str:\n",
    "    \"\"\"Get ticket status information.\"\"\"\n",
    "    if ticket_id in TICKET_STATUSES:\n",
    "        ticket = TICKET_STATUSES[ticket_id]\n",
    "        return f\"Ticket {ticket_id}: Status={ticket['status']}, Priority={ticket['priority']}, Assigned to={ticket['assigned_to']}, Last update={ticket['last_update']}\"\n",
    "    return f\"Ticket {ticket_id} not found in system.\"\n",
    "\n",
    "print(\"‚úÖ Support tools defined: lookup_customer_sla, get_incident_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ee9eb",
   "metadata": {},
   "source": [
    "## Attach Tools to Agent\n",
    "\n",
    "Pass tools when creating the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create support agent with tools\n",
    "support_agent_with_tools = chat_client.as_agent(\n",
    "    name=\"SupportAgentWithTools\",\n",
    "    instructions=\"\"\"You are a customer support agent with access to internal systems.\n",
    "When handling emails:\n",
    "1. Look up the customer's SLA tier to understand their service level\n",
    "2. Check ticket status if a ticket ID is mentioned\n",
    "3. Use this information to provide appropriate responses and set expectations\n",
    "\n",
    "Always be empathetic and use the customer's SLA tier to guide your response (e.g., Premium customers get expedited service).\"\"\",\n",
    "    tools=[lookup_customer_sla, get_incident_status]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ support_agent_with_tools created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461f75e",
   "metadata": {},
   "source": [
    "## Execute with Tools\n",
    "\n",
    "The agent autonomously decides when to invoke tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cff641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the legitimate email that has customer_id and ticket_id\n",
    "prompt = f\"\"\"Handle this customer support email. Look up their SLA and ticket status first:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "Customer ID: {LEGIT_EMAIL.customer_id}\n",
    "Ticket ID: {LEGIT_EMAIL.ticket_id}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "\n",
    "result = await support_agent_with_tools.run(prompt)\n",
    "print(\"üìß Response (with tool lookups):\\n\")\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530d0d1",
   "metadata": {},
   "source": [
    "# 5. Human-in-the-Loop Approval\n",
    "\n",
    "Require human confirmation before executing sensitive actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479786af",
   "metadata": {},
   "source": [
    "## Approval-Required Tool\n",
    "\n",
    "Set `approval_mode=\"always_require\"` on sensitive tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cbdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import ChatMessage, Content, Role\n",
    "\n",
    "# Tool that requires human approval before sending\n",
    "@tool(approval_mode=\"always_require\", name=\"send_email_reply\", description=\"Send an email reply to the customer. Requires human approval.\")\n",
    "def send_email_reply(\n",
    "    to: Annotated[str, Field(description=\"Recipient email address\")],\n",
    "    subject: Annotated[str, Field(description=\"Email subject\")],\n",
    "    body: Annotated[str, Field(description=\"Email body content\")]\n",
    ") -> str:\n",
    "    \"\"\"Send an email reply to the customer. Requires human approval.\"\"\"\n",
    "    # In production, this would actually send the email\n",
    "    return f\"‚úÖ Email sent to {to} with subject '{subject}'\"\n",
    "\n",
    "# Create agent with the approval-required tool\n",
    "approval_agent = chat_client.as_agent(\n",
    "    name=\"ApprovalSupportAgent\",\n",
    "    instructions=\"\"\"You are a customer support agent. After drafting a response, \n",
    "use the send_email_reply tool to send it. This will require human approval.\"\"\",\n",
    "    tools=[lookup_customer_sla, get_incident_status, send_email_reply]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ approval_agent created with send_email_reply tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de90248",
   "metadata": {},
   "source": [
    "## Check for Pending Approvals\n",
    "\n",
    "Approval-required calls return `user_input_requests` instead of executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6063ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the agent to handle and send a response\n",
    "prompt = f\"\"\"Handle this email and IMMEDIATELY use the send_email_reply tool to send a response. \n",
    "Do not ask for permission - just use the tool directly.\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "Customer ID: {LEGIT_EMAIL.customer_id}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "\n",
    "result = await approval_agent.run(prompt)\n",
    "\n",
    "# Check if approval is needed\n",
    "if result.user_input_requests:\n",
    "    print(\"üîí APPROVAL REQUIRED!\")\n",
    "    for user_input_needed in result.user_input_requests:\n",
    "        print(f\"  Function: {user_input_needed.function_call.name}\")\n",
    "        print(f\"  Arguments: {user_input_needed.function_call.arguments}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No approval requested - agent didn't call the tool\")\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e80ab2",
   "metadata": {},
   "source": [
    "## Grant Approval\n",
    "\n",
    "Respond with `to_function_approval_response(True/False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa70478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Handling Approval ---\\n\")\n",
    "\n",
    "# Provide approval and continue the conversation\n",
    "if result.user_input_requests:\n",
    "    user_input_needed = result.user_input_requests[0]\n",
    "    \n",
    "    # Simulate human approval (in production, this would be interactive)\n",
    "    user_approval = True\n",
    "    print(f\"‚úÖ Human approved: {user_approval}\\n\")\n",
    "    \n",
    "    # Create approval response message\n",
    "    approval_message = ChatMessage(\n",
    "        role=Role.USER,\n",
    "        contents=[user_input_needed.to_function_approval_response(user_approval)]\n",
    "    )\n",
    "    \n",
    "    # Continue with approval\n",
    "    final_result = await approval_agent.run([\n",
    "        prompt,\n",
    "        ChatMessage(role=Role.ASSISTANT, contents=[user_input_needed]),\n",
    "        approval_message\n",
    "    ])\n",
    "    print(f\"üìä Final Result:\\n{final_result.text}\")\n",
    "else:\n",
    "    print(\"‚ùå No approval was requested in the previous cell.\")\n",
    "    print(\"   The agent needs to call the send_email_reply tool to trigger approval.\")\n",
    "    print(\"   Re-run the previous cell to try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbefd9d",
   "metadata": {},
   "source": [
    "# 6. Middleware\n",
    "\n",
    "Intercept agent execution for logging, metrics, and observability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8829240",
   "metadata": {},
   "source": [
    "## Define Middleware\n",
    "\n",
    "Middleware wraps execution with `context` and `next` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae080f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Awaitable\n",
    "from agent_framework import AgentRunContext, FunctionInvocationContext\n",
    "import time\n",
    "\n",
    "async def logging_agent_middleware(\n",
    "    context: AgentRunContext,\n",
    "    next: Callable[[AgentRunContext], Awaitable[None]],\n",
    ") -> None:\n",
    "    \"\"\"Log agent execution with timing.\"\"\"\n",
    "    print(f\"üöÄ Agent starting... ({len(context.messages)} message(s))\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    await next(context)  # Continue to agent execution\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Agent finished in {elapsed:.2f}s\")\n",
    "\n",
    "async def logging_function_middleware(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: Callable[[FunctionInvocationContext], Awaitable[None]],\n",
    ") -> None:\n",
    "    \"\"\"Log function tool calls.\"\"\"\n",
    "    print(f\"  üìû Calling: {context.function.name}({context.arguments})\")\n",
    "    \n",
    "    await next(context)\n",
    "    \n",
    "    print(f\"  üì§ Result: {context.result[:100]}...\" if len(str(context.result)) > 100 else f\"  üì§ Result: {context.result}\")\n",
    "\n",
    "print(\"‚úÖ Middleware defined: logging_agent_middleware, logging_function_middleware\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e9a62",
   "metadata": {},
   "source": [
    "## Attach Middleware\n",
    "\n",
    "Pass middleware list when creating the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with middleware for logging\n",
    "middleware_agent = chat_client.as_agent(\n",
    "    name=\"LoggingSupportAgent\",\n",
    "    instructions=\"You are a support agent. Look up customer information when handling requests.\",\n",
    "    tools=[lookup_customer_sla, get_incident_status],\n",
    "    middleware=[logging_agent_middleware, logging_function_middleware]\n",
    ")\n",
    "\n",
    "# Test - you'll see logs for agent and function calls\n",
    "prompt = f\"Check the SLA for customer {LEGIT_EMAIL.customer_id} and ticket status for {LEGIT_EMAIL.ticket_id}\"\n",
    "result = await middleware_agent.run(prompt)\n",
    "print(f\"\\nüí¨ Response: {result.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffbde4",
   "metadata": {},
   "source": [
    "# 7. Agent Memory\n",
    "\n",
    "Persist context across calls using a `ContextProvider`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ced48",
   "metadata": {},
   "source": [
    "## Preferences Model\n",
    "\n",
    "Define what to remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3468ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportPreferences(BaseModel):\n",
    "    \"\"\"User preferences for support interactions.\"\"\"\n",
    "    name: str | None = None\n",
    "    preferred_language: Literal[\"English\", \"Hebrew\", \"Spanish\"] = \"English\"\n",
    "    preferred_tone: Literal[\"formal\", \"friendly\", \"brief\"] = \"formal\"\n",
    "\n",
    "print(\"‚úÖ SupportPreferences model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdd4af",
   "metadata": {},
   "source": [
    "## Implement ContextProvider\n",
    "\n",
    "Two methods: `invoking` (inject context before calls) and `invoked` (extract state after calls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082eefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import MutableSequence, Sequence\n",
    "from typing import Any\n",
    "\n",
    "from agent_framework import ContextProvider, Context, ChatAgent, ChatOptions\n",
    "\n",
    "\n",
    "class SupportMemory(ContextProvider):\n",
    "    \"\"\"Memory that tracks user preferences for support interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self, chat_client, preferences: SupportPreferences | None = None, **kwargs: Any):\n",
    "        \"\"\"Create the memory.\n",
    "        \n",
    "        Args:\n",
    "            chat_client: The chat client to use for extracting structured data\n",
    "            preferences: Optional initial preferences\n",
    "            **kwargs: Additional keyword arguments for deserialization\n",
    "        \"\"\"\n",
    "        self._chat_client = chat_client\n",
    "        if preferences:\n",
    "            self.preferences = preferences\n",
    "        elif kwargs:\n",
    "            self.preferences = SupportPreferences.model_validate(kwargs)\n",
    "        else:\n",
    "            self.preferences = SupportPreferences()\n",
    "    \n",
    "    async def invoked(\n",
    "        self,\n",
    "        request_messages: ChatMessage | Sequence[ChatMessage],\n",
    "        response_messages: ChatMessage | Sequence[ChatMessage] | None = None,\n",
    "        invoke_exception: Exception | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Extract preferences from user messages after each call.\"\"\"\n",
    "        # Ensure request_messages is a list\n",
    "        messages_list = [request_messages] if isinstance(request_messages, ChatMessage) else list(request_messages)\n",
    "        \n",
    "        # Check if we have user messages\n",
    "        user_messages = [msg for msg in messages_list if msg.role.value == \"user\"]\n",
    "        \n",
    "        if user_messages:\n",
    "            try:\n",
    "                # Use the chat client to extract structured information\n",
    "                # NOTE: Use `options=` not `chat_options=`\n",
    "                result = await self._chat_client.get_response(\n",
    "                    messages=messages_list,\n",
    "                    options=ChatOptions(\n",
    "                        instructions=(\n",
    "                            \"Extract the user's name, preferred tone (formal/friendly/brief), \"\n",
    "                            \"and preferred language (English/Hebrew/Spanish) from the messages if present. \"\n",
    "                            \"If not present, return None for that field.\"\n",
    "                        ),\n",
    "                        response_format=SupportPreferences,\n",
    "                    ),\n",
    "                )\n",
    "                \n",
    "                # result.value should now be a SupportPreferences instance\n",
    "                extracted = result.value\n",
    "                \n",
    "                # Update preferences with extracted data\n",
    "                if extracted and isinstance(extracted, SupportPreferences):\n",
    "                    if self.preferences.name is None and extracted.name:\n",
    "                        self.preferences.name = extracted.name\n",
    "                        print(f\"   üß† Memory updated: name = {extracted.name}\")\n",
    "                    \n",
    "                    if extracted.preferred_tone != \"formal\":  # formal is default\n",
    "                        self.preferences.preferred_tone = extracted.preferred_tone\n",
    "                        print(f\"   üß† Memory updated: tone = {extracted.preferred_tone}\")\n",
    "                    \n",
    "                    if extracted.preferred_language != \"English\":  # English is default\n",
    "                        self.preferences.preferred_language = extracted.preferred_language\n",
    "                        print(f\"   üß† Memory updated: language = {extracted.preferred_language}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Failed to extract preferences: {e}\")\n",
    "    \n",
    "    async def invoking(self, messages: ChatMessage | MutableSequence[ChatMessage], **kwargs: Any) -> Context:\n",
    "        \"\"\"Provide preference context before each agent call.\"\"\"\n",
    "        instructions: list[str] = []\n",
    "        \n",
    "        if self.preferences.name:\n",
    "            instructions.append(f\"The user's name is {self.preferences.name}. Address them by name.\")\n",
    "        \n",
    "        instructions.append(f\"Respond in {self.preferences.preferred_language}.\")\n",
    "        instructions.append(f\"Use a {self.preferences.preferred_tone} tone.\")\n",
    "        \n",
    "        return Context(instructions=\" \".join(instructions))\n",
    "    \n",
    "    def serialize(self) -> str:\n",
    "        \"\"\"Serialize for persistence.\"\"\"\n",
    "        return self.preferences.model_dump_json()\n",
    "\n",
    "print(\"‚úÖ SupportMemory ContextProvider defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d6fd8",
   "metadata": {},
   "source": [
    "## Test Memory\n",
    "\n",
    "The agent automatically extracts and applies preferences across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the memory provider using the existing chat_client\n",
    "support_memory = SupportMemory(chat_client)\n",
    "\n",
    "# Create the agent with memory\n",
    "memory_agent = ChatAgent(\n",
    "    name=\"MemorySupportAgent\",\n",
    "    instructions=\"You are a friendly support agent. Adapt your responses based on user preferences.\",\n",
    "    chat_client=chat_client,\n",
    "    context_provider=support_memory,\n",
    ")\n",
    "\n",
    "# Turn 1: User introduces themselves\n",
    "print(\"Turn 1: User introduction\")\n",
    "print(\"-\" * 50)\n",
    "result1 = await memory_agent.run(\"Hi, my name is David\")\n",
    "print(f\"Agent: {result1.text}\\n\")\n",
    "\n",
    "# Turn 2: User sets preference\n",
    "print(\"Turn 2: Setting preference\")\n",
    "print(\"-\" * 50)\n",
    "result2 = await memory_agent.run(\"Please keep responses brief and casual\")\n",
    "print(f\"Agent: {result2.text}\\n\")\n",
    "\n",
    "# Turn 3: Ask a question - memory should apply name and brief tone\n",
    "print(\"Turn 3: Question with preferences applied\")\n",
    "print(\"-\" * 50)\n",
    "result3 = await memory_agent.run(\"What's your return policy?\")\n",
    "print(f\"Agent: {result3.text}\\n\")\n",
    "\n",
    "# Check memory state - access the original support_memory object directly\n",
    "print(\"üß† Memory State (tracked by ContextProvider):\")\n",
    "print(f\"   Name: {support_memory.preferences.name}\")\n",
    "print(f\"   Language: {support_memory.preferences.preferred_language}\")\n",
    "print(f\"   Tone: {support_memory.preferences.preferred_tone}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8a13c",
   "metadata": {},
   "source": [
    "## Workflows Overview\n",
    "\n",
    "**Workflows** enable orchestrating multiple agents, humans, and external systems in a predefined execution graph.\n",
    "\n",
    "### Agent vs. Workflow\n",
    "\n",
    "| AI Agent | Workflow |\n",
    "|----------|----------|\n",
    "| Single reasoning loop | Orchestrates multiple components |\n",
    "| Dynamic tool selection | Predefined execution paths |\n",
    "| Best for: focused tasks | Best for: multi-step processes |\n",
    "\n",
    "### When to Use Workflows\n",
    "\n",
    "| Pattern | Use Case |\n",
    "|---------|----------|\n",
    "| **Sequential** | Steps must run in order (classify ‚Üí draft ‚Üí review) |\n",
    "| **Branching** | Different paths based on conditions (spam vs. legitimate) |\n",
    "| **Parallel (Fan-out/Fan-in)** | Independent tasks that can run concurrently |\n",
    "| **Group Chat** | Iterative refinement with multiple reviewers |\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Executor** | Unit of work ‚Äî agent or custom logic |\n",
    "| **Edge** | Connection between executors with optional conditions |\n",
    "| **WorkflowBuilder** | Constructs the execution graph |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429fcad",
   "metadata": {},
   "source": [
    "# 8. Sequential Workflows\n",
    "\n",
    "![Sequential Workflow](images/sequential-workflow.png)\n",
    "\n",
    "Chain multiple agents/executors in sequence: Classify ‚Üí Draft ‚Üí Review.\n",
    "\n",
    "**When to Use:**\n",
    "- Tasks with clear, ordered steps (e.g., parse ‚Üí validate ‚Üí transform)\n",
    "- When each step's output is the next step's input\n",
    "- Processing pipelines where order matters\n",
    "\n",
    "**When NOT to Use:**\n",
    "- Steps can run independently (use Concurrent instead)\n",
    "- Dynamic routing needed (use Branching instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6274db5",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Executor** | Unit of work (`@executor` or class with `@handler`) |\n",
    "| **WorkflowBuilder** | Connects executors with `add_edge()` |\n",
    "| `ctx.send_message()` | Pass data to next executor |\n",
    "| `ctx.yield_output()` | Return final result |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80541f",
   "metadata": {},
   "source": [
    "## Define Executors\n",
    "\n",
    "Create agent executors for classification, writing, and review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Never\n",
    "from agent_framework import (\n",
    "    WorkflowBuilder, WorkflowContext, WorkflowOutputEvent,\n",
    "    Executor, executor, handler, AgentExecutor, AgentExecutorRequest, AgentExecutorResponse\n",
    ")\n",
    "\n",
    "# === CLASSIFIER AGENT ===\n",
    "classifier_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"Classifier\",\n",
    "        instructions=\"\"\"Classify incoming emails. Return JSON with:\n",
    "- category: \"spam\", \"not_spam\", or \"uncertain\"\n",
    "- confidence: float 0-1\n",
    "- reason: brief explanation\"\"\",\n",
    "        response_format=ClassificationResult,\n",
    "    ),\n",
    "    id=\"classifier\",\n",
    ")\n",
    "\n",
    "# === DRAFT WRITER AGENT ===\n",
    "writer_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"DraftWriter\",\n",
    "        instructions=\"\"\"Draft professional support responses. Return JSON with:\n",
    "- subject: reply subject line\n",
    "- body: reply body\n",
    "- tone: \"formal\", \"friendly\", or \"apologetic\"\n",
    "- needs_review: true if sensitive or complex\"\"\",\n",
    "        response_format=DraftResponse,\n",
    "    ),\n",
    "    id=\"writer\",\n",
    ")\n",
    "\n",
    "# === REVIEWER AGENT ===\n",
    "reviewer_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"Reviewer\",\n",
    "        instructions=\"\"\"Review draft responses for quality. Check:\n",
    "- Professionalism and tone\n",
    "- Accuracy of information\n",
    "- Completeness\n",
    "Return approval decision with notes.\"\"\",\n",
    "    ),\n",
    "    id=\"reviewer\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Workflow agents defined: classifier, writer, reviewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f78d8",
   "metadata": {},
   "source": [
    "## Build & Run\n",
    "\n",
    "Connect executors with `add_edge()` and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b511c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequential workflow\n",
    "sequential_support_workflow = (\n",
    "    WorkflowBuilder()\n",
    "    .set_start_executor(classifier_agent)\n",
    "    .add_edge(classifier_agent, writer_agent)\n",
    "    .add_edge(writer_agent, reviewer_agent)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Run with legitimate email\n",
    "async def run_sequential_workflow():\n",
    "    email_prompt = f\"\"\"Process this support email:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "Customer ID: {LEGIT_EMAIL.customer_id}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"üìß Processing email through workflow: Classify ‚Üí Draft ‚Üí Review\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    request = AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=email_prompt)],\n",
    "        should_respond=True\n",
    "    )\n",
    "    \n",
    "    from agent_framework._workflows._events import ExecutorCompletedEvent\n",
    "    \n",
    "    async for event in sequential_support_workflow.run_stream(request):\n",
    "        if isinstance(event, ExecutorCompletedEvent) and event.data:\n",
    "            data = event.data[0] if isinstance(event.data, list) else event.data\n",
    "            if hasattr(data, 'agent_response'):\n",
    "                print(f\"\\n‚úÖ [{event.executor_id}]:\")\n",
    "                print(f\"   {data.agent_response.text[:300]}...\")\n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            print(f\"\\nüéØ FINAL OUTPUT:\")\n",
    "            if isinstance(event.data, list) and event.data:\n",
    "                final = event.data[0]\n",
    "                if hasattr(final, 'agent_response'):\n",
    "                    print(final.agent_response.text)\n",
    "\n",
    "await run_sequential_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37a59d",
   "metadata": {},
   "source": [
    "# 9. Branching Logic\n",
    "\n",
    "Route execution based on conditions: Spam ‚Üí Block, NotSpam ‚Üí Draft, Uncertain ‚Üí Review.\n",
    "\n",
    "**When to Use:**\n",
    "- Different paths based on classification or conditions\n",
    "- Error handling with fallback routes\n",
    "- Multi-way routing (switch-case patterns)\n",
    "\n",
    "**When NOT to Use:**\n",
    "- All items follow the same path (use Sequential)\n",
    "- Need parallel execution of branches (use Fan-Out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec9cf6",
   "metadata": {},
   "source": [
    "## Routing Patterns\n",
    "\n",
    "| Pattern | Use Case |\n",
    "|---------|----------|\n",
    "| **Conditional Edge** | Binary if/else |\n",
    "| **Switch-Case** | Multi-way routing |\n",
    "| **Multi-Selection** | Dynamic fan-out |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60db70",
   "metadata": {},
   "source": [
    "## Define Branch Handlers\n",
    "\n",
    "Create handlers for each classification outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38082206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from uuid import uuid4\n",
    "from agent_framework import Case, Default\n",
    "\n",
    "# Internal payload for routing\n",
    "@dataclass\n",
    "class ClassifiedEmail:\n",
    "    email_id: str\n",
    "    category: str  # spam, not_spam, uncertain\n",
    "    confidence: float\n",
    "    reason: str\n",
    "    original_content: str\n",
    "\n",
    "# Shared state keys\n",
    "EMAIL_KEY = \"current_email\"\n",
    "\n",
    "# Helper to extract JSON from markdown code blocks\n",
    "def extract_json(text: str) -> str:\n",
    "    \"\"\"Extract JSON from text, stripping markdown code blocks if present.\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)```', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Transform classification result to routable payload\n",
    "@executor(id=\"extract_classification\")\n",
    "async def extract_classification(response: Any, ctx: WorkflowContext[ClassifiedEmail]) -> None:\n",
    "    \"\"\"Extract classification from agent response for routing.\"\"\"\n",
    "    if isinstance(response, list):\n",
    "        response = response[0]\n",
    "    \n",
    "    # Extract JSON (handles markdown code blocks)\n",
    "    json_text = extract_json(response.agent_response.text)\n",
    "    classification = ClassificationResult.model_validate_json(json_text)\n",
    "    \n",
    "    # Get original email from shared state\n",
    "    original_content = await ctx.get_shared_state(EMAIL_KEY) or \"Unknown\"\n",
    "    \n",
    "    payload = ClassifiedEmail(\n",
    "        email_id=str(uuid4()),\n",
    "        category=classification.category,\n",
    "        confidence=classification.confidence,\n",
    "        reason=classification.reason,\n",
    "        original_content=original_content\n",
    "    )\n",
    "    await ctx.send_message(payload)\n",
    "\n",
    "# Route conditions\n",
    "def is_spam(message: Any) -> bool:\n",
    "    return isinstance(message, ClassifiedEmail) and message.category == \"spam\"\n",
    "\n",
    "def is_not_spam(message: Any) -> bool:\n",
    "    return isinstance(message, ClassifiedEmail) and message.category == \"not_spam\"\n",
    "\n",
    "def is_uncertain(message: Any) -> bool:\n",
    "    return isinstance(message, ClassifiedEmail) and message.category == \"uncertain\"\n",
    "\n",
    "# Terminal handlers\n",
    "@executor(id=\"handle_spam\")\n",
    "async def handle_spam_terminal(email: ClassifiedEmail, ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Handle spam: block and log.\"\"\"\n",
    "    await ctx.yield_output(f\"üö´ SPAM BLOCKED: {email.reason} (confidence: {email.confidence:.0%})\")\n",
    "\n",
    "@executor(id=\"handle_not_spam\")\n",
    "async def handle_not_spam_continue(email: ClassifiedEmail, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Handle not_spam: forward to writer.\"\"\"\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Draft a response to: {email.original_content}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "@executor(id=\"finalize_draft\")\n",
    "async def finalize_draft(response: Any, ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Output the final draft.\"\"\"\n",
    "    if isinstance(response, list):\n",
    "        response = response[0]\n",
    "    # Extract JSON (handles markdown code blocks)\n",
    "    json_text = extract_json(response.agent_response.text)\n",
    "    draft = DraftResponse.model_validate_json(json_text)\n",
    "    await ctx.yield_output(f\"‚úâÔ∏è DRAFT READY:\\nSubject: {draft.subject}\\n\\n{draft.body}\")\n",
    "\n",
    "@executor(id=\"handle_uncertain\")\n",
    "async def handle_uncertain_terminal(email: ClassifiedEmail, ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Handle uncertain: flag for human review.\"\"\"\n",
    "    await ctx.yield_output(f\"‚ö†Ô∏è NEEDS HUMAN REVIEW: {email.reason} (confidence: {email.confidence:.0%})\\n\\nOriginal: {email.original_content[:200]}...\")\n",
    "\n",
    "print(\"‚úÖ Branching executors defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f06000",
   "metadata": {},
   "source": [
    "## Build Switch-Case Workflow\n",
    "\n",
    "Route based on classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store email and start classification\n",
    "@executor(id=\"start_classification\")\n",
    "async def start_classification(email_text: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Store email and send for classification.\"\"\"\n",
    "    await ctx.set_shared_state(EMAIL_KEY, email_text)\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Classify this email:\\n\\n{email_text}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Build branching workflow\n",
    "branching_workflow = (\n",
    "    WorkflowBuilder()\n",
    "    .set_start_executor(start_classification)\n",
    "    .add_edge(start_classification, classifier_agent)\n",
    "    .add_edge(classifier_agent, extract_classification)\n",
    "    # Switch-case routing\n",
    "    .add_switch_case_edge_group(\n",
    "        extract_classification,\n",
    "        [\n",
    "            Case(condition=is_spam, target=handle_spam_terminal),\n",
    "            Case(condition=is_not_spam, target=handle_not_spam_continue),\n",
    "            Default(target=handle_uncertain_terminal),  # Catches uncertain + unexpected\n",
    "        ],\n",
    "    )\n",
    "    # Continue not_spam path to draft\n",
    "    .add_edge(handle_not_spam_continue, writer_agent)\n",
    "    .add_edge(writer_agent, finalize_draft)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Branching workflow built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6f304",
   "metadata": {},
   "source": [
    "## Test Branching\n",
    "\n",
    "Run all three email types through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59110839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all three paths\n",
    "async def test_branching():\n",
    "    test_cases = [\n",
    "        (\"LEGITIMATE\", LEGIT_EMAIL),\n",
    "        (\"SPAM\", SPAM_EMAIL),\n",
    "        (\"AMBIGUOUS\", AMBIGUOUS_EMAIL),\n",
    "    ]\n",
    "    \n",
    "    for label, email in test_cases:\n",
    "        print(f\"\\nüìß Testing {label} email...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        email_text = f\"From: {email.sender}\\nSubject: {email.subject}\\n\\n{email.body}\"\n",
    "        \n",
    "        async for event in branching_workflow.run_stream(email_text):\n",
    "            if isinstance(event, WorkflowOutputEvent):\n",
    "                print(event.data)\n",
    "\n",
    "await test_branching()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecb767",
   "metadata": {},
   "source": [
    "# 10. Fan-Out / Fan-In\n",
    "\n",
    "![Concurrent Workflow](images/concurrent-workflow.png)\n",
    "\n",
    "Process multiple paths in parallel and aggregate results.\n",
    "\n",
    "**When to Use:**\n",
    "- Independent tasks that can run concurrently\n",
    "- Aggregating results from multiple sources\n",
    "- Performance optimization through parallelization\n",
    "\n",
    "**When NOT to Use:**\n",
    "- Tasks have dependencies on each other\n",
    "- Order of execution matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863e4e6",
   "metadata": {},
   "source": [
    "## Define Parallel Paths\n",
    "\n",
    "For long emails: respond AND summarize concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary model\n",
    "class EmailSummary(BaseModel):\n",
    "    \"\"\"Concise email summary.\"\"\"\n",
    "    key_points: list[str] = Field(description=\"Main points from the email\")\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Urgency level\")\n",
    "    action_required: str = Field(description=\"Primary action needed\")\n",
    "\n",
    "# Summarizer agent\n",
    "summarizer_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"Summarizer\",\n",
    "        instructions=\"\"\"Summarize emails concisely. Return JSON with:\n",
    "- key_points: list of main points\n",
    "- urgency: low/medium/high\n",
    "- action_required: primary action needed\"\"\",\n",
    "        response_format=EmailSummary,\n",
    "    ),\n",
    "    id=\"summarizer\",\n",
    ")\n",
    "\n",
    "# Threshold for \"long\" emails\n",
    "LONG_EMAIL_THRESHOLD = 200  # characters\n",
    "\n",
    "@dataclass\n",
    "class EnrichedEmail:\n",
    "    \"\"\"Email with metadata for routing.\"\"\"\n",
    "    email_id: str\n",
    "    content: str\n",
    "    is_long: bool\n",
    "    category: str\n",
    "\n",
    "# Selection function for multi-selection routing\n",
    "def select_parallel_paths(email: EnrichedEmail, target_ids: list[str]) -> list[str]:\n",
    "    \"\"\"Select paths based on email length.\"\"\"\n",
    "    # target_ids order: [respond_path, summarize_path]\n",
    "    respond_id, summarize_id = target_ids\n",
    "    \n",
    "    if email.is_long:\n",
    "        return [respond_id, summarize_id]  # Both paths in parallel\n",
    "    else:\n",
    "        return [respond_id]  # Only respond for short emails\n",
    "\n",
    "# Executors for parallel paths\n",
    "@executor(id=\"prepare_parallel\")\n",
    "async def prepare_parallel(classified: ClassifiedEmail, ctx: WorkflowContext[EnrichedEmail]) -> None:\n",
    "    \"\"\"Prepare email for parallel processing.\"\"\"\n",
    "    enriched = EnrichedEmail(\n",
    "        email_id=classified.email_id,\n",
    "        content=classified.original_content,\n",
    "        is_long=len(classified.original_content) > LONG_EMAIL_THRESHOLD,\n",
    "        category=classified.category\n",
    "    )\n",
    "    await ctx.send_message(enriched)\n",
    "\n",
    "@executor(id=\"respond_path\")\n",
    "async def respond_path(email: EnrichedEmail, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Send to writer for response.\"\"\"\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Draft a response to:\\n{email.content}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "@executor(id=\"summarize_path\")\n",
    "async def summarize_path(email: EnrichedEmail, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Send to summarizer.\"\"\"\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Summarize this email:\\n{email.content}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Aggregator to combine parallel results\n",
    "class ParallelAggregator(Executor):\n",
    "    def __init__(self):\n",
    "        super().__init__(id=\"parallel_aggregator\")\n",
    "    \n",
    "    @handler\n",
    "    async def aggregate(self, results: list[Any], ctx: WorkflowContext[Never, str]) -> None:\n",
    "        \"\"\"Combine response and summary.\"\"\"\n",
    "        output_parts = []\n",
    "        \n",
    "        for result in results:\n",
    "            if isinstance(result, AgentExecutorResponse):\n",
    "                try:\n",
    "                    draft = DraftResponse.model_validate_json(result.agent_response.text)\n",
    "                    output_parts.append(f\"üìß DRAFT RESPONSE:\\nSubject: {draft.subject}\\n{draft.body}\")\n",
    "                except:\n",
    "                    try:\n",
    "                        summary = EmailSummary.model_validate_json(result.agent_response.text)\n",
    "                        points = \"\\n\".join(f\"  ‚Ä¢ {p}\" for p in summary.key_points)\n",
    "                        output_parts.append(f\"üìã SUMMARY:\\n{points}\\nUrgency: {summary.urgency}\\nAction: {summary.action_required}\")\n",
    "                    except:\n",
    "                        output_parts.append(f\"Result: {result.agent_response.text[:200]}...\")\n",
    "        \n",
    "        await ctx.yield_output(\"\\n\\n\" + \"=\"*40 + \"\\n\\n\".join(output_parts))\n",
    "\n",
    "aggregator = ParallelAggregator()\n",
    "\n",
    "print(\"‚úÖ Parallel processing executors defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ef984",
   "metadata": {},
   "source": [
    "## Build Fan-Out/Fan-In Workflow\n",
    "\n",
    "Short emails ‚Üí respond only. Long emails ‚Üí respond + summarize in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e887adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import WorkflowBuilder\n",
    "from agent_framework._workflows._events import ExecutorCompletedEvent\n",
    "from datetime import datetime\n",
    "\n",
    "# Constants\n",
    "LONG_EMAIL_THRESHOLD = 200  # Characters\n",
    "\n",
    "# Start executor - entry point stores email and passes it forward\n",
    "@executor(id=\"fanout_start\")\n",
    "async def fanout_start(email_text: str, ctx: WorkflowContext[str]) -> None:\n",
    "    \"\"\"Entry point: store email length, forward email text.\"\"\"\n",
    "    # Store email length in shared state for selection\n",
    "    await ctx.set_shared_state(\"email_length\", len(email_text))\n",
    "    # Store workflow start time\n",
    "    await ctx.set_shared_state(\"workflow_start_time\", time.time())\n",
    "    await ctx.send_message(email_text)\n",
    "\n",
    "# Selection function that uses shared state\n",
    "def fanout_select_paths(email_text: str, target_ids: list[str]) -> list[str]:\n",
    "    \"\"\"Select paths based on email length (stored in text).\"\"\"\n",
    "    # The email_text is still the raw string at this point\n",
    "    if len(email_text) > LONG_EMAIL_THRESHOLD:\n",
    "        return target_ids  # Both paths for long emails\n",
    "    return [target_ids[0]]  # Only response path for short emails\n",
    "\n",
    "# Response path preparer with timing\n",
    "@executor(id=\"fanout_respond_prep\")\n",
    "async def fanout_respond_prep(email_text: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Prepare email for writer agent.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    start_time = time.time()\n",
    "    elapsed = start_time - workflow_start\n",
    "    print(f\"   ‚è±Ô∏è  [+{elapsed:.2f}s] üìù RESPONSE PATH started\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"response_start_time\", start_time)\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Draft a response to:\\n{email_text}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Summary path preparer with timing\n",
    "@executor(id=\"fanout_summarize_prep\")\n",
    "async def fanout_summarize_prep(email_text: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Prepare email for summarizer agent.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    start_time = time.time()\n",
    "    elapsed = start_time - workflow_start\n",
    "    print(f\"   ‚è±Ô∏è  [+{elapsed:.2f}s] üìã SUMMARY PATH started\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"summary_start_time\", start_time)\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Summarize this email:\\n{email_text}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Capture completion time immediately after writer finishes\n",
    "@executor(id=\"capture_writer_completion\")\n",
    "async def capture_writer_completion(result: Any, ctx: WorkflowContext[Any]) -> None:\n",
    "    \"\"\"Capture writer completion time.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    response_start = await ctx.get_shared_state(\"response_start_time\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_from_start = end_time - workflow_start\n",
    "    duration = end_time - response_start\n",
    "    print(f\"   ‚è±Ô∏è  [+{elapsed_from_start:.2f}s] ‚úÖ RESPONSE PATH completed ({duration:.2f}s)\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"response_end_time\", end_time)\n",
    "    await ctx.send_message(result)\n",
    "\n",
    "# Capture completion time immediately after summarizer finishes\n",
    "@executor(id=\"capture_summarizer_completion\")\n",
    "async def capture_summarizer_completion(result: Any, ctx: WorkflowContext[Any]) -> None:\n",
    "    \"\"\"Capture summarizer completion time.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    summary_start = await ctx.get_shared_state(\"summary_start_time\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_from_start = end_time - workflow_start\n",
    "    duration = end_time - summary_start\n",
    "    print(f\"   ‚è±Ô∏è  [+{elapsed_from_start:.2f}s] ‚úÖ SUMMARY PATH completed ({duration:.2f}s)\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"summary_end_time\", end_time)\n",
    "    await ctx.send_message(result)\n",
    "\n",
    "# Aggregator - combines results from parallel paths with timing\n",
    "@executor(id=\"fanout_aggregator\")\n",
    "async def fanout_aggregator(results: list[Any], ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Combine response and summary results with timing information.\"\"\"\n",
    "    response_start = await ctx.get_shared_state(\"response_start_time\")\n",
    "    summary_start = await ctx.get_shared_state(\"summary_start_time\")\n",
    "    response_end = await ctx.get_shared_state(\"response_end_time\")\n",
    "    summary_end = await ctx.get_shared_state(\"summary_end_time\")\n",
    "    \n",
    "    output_parts = []\n",
    "    response_time = None\n",
    "    summary_time = None\n",
    "    \n",
    "    # Calculate durations from stored times\n",
    "    if response_start and response_end:\n",
    "        response_time = response_end - response_start\n",
    "    if summary_start and summary_end:\n",
    "        summary_time = summary_end - summary_start\n",
    "    \n",
    "    for result in results:\n",
    "        if isinstance(result, AgentExecutorResponse):\n",
    "            try:\n",
    "                draft = DraftResponse.model_validate_json(extract_json(result.agent_response.text))\n",
    "                output_parts.append(\n",
    "                    f\"üì¨ RESPONSE (completed in {response_time:.2f}s):\\n\"\n",
    "                    f\"Subject: {draft.subject}\\n{draft.body}\"\n",
    "                )\n",
    "            except:\n",
    "                try:\n",
    "                    summary = EmailSummary.model_validate_json(extract_json(result.agent_response.text))\n",
    "                    points = \"\\n\".join(f\"  ‚Ä¢ {p}\" for p in summary.key_points)\n",
    "                    output_parts.append(\n",
    "                        f\"üìã SUMMARY (completed in {summary_time:.2f}s):\\n\"\n",
    "                        f\"{points}\\n\"\n",
    "                        f\"Urgency: {summary.urgency}\\n\"\n",
    "                        f\"Action: {summary.action_required}\"\n",
    "                    )\n",
    "                except:\n",
    "                    output_parts.append(f\"Result: {result.agent_response.text[:200]}...\")\n",
    "    \n",
    "    # Calculate overlap to show parallelization\n",
    "    if response_time and summary_time:\n",
    "        total_sequential = response_time + summary_time\n",
    "        total_parallel = max(response_time, summary_time)\n",
    "        time_saved = total_sequential - total_parallel\n",
    "        output_parts.append(\n",
    "            f\"\\n‚ö° PARALLEL EXECUTION BENEFIT:\\n\"\n",
    "            f\"   Sequential time: {total_sequential:.2f}s\\n\"\n",
    "            f\"   Parallel time: {total_parallel:.2f}s\\n\"\n",
    "            f\"   Time saved: {time_saved:.2f}s ({time_saved/total_sequential*100:.1f}%)\"\n",
    "        )\n",
    "    \n",
    "    await ctx.yield_output(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(output_parts))\n",
    "\n",
    "# Build the fan-out workflow\n",
    "# Pattern: start -> [fanout to preparers] -> [agents] -> [capture timing] -> aggregator\n",
    "fanout_workflow = (\n",
    "    WorkflowBuilder()\n",
    "    .set_start_executor(fanout_start)\n",
    "    # Fan-out from start directly to path preparers based on email length\n",
    "    .add_multi_selection_edge_group(\n",
    "        fanout_start,\n",
    "        targets=[fanout_respond_prep, fanout_summarize_prep],\n",
    "        selection_func=fanout_select_paths,\n",
    "    )\n",
    "    # Each preparer sends to its agent\n",
    "    .add_edge(fanout_respond_prep, writer_agent)\n",
    "    .add_edge(fanout_summarize_prep, summarizer_agent)\n",
    "    # Capture completion times immediately after each agent\n",
    "    .add_edge(writer_agent, capture_writer_completion)\n",
    "    .add_edge(summarizer_agent, capture_summarizer_completion)\n",
    "    # Fan-in: collect all results\n",
    "    .add_fan_in_edges([capture_writer_completion, capture_summarizer_completion], fanout_aggregator)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fan-out/fan-in workflow built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b962d58",
   "metadata": {},
   "source": [
    "## Test Parallel Execution\n",
    "\n",
    "Long emails trigger both response and summary paths concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7492286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with long legitimate email\n",
    "async def test_fanout():\n",
    "    email_text = f\"From: {LEGIT_EMAIL.sender}\\nSubject: {LEGIT_EMAIL.subject}\\n\\n{LEGIT_EMAIL.body}\"\n",
    "    \n",
    "    print(f\"üìß Testing LONG email ({len(email_text)} chars > {LONG_EMAIL_THRESHOLD} threshold)\")\n",
    "    print(\"Expected: Response AND Summary in parallel\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    async for event in fanout_workflow.run_stream(email_text):\n",
    "        if isinstance(event, WorkflowOutputEvent):\n",
    "            print(event.data)\n",
    "\n",
    "await test_fanout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a7439",
   "metadata": {},
   "source": [
    "# 11. Group Chat Orchestration\n",
    "\n",
    "![Group Chat Pattern](images/group-chat.png)\n",
    "\n",
    "Multiple agents collaborate in a shared conversation, coordinated by an orchestrator.\n",
    "\n",
    "**When to Use:**\n",
    "- Iterative refinement with multiple review rounds\n",
    "- Collaborative problem-solving with shared context\n",
    "- Multi-perspective analysis (e.g., writer-reviewer workflows)\n",
    "\n",
    "**When NOT to Use:**\n",
    "- Agents should work independently (use Concurrent)\n",
    "- Complex dynamic planning needed (use Magentic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38398cec",
   "metadata": {},
   "source": [
    "## Key Differences\n",
    "\n",
    "| Pattern | Coordination | Use Case |\n",
    "|---------|--------------|----------|\n",
    "| **Concurrent** | No coordination | Independent parallel tasks |\n",
    "| **Group Chat** | Orchestrator selects speakers | Iterative refinement, shared context |\n",
    "| **Magentic** | Manager with dynamic planning | Complex open-ended tasks |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105e4d2",
   "metadata": {},
   "source": [
    "## Define Specialists\n",
    "\n",
    "Create agents with distinct review roles. All agents will see the shared conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b41c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import GroupChatBuilder, GroupChatState, ConcurrentBuilder, MagenticBuilder\n",
    "\n",
    "# Three specialized reviewers - order matters! Last one produces final output.\n",
    "\n",
    "# 1st: Security reviewer - identifies security/compliance issues\n",
    "security_reviewer = ChatAgent(\n",
    "    name=\"SecurityReviewer\",\n",
    "    description=\"Security and compliance specialist - reviews first\",\n",
    "    instructions=\"\"\"You are the FIRST reviewer. Analyze the support response for:\n",
    "- Data exposure risks (customer IDs, case numbers that shouldn't be in emails)\n",
    "- PII handling concerns (names, order details)\n",
    "- Policy compliance issues\n",
    "\n",
    "Be concise. List only the security issues you find. Do NOT rewrite the email - just identify problems for later reviewers to address.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# 2nd: Accuracy reviewer - checks facts and promises\n",
    "accuracy_reviewer = ChatAgent(\n",
    "    name=\"AccuracyReviewer\", \n",
    "    description=\"Factual accuracy specialist - reviews second\",\n",
    "    instructions=\"\"\"You are the SECOND reviewer. Analyze the support response for:\n",
    "- Unrealistic promises or timelines\n",
    "- Unverifiable claims\n",
    "- Compensation appropriateness\n",
    "\n",
    "Consider the security feedback from the previous reviewer. Be concise. List only the accuracy issues. Do NOT rewrite the email - just identify problems for the final reviewer to address.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# 3rd: Tone reviewer - applies all feedback and produces final email\n",
    "tone_reviewer = ChatAgent(\n",
    "    name=\"ToneReviewer\",\n",
    "    description=\"Tone specialist and final editor - produces revised email\",\n",
    "    instructions=\"\"\"You are the FINAL reviewer. Your job is to:\n",
    "1. Consider ALL feedback from SecurityReviewer and AccuracyReviewer\n",
    "2. Review the tone and empathy of the original email\n",
    "3. **PRODUCE A FINAL REVISED EMAIL** that:\n",
    "   - Addresses security concerns (remove/mask sensitive identifiers if needed)\n",
    "   - Fixes accuracy issues (realistic timelines, appropriate promises)\n",
    "   - Maintains professional, empathetic tone\n",
    "   - Is ready to send to the customer\n",
    "\n",
    "End your response with the complete revised email in a clear format.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Three specialist reviewers defined:\")\n",
    "print(\"   1. SecurityReviewer - identifies security issues\")\n",
    "print(\"   2. AccuracyReviewer - checks facts and promises\")  \n",
    "print(\"   3. ToneReviewer - applies all feedback and produces FINAL email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27452e67",
   "metadata": {},
   "source": [
    "## Build Group Chat with Round-Robin\n",
    "\n",
    "Simple selection: each reviewer speaks in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample draft response to review\n",
    "draft_to_review = \"\"\"\n",
    "Subject: Re: Order #12345 - Delivery Issue\n",
    "\n",
    "Dear Sarah,\n",
    "\n",
    "I'm so sorry to hear about the missing package! This must be incredibly frustrating.\n",
    "\n",
    "I've located your order and can confirm it was marked as delivered on Monday. Here's what I'll do:\n",
    "\n",
    "1. I've opened an investigation with our shipping partner (Case #INV-789)\n",
    "2. As a Premium customer, I'm expediting a replacement shipment TODAY\n",
    "3. The replacement will arrive by Thursday, well before your Friday presentation\n",
    "\n",
    "Your account has also been credited $50 for the inconvenience.\n",
    "\n",
    "If you need anything else, reply directly to this email - I'm here to help!\n",
    "\n",
    "Best regards,\n",
    "Support Team\n",
    "\"\"\"\n",
    "\n",
    "# Round-robin selector: each reviewer speaks in order\n",
    "def round_robin_selector(state: GroupChatState) -> str:\n",
    "    \"\"\"Pick the next speaker based on round index.\"\"\"\n",
    "    participants = list(state.participants.keys())\n",
    "    return participants[state.current_round % len(participants)]\n",
    "\n",
    "# Build group chat with round-robin selection\n",
    "# ORDER MATTERS: Security ‚Üí Accuracy ‚Üí Tone (final editor)\n",
    "review_group_chat = (\n",
    "    GroupChatBuilder()\n",
    "    .with_orchestrator(selection_func=round_robin_selector, orchestrator_name=\"RoundRobinOrchestrator\")\n",
    "    .participants([security_reviewer, accuracy_reviewer, tone_reviewer])  # Order: Security ‚Üí Accuracy ‚Üí Tone\n",
    "    .with_termination_condition(lambda msgs: len([m for m in msgs if m.role.value == \"assistant\"]) >= 3)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Group chat built with round-robin selection\")\n",
    "print(\"   Order: SecurityReviewer ‚Üí AccuracyReviewer ‚Üí ToneReviewer (final)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7588de",
   "metadata": {},
   "source": [
    "## Test Round-Robin Group Chat\n",
    "\n",
    "Each reviewer analyzes the draft in turn, building on previous insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the group chat with round-robin selection\n",
    "from agent_framework._workflows._events import AgentRunUpdateEvent\n",
    "\n",
    "async def test_round_robin_group_chat():\n",
    "    print(\"üìù DRAFT TO REVIEW:\")\n",
    "    print(draft_to_review)\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\nüîÑ ROUND-ROBIN GROUP CHAT (each reviewer speaks in turn):\\n\")\n",
    "    \n",
    "    last_executor_id: str | None = None\n",
    "    agent_order = []\n",
    "    \n",
    "    async for event in review_group_chat.run_stream(f\"Review this support response:\\n{draft_to_review}\"):\n",
    "        if isinstance(event, AgentRunUpdateEvent):\n",
    "            eid = event.executor_id\n",
    "            if eid != last_executor_id:\n",
    "                if last_executor_id is not None:\n",
    "                    print(\"\\n\")\n",
    "                agent_order.append(eid)\n",
    "                print(f\"\\nü§ñ [{eid}] (Turn #{len(agent_order)}):\", end=\" \", flush=True)\n",
    "                last_executor_id = eid\n",
    "            print(event.data, end=\"\", flush=True)\n",
    "        \n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            print(\"\\n\\n\" + \"=\" * 60)\n",
    "            print(f\"üìä EXECUTION ORDER: {' ‚Üí '.join(agent_order)}\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "await test_round_robin_group_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent-based orchestrator for intelligent speaker selection\n",
    "from typing import cast\n",
    "from agent_framework._workflows._events import AgentRunUpdateEvent, WorkflowOutputEvent\n",
    "from agent_framework._types import ChatMessage\n",
    "\n",
    "orchestrator_agent = ChatAgent(\n",
    "    name=\"ReviewOrchestrator\",\n",
    "    description=\"Coordinates multi-agent review process\",\n",
    "    instructions=f\"\"\"You coordinate a team reviewing this support response:\n",
    "\n",
    "{draft_to_review}\n",
    "\n",
    "YOUR TEAM:\n",
    "- SecurityReviewer: Identifies security/PII issues (reviews first)\n",
    "- AccuracyReviewer: Checks facts and promises (reviews second)\n",
    "- ToneReviewer: Final editor who produces the revised email (reviews last)\n",
    "\n",
    "YOUR PROCESS:\n",
    "1. Start with SecurityReviewer to check data safety and PII\n",
    "2. Then AccuracyReviewer to verify claims and timelines\n",
    "3. **Finally, ToneReviewer to produce the FINAL REVISED EMAIL** incorporating all feedback\n",
    "4. If needed, you may ask follow-up questions to any reviewer\n",
    "5. End when ToneReviewer delivers the complete revised email\n",
    "\n",
    "Select speakers intelligently. CRITICAL: ToneReviewer must go last and produce the final email.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# Build group chat with agent-based orchestration\n",
    "# ORDER: Security ‚Üí Accuracy ‚Üí Tone (final editor)\n",
    "intelligent_review_chat = (\n",
    "    GroupChatBuilder()\n",
    "    .with_orchestrator(agent=orchestrator_agent)\n",
    "    .participants([security_reviewer, accuracy_reviewer, tone_reviewer])\n",
    "    .with_termination_condition(lambda msgs: len([m for m in msgs if m.role.value == \"assistant\"]) >= 5)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Run with detailed logging\n",
    "async def test_agent_orchestrated_group_chat():\n",
    "    print(\"üìù DRAFT TO REVIEW:\")\n",
    "    print(draft_to_review)\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\nüß† AGENT-ORCHESTRATED GROUP CHAT (intelligent speaker selection):\\n\")\n",
    "    \n",
    "    last_executor_id: str | None = None\n",
    "    agent_calls: dict[str, int] = {}\n",
    "    \n",
    "    async for event in intelligent_review_chat.run_stream(\"Review this support response. Security and Accuracy reviewers identify issues, then ToneReviewer produces the final revised email.\"):\n",
    "        if isinstance(event, AgentRunUpdateEvent):\n",
    "            eid = event.executor_id\n",
    "            if eid != last_executor_id:\n",
    "                if last_executor_id is not None:\n",
    "                    print(\"\\n\")\n",
    "                agent_calls[eid] = agent_calls.get(eid, 0) + 1\n",
    "                print(f\"\\nü§ñ [{eid}] (Call #{agent_calls[eid]}):\", end=\" \", flush=True)\n",
    "                last_executor_id = eid\n",
    "            print(event.data, end=\"\", flush=True)\n",
    "        \n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            output_messages = cast(list[ChatMessage], event.data)\n",
    "            \n",
    "            print(\"\\n\\n\" + \"=\" * 60)\n",
    "            print(\"üìä EXECUTION SUMMARY\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"   Total calls: {sum(agent_calls.values())}\")\n",
    "            print(\"\\n   Calls per agent:\")\n",
    "            for agent, count in sorted(agent_calls.items()):\n",
    "                print(f\"      {agent}: {count} call(s)\")\n",
    "            \n",
    "            print(\"\\n   üí° The orchestrator dynamically selected speakers\")\n",
    "            print(\"      based on what was needed at each step\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"üìß FINAL REVISED EMAIL (from ToneReviewer)\")\n",
    "            print(\"=\" * 60)\n",
    "            for msg in reversed(output_messages):\n",
    "                if msg.role.value == \"assistant\" and \"ToneReviewer\" in str(msg):\n",
    "                    print(msg.text)\n",
    "                    break\n",
    "\n",
    "await test_agent_orchestrated_group_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4e052",
   "metadata": {},
   "source": [
    "# 12. Magentic Orchestration\n",
    "\n",
    "![Magentic Pattern](images/magentic-workflow.png)\n",
    "\n",
    "Magentic is the most powerful orchestration pattern - a manager dynamically plans and delegates to specialists based on evolving task requirements.\n",
    "\n",
    "**When to Use:**\n",
    "- Complex, open-ended tasks requiring multiple iterations\n",
    "- Tasks where the solution path isn't known in advance\n",
    "- Research + analysis workflows with code execution\n",
    "\n",
    "**When NOT to Use:**\n",
    "- Simple linear pipelines (use Sequential)\n",
    "- Fixed review rounds (use Group Chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0d883",
   "metadata": {},
   "source": [
    "## Use Case: Market Research Report\n",
    "\n",
    "A complex task requiring:\n",
    "1. **Research Agent** - Web search for current data\n",
    "2. **Analyst Agent** - Code execution for data processing\n",
    "3. **Manager** - Dynamic planning and synthesis\n",
    "\n",
    "The manager autonomously decides which agent to call and when based on progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magentic Orchestration: Research + Analysis workflow\n",
    "import json\n",
    "from typing import cast\n",
    "from agent_framework import (\n",
    "    AgentRunUpdateEvent,\n",
    "    MagenticOrchestratorEvent,\n",
    "    MagenticProgressLedger,\n",
    ")\n",
    "\n",
    "# Research Agent - uses web search capability\n",
    "# Note: In production, use OpenAI's gpt-4o-search-preview or add web search tools\n",
    "researcher_agent = ChatAgent(\n",
    "    name=\"ResearcherAgent\",\n",
    "    description=\"Specialist in research and information gathering about markets, trends, and data\",\n",
    "    instructions=\"\"\"You are a Research Specialist. Your job is to:\n",
    "- Gather factual information about topics\n",
    "- Find current statistics and trends\n",
    "- Provide sources for your findings\n",
    "\n",
    "When asked about market data, provide realistic example data with citations.\n",
    "Be concise and factual. Format data clearly for analysis.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# Analyst Agent - processes and analyzes data\n",
    "# Note: In production, add HostedCodeInterpreterTool for real code execution\n",
    "analyst_agent = ChatAgent(\n",
    "    name=\"AnalystAgent\",\n",
    "    description=\"Data analyst who processes information and creates insights with calculations\",\n",
    "    instructions=\"\"\"You are a Data Analyst. Your job is to:\n",
    "- Process and analyze data provided by the researcher\n",
    "- Perform calculations (growth rates, comparisons, projections)\n",
    "- Create clear tables and visualizations descriptions\n",
    "- Identify trends and insights\n",
    "\n",
    "Show your calculations step by step. Format results in clear tables.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# Manager Agent - orchestrates the research workflow\n",
    "manager_agent = ChatAgent(\n",
    "    name=\"ResearchManager\",\n",
    "    description=\"Orchestrator that coordinates research and analysis workflows\",\n",
    "    instructions=\"\"\"You manage a research team to complete comprehensive analysis tasks.\n",
    "\n",
    "YOUR TEAM:\n",
    "- ResearcherAgent: Gathers information, statistics, and market data\n",
    "- AnalystAgent: Processes data, performs calculations, creates insights\n",
    "\n",
    "YOUR PROCESS:\n",
    "1. Break down the research request into subtasks\n",
    "2. Delegate to ResearcherAgent to gather relevant data\n",
    "3. Delegate to AnalystAgent to process and analyze the data\n",
    "4. Continue iterating until you have comprehensive insights\n",
    "5. Synthesize all findings into a final report\n",
    "\n",
    "You dynamically decide who to call based on what's needed. You may call agents multiple times.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Magentic agents defined: ResearcherAgent, AnalystAgent, ResearchManager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ddaa3",
   "metadata": {},
   "source": [
    "## Build & Run Magentic Workflow\n",
    "\n",
    "The manager dynamically plans and delegates. Watch how it calls different agents based on the evolving task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919845da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Magentic workflow\n",
    "magentic_research_workflow = (\n",
    "    MagenticBuilder()\n",
    "    .participants([researcher_agent, analyst_agent])\n",
    "    .with_manager(\n",
    "        agent=manager_agent,\n",
    "        max_round_count=10,  # Maximum delegation rounds\n",
    "        max_stall_count=2,   # Replan after 2 stalls\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Research task - complex enough to require multiple agent interactions\n",
    "research_task = \"\"\"\n",
    "Analyze the global electric vehicle (EV) market:\n",
    "1. Find the top 5 EV manufacturers by market share\n",
    "2. Calculate year-over-year growth rates\n",
    "3. Compare EV adoption rates in US, Europe, and China\n",
    "4. Provide a summary table and key insights\n",
    "\"\"\"\n",
    "\n",
    "async def run_magentic_research():\n",
    "    print(\"üî¨ MAGENTIC RESEARCH WORKFLOW\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìã TASK:\\n{research_task}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    last_message_id: str | None = None\n",
    "    agent_calls: dict[str, int] = {}\n",
    "    \n",
    "    async for event in magentic_research_workflow.run_stream(research_task):\n",
    "        # Track streaming from agents\n",
    "        if isinstance(event, AgentRunUpdateEvent):\n",
    "            message_id = event.data.message_id\n",
    "            executor_id = event.executor_id\n",
    "            \n",
    "            if message_id != last_message_id:\n",
    "                if last_message_id is not None:\n",
    "                    print(\"\\n\")\n",
    "                agent_calls[executor_id] = agent_calls.get(executor_id, 0) + 1\n",
    "                print(f\"\\nü§ñ [{executor_id}] (Call #{agent_calls[executor_id]}):\", end=\" \", flush=True)\n",
    "                last_message_id = message_id\n",
    "            \n",
    "            print(event.data, end=\"\", flush=True)\n",
    "        \n",
    "        # Track orchestration events\n",
    "        elif isinstance(event, MagenticOrchestratorEvent):\n",
    "            print(f\"\\n\\n{'='*55}\")\n",
    "            print(f\"üìã ORCHESTRATOR: {event.event_type.name}\")\n",
    "            print(f\"{'='*55}\")\n",
    "            \n",
    "            if isinstance(event.data, MagenticProgressLedger):\n",
    "                ledger = event.data.to_dict()\n",
    "                if \"next_speaker\" in ledger:\n",
    "                    next_info = ledger.get('next_speaker', {})\n",
    "                    if isinstance(next_info, dict):\n",
    "                        print(f\"   ‚û°Ô∏è Next: {next_info.get('answer', 'N/A')}\")\n",
    "                        reason = next_info.get('reason', '')\n",
    "                        if reason:\n",
    "                            print(f\"   üí≠ Why: {reason[:100]}...\")\n",
    "                    else:\n",
    "                        print(f\"   ‚û°Ô∏è Next: {next_info}\")\n",
    "        \n",
    "        # Final output\n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            output_messages = cast(list[ChatMessage], event.data)\n",
    "            \n",
    "            print(\"\\n\\n\" + \"=\" * 60)\n",
    "            print(\"üìä EXECUTION SUMMARY\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"   Total agent calls: {sum(agent_calls.values())}\")\n",
    "            print(\"\\n   Calls per agent:\")\n",
    "            for agent, count in sorted(agent_calls.items()):\n",
    "                print(f\"      {agent}: {count} call(s)\")\n",
    "            \n",
    "            print(\"\\n   ‚ú® Manager dynamically orchestrated:\")\n",
    "            print(f\"      - Broke down complex task into subtasks\")\n",
    "            print(f\"      - Called ResearcherAgent for data gathering\")\n",
    "            print(f\"      - Called AnalystAgent for processing\")\n",
    "            print(f\"      - Synthesized into final report\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"üìë FINAL RESEARCH REPORT\")\n",
    "            print(\"=\" * 60)\n",
    "            for msg in reversed(output_messages):\n",
    "                if msg.role.value == \"assistant\":\n",
    "                    print(msg.text)\n",
    "                    break\n",
    "\n",
    "await run_magentic_research()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
