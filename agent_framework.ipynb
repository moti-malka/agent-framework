{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321ecba2",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Welcome to the InboxOps Agent Framework Demo</h1>\n",
    "\n",
    "<p align=\"start\">\n",
    "InboxOps is an e-commerce operations company handling thousands of inbound support emails per day.\n",
    "In this notebook, we'll build a production-minded multi-agent Support Email Copilot using the\n",
    "<strong>Microsoft Agent Framework (Python SDK)</strong>.\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## The InboxOps Problem\n",
    "\n",
    "InboxOps started with a simple goal:\n",
    "\n",
    "\u2705 Reply faster  \n",
    "\u2705 Reduce agent workload  \n",
    "\u2705 Maintain consistent tone and quality  \n",
    "\u2705 Avoid risky or incorrect customer promises  \n",
    "\n",
    "But as volume increased, we discovered that **a single LLM prompt is not enough**.\n",
    "\n",
    "So we evolve the system step-by-step:\n",
    "\n",
    "**V0 \u2192 V1 \u2192 V2 \u2192 Production-Ready Multi-Agent Workflows**\n",
    "\n",
    "---\n",
    "\n",
    "## What is an Agent?\n",
    "\n",
    "![What is an Agent](images/what-is-agent.png)\n",
    "\n",
    "Unlike traditional LLM deployments that simply respond to prompts, agents follow the **ReAct pattern** (Reasoning + Acting):\n",
    "\n",
    "| Traditional LLM | Agent (ReAct) |\n",
    "|-----------------|---------------|\n",
    "| Input \u2192 Output | Input \u2192 Reason \u2192 Act \u2192 Observe \u2192 Repeat |\n",
    "| Single response | Multi-step execution |\n",
    "| No tool access | Tool integration |\n",
    "| Stateless | Memory & context |\n",
    "\n",
    "Agents autonomously decide *what* to do, *which* tools to use, and *when* to stop.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflows & Multi-Agent Orchestration\n",
    "\n",
    "![Workflow Example](images/workflow-example.png)\n",
    "\n",
    "Complex tasks require coordination between multiple specialized agents. The Agent Framework provides workflow primitives:\n",
    "\n",
    "- **Sequential** \u2014 Agents execute in order (A \u2192 B \u2192 C)\n",
    "- **Parallel (Fan-out/Fan-in)** \u2014 Concurrent execution with result aggregation\n",
    "- **Branching** \u2014 Conditional routing based on outputs\n",
    "- **Group Chat** \u2014 Collaborative multi-agent discussions\n",
    "\n",
    "---\n",
    "\n",
    "## Demo Overview\n",
    "\n",
    "We'll build the **InboxOps Support Email Copilot** that demonstrates core framework concepts:\n",
    "\n",
    "| Section | Concept |\n",
    "|---------|---------|\n",
    "| 1-2 | V0: Basic Agent & Streaming |\n",
    "| 3-4 | V1: Threads & Tools |\n",
    "| 5-7 | V2: Approvals, Middleware, Memory |\n",
    "| 8-10 | Workflows: Sequential, Branching, Parallel |\n",
    "| 11-12 | Multi-Agent: Group Chat & Magentic |\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure subscription with Azure OpenAI access\n",
    "- Azure OpenAI resource with deployed model (e.g., `gpt-4o-mini`)\n",
    "- Azure CLI installed and authenticated (`az login`)\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d841bbb",
   "metadata": {},
   "source": [
    "# Environment Setup (InboxOps Internal Dev Environment)\n",
    "\n",
    "InboxOps is prototyping a Support Email Copilot using Azure OpenAI and the Microsoft Agent Framework.\n",
    "\n",
    "This notebook assumes:\n",
    "- You have access to an Azure OpenAI resource\n",
    "- A model deployment exists (example: `gpt-4o-mini`)\n",
    "- You can authenticate with Azure CLI (`az login`)\n",
    "- Python 3.10+\n",
    "\n",
    "> The goal is to keep the demo reproducible for developers and consistent across environments.\n",
    "\n",
    "## Create Virtual Environment\n",
    "\n",
    "Run the following in your terminal to set up the environment:\n",
    "\n",
    "```bash\n",
    "python3.10 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Or run the cell below to install dependencies directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the virtual environment (run once)\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def find_python():\n",
    "    \"\"\"Find a Python 3.10+ interpreter on the system.\"\"\"\n",
    "    # Check common Python commands in order of preference\n",
    "    candidates = [\n",
    "        \"python3.13\", \"python3.12\", \"python3.11\", \"python3.10\",\n",
    "        \"python3\", \"python\"\n",
    "    ]\n",
    "    \n",
    "    for cmd in candidates:\n",
    "        path = shutil.which(cmd)\n",
    "        if path:\n",
    "            # Verify version is 3.10+\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [path, \"-c\", \"import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')\"],\n",
    "                    capture_output=True, text=True\n",
    "                )\n",
    "                version = result.stdout.strip()\n",
    "                major, minor = map(int, version.split('.'))\n",
    "                if major >= 3 and minor >= 10:\n",
    "                    return path, version\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    raise RuntimeError(\"No Python 3.10+ found. Please install Python 3.10 or higher.\")\n",
    "\n",
    "# Find suitable Python\n",
    "python_path, python_version = find_python()\n",
    "print(f\"\u2705 Found Python {python_version}: {python_path}\")\n",
    "\n",
    "# Create .venv\n",
    "subprocess.run([python_path, \"-m\", \"venv\", \".venv\"])\n",
    "\n",
    "# Install requirements with pre-release flag\n",
    "subprocess.run([\".venv/bin/pip\", \"install\", \"-r\", \"requirements.txt\", \"--pre\"])\n",
    "\n",
    "print(\"\\n\u2705 Virtual environment created at .venv\")\n",
    "print(\"   Activate with: source .venv/bin/activate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e423b",
   "metadata": {},
   "source": [
    "## Initialize the InboxOps Chat Client\n",
    "\n",
    "We create **one shared Azure OpenAI client** and reuse it across the entire notebook.\n",
    "\n",
    "This mirrors how InboxOps would run a long-lived backend service:\n",
    "- The service initializes once\n",
    "- Agents are created from the same client\n",
    "- Tool calls, workflows, memory, and orchestration all share the same foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d946629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework_azure_ai import AzureAIAgentClient\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import AzureCliCredential\n",
    "from agent_framework.azure import AzureOpenAIChatClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create ONE chat client - reused throughout the notebook\n",
    "chat_client = AzureOpenAIChatClient(credential=AzureCliCredential())\n",
    "chat_client_mcp = AzureAIAgentClient(credential=AzureCliCredential())\n",
    "\n",
    "print(\"\u2705 Environment loaded and chat_client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10492593",
   "metadata": {},
   "source": [
    "## Data Models (InboxOps Message Contracts)\n",
    "\n",
    "InboxOps wants predictable, structured outputs\u2014not messy free-text.\n",
    "\n",
    "We define Pydantic schemas used across the system:\n",
    "- Incoming email structure (`EmailInput`)\n",
    "- Classification outputs (`ClassificationResult`)\n",
    "- Draft response formats (`DraftResponse`)\n",
    "- Final approval structure (`FinalResponse`)\n",
    "\n",
    "> These schemas represent the \"API contracts\" between our agents, tools, and workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# === Input Model ===\n",
    "class EmailInput(BaseModel):\n",
    "    \"\"\"Incoming support email.\"\"\"\n",
    "    sender: str = Field(description=\"Email sender address\")\n",
    "    subject: str = Field(description=\"Email subject line\")\n",
    "    body: str = Field(description=\"Email body content\")\n",
    "    customer_id: str | None = Field(default=None, description=\"Customer ID if known\")\n",
    "    ticket_id: str | None = Field(default=None, description=\"Related ticket ID if any\")\n",
    "\n",
    "# === Classification Model ===\n",
    "class ClassificationResult(BaseModel):\n",
    "    \"\"\"Result of email classification.\"\"\"\n",
    "    category: Literal[\"spam\", \"not_spam\", \"uncertain\"] = Field(description=\"Email category\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score 0-1\")\n",
    "    reason: str = Field(description=\"Brief explanation of classification\")\n",
    "\n",
    "# === Draft Response Model ===\n",
    "class DraftResponse(BaseModel):\n",
    "    \"\"\"Draft reply to customer email.\"\"\"\n",
    "    subject: str = Field(description=\"Reply subject line\")\n",
    "    body: str = Field(description=\"Reply body\")\n",
    "    tone: Literal[\"formal\", \"friendly\", \"apologetic\"] = Field(description=\"Tone used\")\n",
    "    needs_review: bool = Field(default=False, description=\"Flag if needs human review\")\n",
    "\n",
    "# === Final Response Model ===\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"Final approved response.\"\"\"\n",
    "    classification: ClassificationResult\n",
    "    draft: DraftResponse | None = Field(default=None, description=\"Draft if not spam\")\n",
    "    review_notes: str | None = Field(default=None, description=\"Reviewer comments\")\n",
    "    approved: bool = Field(default=False, description=\"Whether approved to send\")\n",
    "\n",
    "print(\"\u2705 Shared models defined: EmailInput, ClassificationResult, DraftResponse, FinalResponse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca66c9",
   "metadata": {},
   "source": [
    "## Sample InboxOps Emails\n",
    "\n",
    "We'll use three realistic email types to simulate real inbox traffic:\n",
    "\n",
    "\u2705 Legitimate Customer Issue \u2014 should generate a helpful response  \n",
    "\ud83d\udeab Spam Message \u2014 should be blocked  \n",
    "\u26a0\ufe0f Ambiguous Request \u2014 should be routed for human review  \n",
    "\n",
    "> This is exactly what InboxOps sees daily at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d68e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LEGITIMATE EMAIL ===\n",
    "LEGIT_EMAIL = EmailInput(\n",
    "    sender=\"sarah.chen@acmecorp.com\",\n",
    "    subject=\"Order #12345 - Delivery Issue\",\n",
    "    body=\"\"\"Hi Support Team,\n",
    "\n",
    "I placed order #12345 last week and the tracking shows it was delivered, \n",
    "but I never received the package. I've checked with my neighbors and the building \n",
    "concierge, but no one has seen it.\n",
    "\n",
    "This is urgent as the items were needed for a client presentation on Friday.\n",
    "Can you please help me locate the package or arrange a replacement?\n",
    "\n",
    "Thank you,\n",
    "Sarah Chen\n",
    "Account: ACME-7891\n",
    "\"\"\",\n",
    "    customer_id=\"CUST-7891\",\n",
    "    ticket_id=\"TKT-2024-001\"\n",
    ")\n",
    "\n",
    "# === SPAM EMAIL ===\n",
    "SPAM_EMAIL = EmailInput(\n",
    "    sender=\"winner@prize-notifications.biz\",\n",
    "    subject=\"\ud83c\udf89 CONGRATULATIONS! You've WON $1,000,000!!!\",\n",
    "    body=\"\"\"URGENT NOTIFICATION!!!\n",
    "\n",
    "You have been selected as the WINNER of our international lottery!\n",
    "To claim your $1,000,000 prize, simply send your bank details and \n",
    "a processing fee of $500 to unlock your winnings.\n",
    "\n",
    "ACT NOW - This offer expires in 24 HOURS!!!\n",
    "\n",
    "Click here to claim: http://totally-legit-prize.com/claim\n",
    "\"\"\",\n",
    "    customer_id=None,\n",
    "    ticket_id=None\n",
    ")\n",
    "\n",
    "# === AMBIGUOUS EMAIL ===\n",
    "AMBIGUOUS_EMAIL = EmailInput(\n",
    "    sender=\"j.smith@unknown-domain.net\",\n",
    "    subject=\"Partnership Opportunity\",\n",
    "    body=\"\"\"Hello,\n",
    "\n",
    "I found your company online and I'm interested in discussing a potential \n",
    "business partnership. We have a new product line that might complement your services.\n",
    "\n",
    "Can we schedule a call this week?\n",
    "\n",
    "Best,\n",
    "J. Smith\n",
    "\"\"\",\n",
    "    customer_id=None,\n",
    "    ticket_id=None\n",
    ")\n",
    "\n",
    "print(\"\u2705 Sample emails defined: LEGIT_EMAIL, SPAM_EMAIL, AMBIGUOUS_EMAIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26154c1d",
   "metadata": {},
   "source": [
    "# 1. V0 \u2014 A Single Support Agent\n",
    "\n",
    "![Agent Components](images/agent-components.png)\n",
    "\n",
    "InboxOps started with the simplest solution:\n",
    "\n",
    "**One agent that reads an email and drafts a reply.**\n",
    "\n",
    "This already provides huge value:\n",
    "- Faster draft creation\n",
    "- More consistent tone\n",
    "- Reduced repetitive typing for support reps\n",
    "\n",
    "But this is still \"V0\":\n",
    "- No streaming UX\n",
    "- No tools\n",
    "- No multi-turn context\n",
    "- No approvals or governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the core Support Agent - we'll enhance this throughout the notebook\n",
    "support_agent = chat_client.as_agent(\n",
    "    name=\"SupportAgent\",\n",
    "    instructions=\"\"\"You are a helpful customer support agent for an e-commerce company.\n",
    "Your job is to:\n",
    "1. Understand customer issues from their emails\n",
    "2. Draft professional, empathetic responses\n",
    "3. Provide clear next steps when possible\n",
    "\n",
    "Always be polite, acknowledge the customer's frustration, and offer concrete solutions.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\u2705 support_agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f5e7f",
   "metadata": {},
   "source": [
    "## Run the SupportAgent\n",
    "\n",
    "This is the InboxOps baseline:\n",
    "\n",
    "**Input:** customer email  \n",
    "**Output:** draft reply\n",
    "\n",
    "At this stage, we're validating:\n",
    "- Can the agent understand the issue?\n",
    "- Does it respond empathetically?\n",
    "- Are next steps clear and actionable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0324c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the support agent on our legitimate email\n",
    "async def run_basic_agent():\n",
    "    prompt = f\"\"\"Please draft a response to this customer email:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "    result = await support_agent.run(prompt)\n",
    "    print(\"\ud83d\udce7 Draft Response:\\n\")\n",
    "    print(result.text)\n",
    "\n",
    "asyncio.run(run_basic_agent())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9e31c",
   "metadata": {},
   "source": [
    "# 2. V0.1 \u2014 Streaming Responses (Real-Time UX)\n",
    "\n",
    "InboxOps support reps don't want to wait for a full answer.\n",
    "\n",
    "They want a **live drafting experience**:\n",
    "- The response appears token-by-token\n",
    "- It feels interactive, like a \"Copilot\"\n",
    "- Faster perceived performance\n",
    "\n",
    "Streaming is not just cosmetic\u2014it's a product requirement when humans are in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stream the response token by token using the SAME support_agent\n",
    "async def stream_support_response():\n",
    "    prompt = f\"\"\"Please draft a response to this customer email:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "    print(\"\ud83d\udce7 Streaming Draft Response:\\n\")\n",
    "    async for update in support_agent.run_stream(prompt):\n",
    "        if update.text:\n",
    "            print(update.text, end=\"\", flush=True)\n",
    "    print()  # New line after streaming\n",
    "\n",
    "asyncio.run(stream_support_response())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54494e5c",
   "metadata": {},
   "source": [
    "# 3. V1 \u2014 Multi-Turn Conversations with Threads\n",
    "\n",
    "![Threads and Memory](images/threads-and-memory.png)\n",
    "\n",
    "InboxOps quickly discovered a real-world problem:\n",
    "\n",
    "Customers don't send only one email.\n",
    "\n",
    "They follow up:\n",
    "- \"Any updates?\"\n",
    "- \"This is urgent\"\n",
    "- \"I already tried that\"\n",
    "\n",
    "By default, agents are stateless.\n",
    "So InboxOps introduced **Threads** to preserve context across multiple turns.\n",
    "\n",
    "\u2705 The agent can summarize first  \n",
    "\u2705 Then draft a response using the summary  \n",
    "\u2705 And continue the conversation coherently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4aa94",
   "metadata": {},
   "source": [
    "## Using Threads\n",
    "\n",
    "Create a thread with `agent.get_new_thread()` and pass it to each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread for multi-turn conversation\n",
    "thread = support_agent.get_new_thread()\n",
    "\n",
    "# Turn 1: Summarize the customer issue\n",
    "print(\"Turn 1: Summarize the issue\")\n",
    "print(\"-\" * 50)\n",
    "result1 = await support_agent.run(\n",
    "    f\"Summarize the key issues in this email in 2-3 bullet points:\\n\\n{LEGIT_EMAIL.body}\", \n",
    "    thread=thread\n",
    ")\n",
    "print(result1.text)\n",
    "print()\n",
    "\n",
    "# Turn 2: Draft a response (agent remembers the summary from Turn 1)\n",
    "print(\"Turn 2: Draft response with professional tone\")\n",
    "print(\"-\" * 50)\n",
    "result2 = await support_agent.run(\n",
    "    \"Now draft a professional response addressing each of those issues. Use a formal but empathetic tone.\",\n",
    "    thread=thread\n",
    ")\n",
    "print(result2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d5c10",
   "metadata": {},
   "source": [
    "# 4. V1.1 \u2014 Tools: Connecting InboxOps Internal Systems\n",
    "\n",
    "A drafting agent is helpful\u2026\n",
    "but a production support assistant must also be **correct**.\n",
    "\n",
    "InboxOps needs the agent to reference real internal data, not guess.\n",
    "\n",
    "Examples:\n",
    "- SLA tier (Premium vs Standard)\n",
    "- Current ticket status (Open/Resolved)\n",
    "- Prior actions already taken\n",
    "\n",
    "So we expose internal functions as tools using `@tool`.\n",
    "\n",
    "The agent will autonomously decide when tool calls are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d70fa",
   "metadata": {},
   "source": [
    "## Define InboxOps Tools\n",
    "\n",
    "In a real InboxOps environment these tools would call:\n",
    "- CRM systems\n",
    "- ticketing platforms\n",
    "- order management databases\n",
    "\n",
    "For this demo, we simulate internal systems using in-memory dictionaries.\n",
    "\n",
    "> The key point: the Agent Framework turns Python functions into callable tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963debc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import tool\n",
    "# Simulated database of customer SLAs\n",
    "CUSTOMER_SLAS = {\n",
    "    \"CUST-7891\": {\"tier\": \"Premium\", \"response_time\": \"4 hours\", \"replacement_policy\": \"Free expedited replacement\"},\n",
    "    \"CUST-1234\": {\"tier\": \"Standard\", \"response_time\": \"24 hours\", \"replacement_policy\": \"Standard replacement\"},\n",
    "}\n",
    "\n",
    "# Simulated ticket database\n",
    "TICKET_STATUSES = {\n",
    "    \"TKT-2024-001\": {\"status\": \"Open\", \"priority\": \"High\", \"assigned_to\": \"Support Team\", \"last_update\": \"2024-01-15\"},\n",
    "    \"TKT-2024-002\": {\"status\": \"Resolved\", \"priority\": \"Low\", \"assigned_to\": \"Bot\", \"last_update\": \"2024-01-10\"},\n",
    "}\n",
    "\n",
    "@tool(name=\"lookup_customer_sla\", description=\"Look up a customer's SLA tier and policies\")\n",
    "def lookup_customer_sla(\n",
    "    customer_id: Annotated[str, Field(description=\"The customer ID to look up (e.g., CUST-7891)\")]\n",
    ") -> str:\n",
    "    \"\"\"Look up customer SLA information.\"\"\"\n",
    "    if customer_id in CUSTOMER_SLAS:\n",
    "        sla = CUSTOMER_SLAS[customer_id]\n",
    "        return f\"Customer {customer_id}: {sla['tier']} tier, {sla['response_time']} response time, {sla['replacement_policy']}\"\n",
    "    return f\"Customer {customer_id} not found in system.\"\n",
    "\n",
    "@tool(name=\"get_incident_status\", description=\"Get the current status of a support ticket\")\n",
    "def get_incident_status(\n",
    "    ticket_id: Annotated[str, Field(description=\"The ticket ID to check (e.g., TKT-2024-001)\")]\n",
    ") -> str:\n",
    "    \"\"\"Get ticket status information.\"\"\"\n",
    "    if ticket_id in TICKET_STATUSES:\n",
    "        ticket = TICKET_STATUSES[ticket_id]\n",
    "        return f\"Ticket {ticket_id}: Status={ticket['status']}, Priority={ticket['priority']}, Assigned to={ticket['assigned_to']}, Last update={ticket['last_update']}\"\n",
    "    return f\"Ticket {ticket_id} not found in system.\"\n",
    "\n",
    "print(\"\u2705 Support tools defined: lookup_customer_sla, get_incident_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ee9eb",
   "metadata": {},
   "source": [
    "## Attach Tools to Agent\n",
    "\n",
    "Pass tools when creating the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create support agent with tools\n",
    "support_agent_with_tools = chat_client.as_agent(\n",
    "    name=\"SupportAgentWithTools\",\n",
    "    instructions=\"\"\"You are a customer support agent with access to internal systems.\n",
    "When handling emails:\n",
    "1. Look up the customer's SLA tier to understand their service level\n",
    "2. Check ticket status if a ticket ID is mentioned\n",
    "3. Use this information to provide appropriate responses and set expectations\n",
    "\n",
    "Always be empathetic and use the customer's SLA tier to guide your response (e.g., Premium customers get expedited service).\"\"\",\n",
    "    tools=[lookup_customer_sla, get_incident_status]\n",
    ")\n",
    "\n",
    "print(\"\u2705 support_agent_with_tools created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461f75e",
   "metadata": {},
   "source": [
    "## Execute with Tools\n",
    "\n",
    "The agent autonomously decides when to invoke tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cff641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the legitimate email that has customer_id and ticket_id\n",
    "prompt = f\"\"\"Handle this customer support email. Look up their SLA and ticket status first:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "Customer ID: {LEGIT_EMAIL.customer_id}\n",
    "Ticket ID: {LEGIT_EMAIL.ticket_id}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "\n",
    "result = await support_agent_with_tools.run(prompt)\n",
    "print(\"\ud83d\udce7 Response (with tool lookups):\\n\")\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90598d46",
   "metadata": {},
   "source": [
    "# 4.1. V1.2 \u2014 Multimodal Input (InboxOps Visual Support)\n",
    "\n",
    "## The Problem: Customers Send Screenshots\n",
    "\n",
    "InboxOps customers often attach **error screenshots** instead of describing problems in text:\n",
    "\n",
    "> \"My checkout isn't working\" + \ud83d\uddbc\ufe0f `error_screenshot.png`\n",
    "\n",
    "Our agents need to understand images, PDFs, and attachments to provide accurate support.\n",
    "\n",
    "## Solution: Multimodal Content\n",
    "\n",
    "The Agent Framework supports multimodal input using `Content` objects:\n",
    "\n",
    "Let's enable our Support Agent to handle customer screenshots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a specialized Multimodal Support Agent for handling visual issues\n",
    "multimodal_support_agent = chat_client.as_agent(\n",
    "    name=\"MultimodalSupportAgent\",\n",
    "    instructions=\"\"\"You are a specialized customer support agent with expertise in visual issue diagnosis.\n",
    "\n",
    "IMPORTANT: When you receive an image, you MUST:\n",
    "1. Acknowledge that you can see the image\n",
    "2. Describe what you observe in the screenshot\n",
    "3. Identify any error messages visible\n",
    "4. Provide specific troubleshooting steps based on what you see\n",
    "\n",
    "Your responsibilities:\n",
    "- Analyze both textual descriptions and visual evidence (screenshots, images)\n",
    "- Identify the exact error or problem from the visual content\n",
    "- Provide step-by-step resolution instructions\n",
    "- Consider visual context when recommending solutions\n",
    "- Prioritize urgent issues and offer temporary workarounds\n",
    "\n",
    "Be empathetic, solution-focused, and clear in your guidance.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\u2705 multimodal_support_agent created\")\n",
    "\n",
    "# Load the customer's screenshot from local images folder\n",
    "image_path = \"images/customer_image.png\"\n",
    "\n",
    "print(\"\\n\ud83d\udce7 Email with screenshot received...\")\n",
    "print(f\"\ud83d\udcce Attachment: {image_path}\")\n",
    "\n",
    "# Load the image from file\n",
    "with open(image_path, \"rb\") as f:\n",
    "    image_bytes = f.read()\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create a multimodal message with the local image\n",
    "from agent_framework import ChatMessage, Content, Role\n",
    "\n",
    "multimodal_message = ChatMessage(\n",
    "    role=Role.USER,\n",
    "    contents=[\n",
    "        Content.from_text(text=\"What error do you see in this checkout screenshot? Describe the issue and provide troubleshooting steps.\"),\n",
    "        Content.from_data(data=image_bytes, media_type=\"image/png\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run the specialized multimodal support agent - pass as messages list\n",
    "print(\"\ud83e\udd16 Multimodal Support Agent analyzing email and screenshot...\\n\")\n",
    "result = await multimodal_support_agent.run(messages=[multimodal_message])\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f181ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69c6311d",
   "metadata": {},
   "source": [
    "# 4.2. V1.3 \u2014 Structured Output (InboxOps Ticket Metadata)\n",
    "\n",
    "## The Problem: Downstream Systems Need JSON\n",
    "\n",
    "After the agent drafts a response, InboxOps needs to:\n",
    "- Create a ticket in the CRM with structured metadata\n",
    "- Log priority, category, sentiment\n",
    "- Route to the correct team\n",
    "\n",
    "**Free-text agent output is hard to parse reliably.**\n",
    "\n",
    "## Solution: Structured Output with Pydantic\n",
    "\n",
    "Use `response_format` to enforce a JSON schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class TicketMetadata(BaseModel):\n",
    "    priority: str  # \"low\", \"medium\", \"high\", \"urgent\"\n",
    "    category: str  # \"order\", \"refund\", \"technical\", etc.\n",
    "    sentiment: str  # \"positive\", \"neutral\", \"negative\"\n",
    "    estimated_resolution_time: str\n",
    "\n",
    "agent = chat_client.as_agent(\n",
    "    name=\"TicketMetadataExtractor\",\n",
    "    response_format=TicketMetadata  # Force structured output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed588fa",
   "metadata": {},
   "source": [
    "Let's extract ticket metadata automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define ticket metadata schema\n",
    "class TicketMetadata(BaseModel):\n",
    "    \"\"\"Structured metadata for InboxOps support tickets\"\"\"\n",
    "    priority: str = Field(description=\"Priority level: low, medium, high, or urgent\")\n",
    "    category: str = Field(description=\"Ticket category: order, refund, technical, shipping, account, other\")\n",
    "    sentiment: str = Field(description=\"Customer sentiment: positive, neutral, or negative\")\n",
    "    estimated_resolution_time: str = Field(description=\"Estimated time to resolve (e.g., '1 hour', '24 hours', '3-5 days')\")\n",
    "    requires_human_review: bool = Field(description=\"Whether this ticket needs escalation to a human agent\")\n",
    "\n",
    "# Create a metadata extraction agent\n",
    "metadata_agent = chat_client.as_agent(\n",
    "    name=\"TicketMetadataExtractor\",\n",
    "    instructions=\"\"\"You are an InboxOps ticket classification system.\n",
    "    Extract structured metadata from customer support emails.\n",
    "    Be accurate and consistent with your classifications.\n",
    "    \n",
    "    You must return JSON with these exact fields:\n",
    "    - priority: low, medium, high, or urgent\n",
    "    - category: order, refund, technical, shipping, account, or other\n",
    "    - sentiment: positive, neutral, or negative\n",
    "    - estimated_resolution_time: estimated time like \"1 hour\", \"24 hours\", \"3-5 days\"\n",
    "    - requires_human_review: true or false\"\"\",\n",
    "    response_format=TicketMetadata  # Enforce structured output\n",
    ")\n",
    "\n",
    "# Test with the legitimate email\n",
    "test_email = LEGIT_EMAIL.body\n",
    "\n",
    "print(\"\ud83d\udce7 Extracting metadata from email...\\n\")\n",
    "result = await metadata_agent.run(test_email)\n",
    "\n",
    "# Debug: Show what the agent returned\n",
    "print(\"\ud83d\udd0d Raw agent output:\")\n",
    "print(result.text)\n",
    "print()\n",
    "\n",
    "# Parse the structured output\n",
    "metadata = TicketMetadata.model_validate_json(result.text)\n",
    "\n",
    "print(\"\ud83d\udcca TICKET METADATA\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Priority:              {metadata.priority}\")\n",
    "print(f\"Category:              {metadata.category}\")\n",
    "print(f\"Sentiment:             {metadata.sentiment}\")\n",
    "print(f\"Est. Resolution Time:  {metadata.estimated_resolution_time}\")\n",
    "print(f\"Needs Human Review:    {metadata.requires_human_review}\")\n",
    "print(\"\\n\u2705 Structured output ready for CRM ingestion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd7b8a",
   "metadata": {},
   "source": [
    "# 4.3. V1.4 \u2014 MCP Integration (InboxOps External Tool Connections)\n",
    "\n",
    "## The Problem: Need to Connect External Systems\n",
    "\n",
    "InboxOps uses **Zendesk** for ticketing, **Shopify** for orders, and **Stripe** for payments.\n",
    "\n",
    "Instead of building custom API wrappers for each system, we can use **Model Context Protocol (MCP)** to connect agents to external tools.\n",
    "\n",
    "## Solution: MCP Tools\n",
    "\n",
    "MCP provides a standardized way to expose tools from external systems:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad459d0f",
   "metadata": {},
   "source": [
    "Let's simulate connecting to a Zendesk-like ticket system via MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import ChatAgent, HostedMCPTool, ChatMessage\n",
    "\n",
    "# Recreate the MCP tool with auto-approval\n",
    "learn_mcp_tool = HostedMCPTool(\n",
    "    name=\"MicrosoftLearn\",\n",
    "    url=\"https://learn.microsoft.com/api/mcp\",\n",
    "    approval_mode=\"never_require\"  # Auto-approve MCP tool calls\n",
    ")\n",
    "\n",
    "# Create the agent with the new tool\n",
    "mcp_support_agent = ChatAgent(\n",
    "    chat_client=chat_client_mcp,\n",
    "    name=\"MCPSupportAgent\",\n",
    "    instructions=\"\"\"You are a documentation assistant agent with access to Microsoft Learn documentation via MCP. \n",
    "When asked about Azure features, you MUST use the MCP tool to search for information.\"\"\",\n",
    "    tools=[learn_mcp_tool],\n",
    ")\n",
    "\n",
    "# Test: Ask a very specific recent question that requires the MCP tool\n",
    "test_request = \"\"\"\n",
    "A customer is asking: \"What are the latest Azure AI Foundry features announced in January 2026?\"\n",
    "\n",
    "You MUST use the MCP tool to search for this information.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\ud83d\udd0c MCP Agent with Microsoft Learn tool connection...\")\n",
    "print(f\"Request: {test_request}\\n\")\n",
    "\n",
    "# Run with auto-approval loop\n",
    "from agent_framework import AgentThread\n",
    "\n",
    "thread = AgentThread()\n",
    "max_approvals = 5  # Safety limit\n",
    "approval_count = 0\n",
    "\n",
    "result = await mcp_support_agent.run(test_request, thread=thread)\n",
    "\n",
    "# Handle approval requests automatically\n",
    "while approval_count < max_approvals:\n",
    "    # Check for approval requests\n",
    "    has_approval_request = False\n",
    "    approval_responses = []\n",
    "    \n",
    "    if hasattr(result, 'messages') and result.messages:\n",
    "        for msg in result.messages:\n",
    "            if hasattr(msg, 'contents'):\n",
    "                for content in msg.contents:\n",
    "                    if content.type == \"function_approval_request\":\n",
    "                        has_approval_request = True\n",
    "                        # Auto-approve by converting to approval response\n",
    "                        approval_response = content.to_function_approval_response(approved=True)\n",
    "                        tool_name = content.function_call.name if hasattr(content, 'function_call') and content.function_call else 'unknown'\n",
    "                        print(f\"\ud83d\udd10 Auto-approving MCP tool call: {tool_name}\")\n",
    "                        approval_responses.append(approval_response)\n",
    "    \n",
    "    if not has_approval_request:\n",
    "        break\n",
    "    \n",
    "    # Continue the conversation with approval responses wrapped in a message\n",
    "    approval_message = ChatMessage(role=\"tool\", contents=approval_responses)\n",
    "    result = await mcp_support_agent.run(\n",
    "        [approval_message],\n",
    "        thread=thread\n",
    "    )\n",
    "    approval_count += 1\n",
    "\n",
    "# Debug output\n",
    "print(f\"\\n\ud83d\udd0d Total approvals granted: {approval_count}\")\n",
    "print(f\"   Result text length: {len(result.text) if result.text else 0}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcdd Agent Response:\")\n",
    "print(\"=\"*60)\n",
    "print(result.text if result.text else \"(empty response)\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530d0d1",
   "metadata": {},
   "source": [
    "# 5. V2 \u2014 Human-in-the-Loop Approval (InboxOps Safety Gate)\n",
    "\n",
    "Drafting is safe.\n",
    "\n",
    "**Sending an email is not.**\n",
    "\n",
    "InboxOps policy:\n",
    "\u2705 AI may draft responses  \n",
    "\ud83d\udd12 A human must approve before sending  \n",
    "\n",
    "So we mark the sending tool as approval-required.\n",
    "\n",
    "This creates a safety mechanism:\n",
    "- The agent can propose the action\n",
    "- The platform pauses execution\n",
    "- A human confirms or rejects\n",
    "- Only then can the workflow continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479786af",
   "metadata": {},
   "source": [
    "## Approval-Required Action Tool\n",
    "\n",
    "We treat sending a reply as a sensitive business action.\n",
    "\n",
    "We set:\n",
    "\n",
    "`approval_mode=\"always_require\"`\n",
    "\n",
    "This ensures:\n",
    "- No accidental customer emails\n",
    "- No legal/compliance surprises\n",
    "- Brand safety for InboxOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cbdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import ChatMessage, Content, Role\n",
    "\n",
    "# Tool that requires human approval before sending\n",
    "@tool(approval_mode=\"always_require\", name=\"send_email_reply\", description=\"Send an email reply to the customer. Requires human approval.\")\n",
    "def send_email_reply(\n",
    "    to: Annotated[str, Field(description=\"Recipient email address\")],\n",
    "    subject: Annotated[str, Field(description=\"Email subject\")],\n",
    "    body: Annotated[str, Field(description=\"Email body content\")]\n",
    ") -> str:\n",
    "    \"\"\"Send an email reply to the customer. Requires human approval.\"\"\"\n",
    "    # In production, this would actually send the email\n",
    "    return f\"\u2705 Email sent to {to} with subject '{subject}'\"\n",
    "\n",
    "# Create agent with the approval-required tool\n",
    "approval_agent = chat_client.as_agent(\n",
    "    name=\"ApprovalSupportAgent\",\n",
    "    instructions=\"\"\"You are a customer support agent. When you finish drafting a response, \n",
    "you MUST call the send_email_reply tool to send it. Do not ask for permission - just call the tool.\n",
    "The system will automatically handle approval. Always use the tool to send your response.\"\"\",\n",
    "    tools=[lookup_customer_sla, get_incident_status, send_email_reply]\n",
    ")\n",
    "\n",
    "print(\"\u2705 approval_agent created with send_email_reply tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de90248",
   "metadata": {},
   "source": [
    "## Check for Pending Approvals\n",
    "\n",
    "Approval-required calls return `user_input_requests` instead of executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6063ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the agent to handle and send a response\n",
    "prompt = f\"\"\"Handle this email and propose sending the response using the send_email_reply tool.\n",
    "The platform will automatically require human approval before execution.\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "Customer ID: {LEGIT_EMAIL.customer_id}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "\n",
    "result = await approval_agent.run(prompt)\n",
    "\n",
    "# Check if approval is needed\n",
    "if result.user_input_requests:\n",
    "    print(\"\ud83d\udd12 APPROVAL REQUIRED!\")\n",
    "    for user_input_needed in result.user_input_requests:\n",
    "        print(f\"  Function: {user_input_needed.function_call.name}\")\n",
    "        print(f\"  Arguments: {user_input_needed.function_call.arguments}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No approval requested - agent didn't call the tool\")\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e80ab2",
   "metadata": {},
   "source": [
    "## Grant Approval\n",
    "\n",
    "Respond with `to_function_approval_response(True/False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa70478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Handling Approval ---\\n\")\n",
    "\n",
    "# Provide approval and continue the conversation\n",
    "if result.user_input_requests:\n",
    "    user_input_needed = result.user_input_requests[0]\n",
    "    \n",
    "    # Simulate human approval (in production, this would be interactive)\n",
    "    user_approval = True\n",
    "    print(f\"\u2705 Human approved: {user_approval}\\n\")\n",
    "    \n",
    "    # Create approval response message\n",
    "    approval_message = ChatMessage(\n",
    "        role=Role.USER,\n",
    "        contents=[user_input_needed.to_function_approval_response(user_approval)]\n",
    "    )\n",
    "    \n",
    "    # Continue with approval\n",
    "    final_result = await approval_agent.run([\n",
    "        prompt,\n",
    "        ChatMessage(role=Role.ASSISTANT, contents=[user_input_needed]),\n",
    "        approval_message\n",
    "    ])\n",
    "    print(f\"\ud83d\udcca Final Result:\\n{final_result.text}\")\n",
    "else:\n",
    "    print(\"\u274c No approval was requested in the previous cell.\")\n",
    "    print(\"   The agent needs to call the send_email_reply tool to trigger approval.\")\n",
    "    print(\"   Re-run the previous cell to try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbefd9d",
   "metadata": {},
   "source": [
    "# 6. V2.1 \u2014 Middleware (Observability for Production)\n",
    "\n",
    "InboxOps engineering asked the next obvious question:\n",
    "\n",
    "\"How do we monitor this system in production?\"\n",
    "\n",
    "They need:\n",
    "- execution timing\n",
    "- tool call logging\n",
    "- tracing / visibility for debugging\n",
    "- metrics for performance\n",
    "\n",
    "Middleware gives InboxOps **observability hooks** without rewriting agent code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8829240",
   "metadata": {},
   "source": [
    "## Define Middleware\n",
    "\n",
    "Middleware wraps execution with `context` and `next` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae080f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Awaitable\n",
    "from agent_framework import AgentRunContext, FunctionInvocationContext\n",
    "import time\n",
    "\n",
    "async def logging_agent_middleware(\n",
    "    context: AgentRunContext,\n",
    "    next: Callable[[AgentRunContext], Awaitable[None]],\n",
    ") -> None:\n",
    "    \"\"\"Log agent execution with timing.\"\"\"\n",
    "    print(f\"\ud83d\ude80 Agent starting... ({len(context.messages)} message(s))\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    await next(context)  # Continue to agent execution\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\u2705 Agent finished in {elapsed:.2f}s\")\n",
    "\n",
    "async def logging_function_middleware(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: Callable[[FunctionInvocationContext], Awaitable[None]],\n",
    ") -> None:\n",
    "    \"\"\"Log function tool calls.\"\"\"\n",
    "    print(f\"  \ud83d\udcde Calling: {context.function.name}({context.arguments})\")\n",
    "    \n",
    "    await next(context)\n",
    "    \n",
    "    print(f\"  \ud83d\udce4 Result: {context.result[:100]}...\" if len(str(context.result)) > 100 else f\"  \ud83d\udce4 Result: {context.result}\")\n",
    "\n",
    "print(\"\u2705 Middleware defined: logging_agent_middleware, logging_function_middleware\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e9a62",
   "metadata": {},
   "source": [
    "## Attach Middleware\n",
    "\n",
    "Pass middleware list when creating the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with middleware for logging\n",
    "middleware_agent = chat_client.as_agent(\n",
    "    name=\"LoggingSupportAgent\",\n",
    "    instructions=\"You are a support agent. Look up customer information when handling requests.\",\n",
    "    tools=[lookup_customer_sla, get_incident_status],\n",
    "    middleware=[logging_agent_middleware, logging_function_middleware]\n",
    ")\n",
    "\n",
    "# Test - you'll see logs for agent and function calls\n",
    "prompt = f\"Check the SLA for customer {LEGIT_EMAIL.customer_id} and ticket status for {LEGIT_EMAIL.ticket_id}\"\n",
    "result = await middleware_agent.run(prompt)\n",
    "print(f\"\\n\ud83d\udcac Response: {result.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1. V2.2 \u2014 Error Handling & Retry (InboxOps Resilience)\n",
    "\n",
    "## The Problem: External Systems Are Unreliable\n",
    "\n",
    "During Black Friday, InboxOps experiences:\n",
    "- **Zendesk API timeouts** (504 Gateway Timeout)\n",
    "- **OpenAI rate limiting** (429 Too Many Requests)\n",
    "- **Database connection drops** (network failures)\n",
    "\n",
    "Without retry logic, customers get cryptic errors and agents crash mid-conversation.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: Multi-Layered Error Handling\n",
    "\n",
    "InboxOps implements 3 resilience patterns:\n",
    "\n",
    "1. **Retry with Exponential Backoff** - Automatically retry transient failures\n",
    "2. **Timeout Protection** - Prevent hung requests from blocking the system\n",
    "3. **Circuit Breaker** - Stop calling failing services to prevent cascading failures\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution 1: Retry with Exponential Backoff\n",
    "# Uses tenacity library for production-grade retry logic\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "import random\n",
    "import asyncio\n",
    "\n",
    "@tool\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),  # Max 3 attempts\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=10),  # 1s, 2s, 4s delays\n",
    "    retry=retry_if_exception_type((TimeoutError, ConnectionError))\n",
    ")\n",
    "async def create_ticket_with_retry(\n",
    "    customer_id: Annotated[str, \"Customer ID\"],\n",
    "    subject: Annotated[str, \"Ticket subject\"],\n",
    "    description: Annotated[str, \"Ticket description\"]\n",
    ") -> dict:\n",
    "    \"\"\"Create a Zendesk ticket with automatic retry on transient failures.\"\"\"\n",
    "    \n",
    "    # Simulate API call that might fail (30% failure rate)\n",
    "    if random.random() < 0.3:\n",
    "        raise ConnectionError(\"Zendesk API temporarily unavailable\")\n",
    "    \n",
    "    ticket_id = f\"TKT-{random.randint(1000, 9999)}\"\n",
    "    print(f\"\u2705 Ticket {ticket_id} created successfully\")\n",
    "    \n",
    "    return {\n",
    "        \"ticket_id\": ticket_id,\n",
    "        \"customer_id\": customer_id,\n",
    "        \"subject\": subject,\n",
    "        \"status\": \"open\"\n",
    "    }\n",
    "\n",
    "# Test the retry logic\n",
    "print(\"\ud83d\udd04 Testing automatic retry...\")\n",
    "try:\n",
    "    result = await create_ticket_with_retry(\n",
    "        \"CUST-001\",\n",
    "        \"Website is slow\",\n",
    "        \"Customer reports 5+ second page load times\"\n",
    "    )\n",
    "    print(f\"Success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed after 3 retries: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution 2: Timeout Protection\n",
    "# Prevent hung requests from blocking the system\n",
    "\n",
    "from asyncio import timeout as asyncio_timeout\n",
    "import time\n",
    "\n",
    "async def safe_agent_call(agent, message: str, timeout_sec: int = 30) -> str:\n",
    "    \"\"\"Call agent with timeout protection.\"\"\"\n",
    "    try:\n",
    "        async with asyncio_timeout(timeout_sec):\n",
    "            result = await agent.run(message)\n",
    "            return result.text\n",
    "    except asyncio.TimeoutError:\n",
    "        return \"\u23f1\ufe0f Request timed out. The system is under heavy load. Please try again in a few moments.\"\n",
    "    except Exception as e:\n",
    "        return f\"\u274c Error: {str(e)}\"\n",
    "\n",
    "# Test timeout protection\n",
    "print(\"Testing timeout protection...\")\n",
    "\n",
    "# Create a test agent\n",
    "test_agent = ChatAgent(\n",
    "    chat_client=client,\n",
    "    name=\"TestAgent\",\n",
    "    instructions=\"You are a test agent.\"\n",
    ")\n",
    "\n",
    "# This should complete normally\n",
    "response = await safe_agent_call(\n",
    "    test_agent,\n",
    "    \"Say hello\",\n",
    "    timeout_sec=10\n",
    ")\n",
    "print(f\"Response: {response[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution 3: Circuit Breaker Pattern\n",
    "# Stop calling failing services to prevent cascading failures\n",
    "\n",
    "from typing import Optional, Callable\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker to prevent cascading failures.\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):\n",
    "        self.failure_count = 0\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.recovery_timeout = recovery_timeout\n",
    "        self.last_failure_time: Optional[datetime] = None\n",
    "        self.is_open = False\n",
    "    \n",
    "    def call(self, func: Callable):\n",
    "        \"\"\"Execute function with circuit breaker protection.\"\"\"\n",
    "        if self.is_open:\n",
    "            # Check if recovery timeout has passed\n",
    "            if (datetime.now() - self.last_failure_time).seconds > self.recovery_timeout:\n",
    "                print(\"\ud83d\udd27 Circuit breaker attempting to close...\")\n",
    "                self.is_open = False\n",
    "                self.failure_count = 0\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f\"\u26a0\ufe0f Circuit breaker is OPEN - service unavailable. \"\n",
    "                    f\"Retry after {self.recovery_timeout} seconds.\"\n",
    "                )\n",
    "        \n",
    "        try:\n",
    "            result = func()\n",
    "            self.failure_count = 0  # Reset on success\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.failure_count += 1\n",
    "            self.last_failure_time = datetime.now()\n",
    "            print(f\"\u274c Failure #{self.failure_count}: {e}\")\n",
    "            \n",
    "            if self.failure_count >= self.failure_threshold:\n",
    "                self.is_open = True\n",
    "                print(f\"\ud83d\udea8 Circuit breaker OPENED after {self.failure_count} failures\")\n",
    "            \n",
    "            raise e\n",
    "\n",
    "# Create circuit breaker for customer database\n",
    "customer_db_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)\n",
    "\n",
    "@tool\n",
    "def get_customer_data_protected(\n",
    "    customer_id: Annotated[str, \"Customer ID\"]\n",
    ") -> dict:\n",
    "    \"\"\"Get customer data with circuit breaker protection.\"\"\"\n",
    "    \n",
    "    def _fetch():\n",
    "        # Simulate database query (sometimes fails)\n",
    "        if random.random() < 0.4:  # 40% failure rate\n",
    "            raise ConnectionError(\"Database connection timeout\")\n",
    "        \n",
    "        return {\n",
    "            \"customer_id\": customer_id,\n",
    "            \"tier\": \"premium\",\n",
    "            \"email\": f\"{customer_id}@example.com\"\n",
    "        }\n",
    "    \n",
    "    return customer_db_breaker.call(_fetch)\n",
    "\n",
    "# Test circuit breaker\n",
    "print(\"Testing circuit breaker pattern...\")\n",
    "print(\"Making 10 rapid requests to a flaky service:\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        result = get_customer_data_protected(\"CUST-123\")\n",
    "        print(f\"\u2705 Request {i+1}: Success\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Request {i+1}: {str(e)[:80]}\")\n",
    "    \n",
    "    time.sleep(0.5)  # Small delay between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting It All Together: Resilient Agent\n",
    "\n",
    "resilient_agent = ChatAgent(\n",
    "    chat_client=client,\n",
    "    name=\"ResilientInboxOps\",\n",
    "    instructions=\"\"\"\n",
    "    You are InboxOps customer support with enterprise-grade resilience.\n",
    "    Use tools to help customers. If a tool fails, retry automatically.\n",
    "    If all retries fail, apologize and provide next steps.\n",
    "    \"\"\",\n",
    "    tools=[\n",
    "        create_ticket_with_retry,  # Auto-retry on failures\n",
    "        get_customer_data_protected  # Circuit breaker protection\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Test resilient agent\n",
    "print(\"Testing resilient agent with flaky tools...\\n\")\n",
    "\n",
    "test_message = \"\"\"\n",
    "Customer CUST-999 is reporting slow website performance.\n",
    "Look up their account details and create a support ticket.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    response = await safe_agent_call(\n",
    "        resilient_agent,\n",
    "        test_message,\n",
    "        timeout_sec=60  # Allow time for retries\n",
    "    )\n",
    "    print(f\"\\n\u2705 Agent Response:\\n{response}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2705 V2.2 Complete: InboxOps can now handle API failures gracefully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568cf38",
   "metadata": {},
   "source": [
    "# 6.2. V2.3 \u2014 Rate Limiting (InboxOps Black Friday Protection)\n",
    "\n",
    "## The Problem: Email Surges Overwhelm the System\n",
    "\n",
    "**Black Friday scenario:**\n",
    "- Normal load: 1,000 emails/hour\n",
    "- Black Friday: **100,000 emails/hour** \ud83d\udd25\n",
    "\n",
    "Without rate limiting:\n",
    "- OpenAI API quota exhausted\n",
    "- $10,000+ in unexpected costs\n",
    "- System crashes\n",
    "\n",
    "## Solution: Token Bucket Rate Limiting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9013361e",
   "metadata": {},
   "source": [
    "Let's protect InboxOps from traffic surges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d19348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from typing import Callable, Awaitable\n",
    "from agent_framework import ChatAgent, AgentRunContext\n",
    "\n",
    "# Simple rate limiter using token bucket algorithm\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_requests: int, time_window: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_requests: Maximum requests allowed in time window\n",
    "            time_window: Time window in seconds\n",
    "        \"\"\"\n",
    "        self.max_requests = max_requests\n",
    "        self.time_window = time_window\n",
    "        self.requests = []\n",
    "    \n",
    "    def allow_request(self) -> bool:\n",
    "        \"\"\"Check if request is allowed under rate limit\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        # Remove old requests outside time window\n",
    "        self.requests = [req_time for req_time in self.requests \n",
    "                        if now - req_time < self.time_window]\n",
    "        \n",
    "        # Check if under limit\n",
    "        if len(self.requests) < self.max_requests:\n",
    "            self.requests.append(now)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_wait_time(self) -> float:\n",
    "        \"\"\"Get time to wait before next request is allowed\"\"\"\n",
    "        if not self.requests:\n",
    "            return 0.0\n",
    "        \n",
    "        oldest_request = min(self.requests)\n",
    "        time_passed = time.time() - oldest_request\n",
    "        return max(0.0, self.time_window - time_passed)\n",
    "\n",
    "# Create rate limiter: 3 requests per 5 seconds (stricter for demo)\n",
    "rate_limiter = RateLimiter(max_requests=3, time_window=5.0)\n",
    "\n",
    "# Middleware function for rate limiting with proper signature\n",
    "async def rate_limit_middleware(\n",
    "    context: AgentRunContext,\n",
    "    next: Callable[[AgentRunContext], Awaitable[None]]\n",
    ") -> None:\n",
    "    \"\"\"Middleware that enforces rate limits\"\"\"\n",
    "    if not rate_limiter.allow_request():\n",
    "        wait_time = rate_limiter.get_wait_time()\n",
    "        error_msg = f\"\u26a0\ufe0f RATE LIMIT EXCEEDED. Wait {wait_time:.1f}s before next request.\"\n",
    "        print(error_msg)\n",
    "        # In production, you might queue the request or return an error\n",
    "        # For demo, we'll wait the required time\n",
    "        print(f\"   \u23f3 Waiting {wait_time:.1f}s...\")\n",
    "        await asyncio.sleep(wait_time)\n",
    "        # After waiting, allow the request\n",
    "        rate_limiter.allow_request()\n",
    "    \n",
    "    await next(context)\n",
    "\n",
    "# Create rate-limited agent\n",
    "rate_limited_agent = chat_client.as_agent(\n",
    "    name=\"RateLimitedSupportAgent\",\n",
    "    instructions=\"You are an InboxOps support agent. Answer briefly in 1-2 sentences.\",\n",
    "    middleware=[rate_limit_middleware]\n",
    ")\n",
    "\n",
    "# Simulate Black Friday email surge\n",
    "print(\"\ud83d\udecd\ufe0f BLACK FRIDAY EMAIL SURGE SIMULATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Rate Limit: 3 requests per 5 seconds\")\n",
    "print(\"Simulating rapid-fire emails to trigger rate limiting...\\n\")\n",
    "\n",
    "async def simulate_email_surge():\n",
    "    emails = [\n",
    "        \"Where is my order #12345?\",\n",
    "        \"I need a refund for order #67890\",\n",
    "        \"Is the sale still active?\",\n",
    "        \"My promo code isn't working\",\n",
    "        \"When will item XYZ be back in stock?\",\n",
    "        \"I can't log into my account\",\n",
    "        \"Need help with shipping address\",\n",
    "        \"Is free shipping available?\",\n",
    "    ]\n",
    "    \n",
    "    for i, email in enumerate(emails, 1):\n",
    "        print(f\"\ud83d\udce7 Email {i}/{len(emails)}: {email[:40]}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Don't await run_stream - it returns an async generator\n",
    "        result = rate_limited_agent.run_stream(email)\n",
    "        \n",
    "        # Consume stream\n",
    "        response = \"\"\n",
    "        async for chunk in result:\n",
    "            if hasattr(chunk, 'delta') and chunk.delta:\n",
    "                response += chunk.delta\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"   \u2705 Response: {response[:60]}...\")\n",
    "        print(f\"   \u23f1\ufe0f  Processed in {elapsed:.2f}s\\n\")\n",
    "        \n",
    "        # NO delay between requests - fire them as fast as possible to trigger rate limit!\n",
    "        # await asyncio.sleep(0.5)\n",
    "\n",
    "# Run the simulation\n",
    "await simulate_email_surge()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2705 Rate limiting protected the system from being overwhelmed!\")\n",
    "print(\"\ud83d\udca1 In production, rate-limited requests would be queued or delayed.\")\n",
    "print(f\"\ud83d\udcca Total requests in rate limiter: {len(rate_limiter.requests)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced2740",
   "metadata": {},
   "source": [
    "# 6.3. V2.4 \u2014 Caching (InboxOps FAQ Optimization)\n",
    "\n",
    "## The Problem: Repetitive Questions Waste API Calls\n",
    "\n",
    "InboxOps receives the same questions repeatedly:\n",
    "- \"What's your return policy?\" (asked 500 times/day)\n",
    "- \"Do you ship internationally?\" (asked 300 times/day)\n",
    "- \"How do I reset my password?\" (asked 200 times/day)\n",
    "\n",
    "**Each question costs $0.002 in API calls = $2,000/day wasted!**\n",
    "\n",
    "## Solution: Response Caching\n",
    "\n",
    "Cache responses for common questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def get_cached_response(message: str) -> str | None:\n",
    "    cache_key = hashlib.sha256(message.encode()).hexdigest()\n",
    "    \n",
    "    if cache_key in cache:\n",
    "        cached_item = cache[cache_key]\n",
    "        if time.time() - cached_item['timestamp'] < TTL:\n",
    "            return cached_item['response']  # Cache hit!\n",
    "    \n",
    "    return None  # Cache miss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b80dad",
   "metadata": {},
   "source": [
    "Let's reduce InboxOps API costs with smart caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb74af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "from typing import Optional\n",
    "from agent_framework import ChatAgent, Context\n",
    "\n",
    "# Simple in-memory cache with TTL (Time To Live)\n",
    "class ResponseCache:\n",
    "    def __init__(self, ttl_seconds: int = 3600):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ttl_seconds: Cache TTL in seconds (default: 1 hour)\n",
    "        \"\"\"\n",
    "        self.cache = {}\n",
    "        self.ttl = ttl_seconds\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _get_cache_key(self, message: str) -> str:\n",
    "        \"\"\"Generate cache key from message\"\"\"\n",
    "        # Normalize message (lowercase, strip whitespace)\n",
    "        normalized = message.lower().strip()\n",
    "        return hashlib.sha256(normalized.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, message: str) -> Optional[str]:\n",
    "        \"\"\"Get cached response if available and not expired\"\"\"\n",
    "        cache_key = self._get_cache_key(message)\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            cached_item = self.cache[cache_key]\n",
    "            age = time.time() - cached_item['timestamp']\n",
    "            \n",
    "            if age < self.ttl:\n",
    "                self.hits += 1\n",
    "                print(f\"   \ud83d\udcb0 CACHE HIT! (age: {age:.1f}s, saved $0.002)\")\n",
    "                return cached_item['response']\n",
    "            else:\n",
    "                # Expired, remove from cache\n",
    "                del self.cache[cache_key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, message: str, response: str):\n",
    "        \"\"\"Cache a response\"\"\"\n",
    "        cache_key = self._get_cache_key(message)\n",
    "        self.cache[cache_key] = {\n",
    "            'response': response,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'hits': self.hits,\n",
    "            'misses': self.misses,\n",
    "            'hit_rate': f\"{hit_rate:.1f}%\",\n",
    "            'size': len(self.cache),\n",
    "            'estimated_savings': f\"${self.hits * 0.002:.2f}\"\n",
    "        }\n",
    "\n",
    "# Create cache\n",
    "response_cache = ResponseCache(ttl_seconds=300)  # 5 minute TTL\n",
    "\n",
    "# Middleware for caching\n",
    "async def cache_middleware(messages: list, context: Context, next_fn):\n",
    "    \"\"\"Middleware that caches agent responses\"\"\"\n",
    "    if not messages:\n",
    "        return await next_fn(messages, context)\n",
    "    \n",
    "    last_message = messages[-1]\n",
    "    user_message = last_message.get('content', '') if isinstance(last_message, dict) else str(last_message)\n",
    "    \n",
    "    # Try to get cached response\n",
    "    cached_response = response_cache.get(user_message)\n",
    "    if cached_response:\n",
    "        # Return cached response without calling LLM\n",
    "        from agent_framework import ChatMessage, Role\n",
    "        return type('CachedResult', (), {\n",
    "            'output': cached_response,\n",
    "            'messages': [ChatMessage(role=Role.ASSISTANT, content=cached_response)]\n",
    "        })()\n",
    "    \n",
    "    # Cache miss - call the agent\n",
    "    result = await next_fn(messages, context)\n",
    "    \n",
    "    # Cache the response\n",
    "    if hasattr(result, 'output'):\n",
    "        response_cache.set(user_message, result.output)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create cached agent\n",
    "cached_agent = ChatAgent(\n",
    "    name=\"CachedSupportAgent\",\n",
    "    model_client=model,\n",
    "    instructions=\"\"\"You are an InboxOps support agent. Provide concise, helpful answers to customer questions.\"\"\",\n",
    "    middleware=[cache_middleware]\n",
    ")\n",
    "\n",
    "# Test with frequently asked questions\n",
    "faq_questions = [\n",
    "    \"What is your return policy?\",\n",
    "    \"Do you ship internationally?\",\n",
    "    \"What is your return policy?\",  # Duplicate - should hit cache\n",
    "    \"How do I reset my password?\",\n",
    "    \"Do you ship internationally?\",  # Duplicate - should hit cache\n",
    "    \"What is your return policy?\",  # Duplicate - should hit cache\n",
    "]\n",
    "\n",
    "print(\"\ud83e\uddea TESTING FAQ CACHING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "async def test_caching():\n",
    "    for i, question in enumerate(faq_questions, 1):\n",
    "        print(f\"\\n\ud83d\udce7 Question {i}: {question}\")\n",
    "        \n",
    "        result = await cached_agent.run_stream(\n",
    "            messages=[{\"role\": \"user\", \"content\": question}]\n",
    "        )\n",
    "        \n",
    "        # Consume stream\n",
    "        response = \"\"\n",
    "        async for chunk in result.stream:\n",
    "            if hasattr(chunk, 'delta') and chunk.delta:\n",
    "                response += chunk.delta\n",
    "        \n",
    "        if not response_cache.get(question):  # If wasn't cached before\n",
    "            print(f\"   \ud83d\udd04 Generated new response\")\n",
    "\n",
    "await test_caching()\n",
    "\n",
    "# Show cache statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udcca CACHE STATISTICS\")\n",
    "stats = response_cache.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n\u2705 Caching dramatically reduces API costs for repetitive questions!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffbde4",
   "metadata": {},
   "source": [
    "# 7. V3 \u2014 Memory That Survives Beyond a Single Thread\n",
    "\n",
    "Threads remember a conversation.\n",
    "\n",
    "But InboxOps also needs persistent preferences across conversations, such as:\n",
    "- preferred language\n",
    "- preferred tone\n",
    "- customer name\n",
    "\n",
    "Example:\n",
    "A customer always wants **brief responses**.\n",
    "Or requests replies in **Hebrew**.\n",
    "Or is a VIP account.\n",
    "\n",
    "This is where a ContextProvider-based memory layer becomes powerful:\n",
    "\u2705 Extract preferences automatically  \n",
    "\u2705 Inject them as context into future calls  \n",
    "\u2705 Maintain consistent customer experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ced48",
   "metadata": {},
   "source": [
    "## Preferences Model\n",
    "\n",
    "Define what to remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3468ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportPreferences(BaseModel):\n",
    "    \"\"\"User preferences for support interactions.\"\"\"\n",
    "    name: str | None = None\n",
    "    preferred_language: Literal[\"English\", \"Hebrew\", \"Spanish\"] = \"English\"\n",
    "    preferred_tone: Literal[\"formal\", \"friendly\", \"brief\"] = \"formal\"\n",
    "\n",
    "print(\"\u2705 SupportPreferences model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdd4af",
   "metadata": {},
   "source": [
    "## Implement ContextProvider\n",
    "\n",
    "Two methods: `invoking` (inject context before calls) and `invoked` (extract state after calls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082eefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import MutableSequence, Sequence\n",
    "from typing import Any\n",
    "\n",
    "from agent_framework import ContextProvider, Context, ChatAgent, ChatOptions\n",
    "\n",
    "\n",
    "class SupportMemory(ContextProvider):\n",
    "    \"\"\"Memory that tracks user preferences for support interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self, chat_client, preferences: SupportPreferences | None = None, **kwargs: Any):\n",
    "        \"\"\"Create the memory.\n",
    "        \n",
    "        Args:\n",
    "            chat_client: The chat client to use for extracting structured data\n",
    "            preferences: Optional initial preferences\n",
    "            **kwargs: Additional keyword arguments for deserialization\n",
    "        \"\"\"\n",
    "        self._chat_client = chat_client\n",
    "        if preferences:\n",
    "            self.preferences = preferences\n",
    "        elif kwargs:\n",
    "            self.preferences = SupportPreferences.model_validate(kwargs)\n",
    "        else:\n",
    "            self.preferences = SupportPreferences()\n",
    "    \n",
    "    async def invoked(\n",
    "        self,\n",
    "        request_messages: ChatMessage | Sequence[ChatMessage],\n",
    "        response_messages: ChatMessage | Sequence[ChatMessage] | None = None,\n",
    "        invoke_exception: Exception | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Extract preferences from user messages after each call.\"\"\"\n",
    "        # Ensure request_messages is a list\n",
    "        messages_list = [request_messages] if isinstance(request_messages, ChatMessage) else list(request_messages)\n",
    "        \n",
    "        # Check if we have user messages\n",
    "        user_messages = [msg for msg in messages_list if msg.role.value == \"user\"]\n",
    "        \n",
    "        if user_messages:\n",
    "            try:\n",
    "                # Use the chat client to extract structured information\n",
    "                # NOTE: Use `options=` not `chat_options=`\n",
    "                result = await self._chat_client.get_response(\n",
    "                    messages=messages_list,\n",
    "                    options=ChatOptions(\n",
    "                        instructions=(\n",
    "                            \"Extract the user's name, preferred tone (formal/friendly/brief), \"\n",
    "                            \"and preferred language (English/Hebrew/Spanish) from the messages if present. \"\n",
    "                            \"If not present, return None for that field.\"\n",
    "                        ),\n",
    "                        response_format=SupportPreferences,\n",
    "                    ),\n",
    "                )\n",
    "                \n",
    "                # result.value should now be a SupportPreferences instance\n",
    "                extracted = result.value\n",
    "                \n",
    "                # Update preferences with extracted data\n",
    "                if extracted and isinstance(extracted, SupportPreferences):\n",
    "                    if self.preferences.name is None and extracted.name:\n",
    "                        self.preferences.name = extracted.name\n",
    "                        print(f\"   \ud83e\udde0 Memory updated: name = {extracted.name}\")\n",
    "                    \n",
    "                    if extracted.preferred_tone != \"formal\":  # formal is default\n",
    "                        self.preferences.preferred_tone = extracted.preferred_tone\n",
    "                        print(f\"   \ud83e\udde0 Memory updated: tone = {extracted.preferred_tone}\")\n",
    "                    \n",
    "                    if extracted.preferred_language != \"English\":  # English is default\n",
    "                        self.preferences.preferred_language = extracted.preferred_language\n",
    "                        print(f\"   \ud83e\udde0 Memory updated: language = {extracted.preferred_language}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   \u26a0\ufe0f Failed to extract preferences: {e}\")\n",
    "    \n",
    "    async def invoking(self, messages: ChatMessage | MutableSequence[ChatMessage], **kwargs: Any) -> Context:\n",
    "        \"\"\"Provide preference context before each agent call.\"\"\"\n",
    "        instructions: list[str] = []\n",
    "        \n",
    "        if self.preferences.name:\n",
    "            instructions.append(f\"The user's name is {self.preferences.name}. Address them by name.\")\n",
    "        \n",
    "        instructions.append(f\"Respond in {self.preferences.preferred_language}.\")\n",
    "        instructions.append(f\"Use a {self.preferences.preferred_tone} tone.\")\n",
    "        \n",
    "        return Context(instructions=\" \".join(instructions))\n",
    "    \n",
    "    def serialize(self) -> str:\n",
    "        \"\"\"Serialize for persistence.\"\"\"\n",
    "        return self.preferences.model_dump_json()\n",
    "\n",
    "print(\"\u2705 SupportMemory ContextProvider defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d6fd8",
   "metadata": {},
   "source": [
    "## Test Memory\n",
    "\n",
    "The agent automatically extracts and applies preferences across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the memory provider using the existing chat_client\n",
    "support_memory = SupportMemory(chat_client)\n",
    "\n",
    "# Create the agent with memory\n",
    "memory_agent = ChatAgent(\n",
    "    name=\"MemorySupportAgent\",\n",
    "    instructions=\"You are a friendly support agent. Adapt your responses based on user preferences.\",\n",
    "    chat_client=chat_client,\n",
    "    context_provider=support_memory,\n",
    ")\n",
    "\n",
    "# Turn 1: User introduces themselves\n",
    "print(\"Turn 1: User introduction\")\n",
    "print(\"-\" * 50)\n",
    "result1 = await memory_agent.run(\"Hi, my name is David\")\n",
    "print(f\"Agent: {result1.text}\\n\")\n",
    "\n",
    "# Turn 2: User sets preference\n",
    "print(\"Turn 2: Setting preference\")\n",
    "print(\"-\" * 50)\n",
    "result2 = await memory_agent.run(\"Please keep responses brief and casual\")\n",
    "print(f\"Agent: {result2.text}\\n\")\n",
    "\n",
    "# Turn 3: Ask a question - memory should apply name and brief tone\n",
    "print(\"Turn 3: Question with preferences applied\")\n",
    "print(\"-\" * 50)\n",
    "result3 = await memory_agent.run(\"What's your return policy?\")\n",
    "print(f\"Agent: {result3.text}\\n\")\n",
    "\n",
    "# Check memory state - access the original support_memory object directly\n",
    "print(\"\ud83e\udde0 Memory State (tracked by ContextProvider):\")\n",
    "print(f\"   Name: {support_memory.preferences.name}\")\n",
    "print(f\"   Language: {support_memory.preferences.preferred_language}\")\n",
    "print(f\"   Tone: {support_memory.preferences.preferred_tone}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8a13c",
   "metadata": {},
   "source": [
    "# Workflows: InboxOps Pipeline Automation\n",
    "\n",
    "At scale, InboxOps realized something important:\n",
    "\n",
    "**A single agent loop is not enough.**\n",
    "\n",
    "They need repeatable, testable execution paths:\n",
    "- classify \u2192 draft \u2192 review\n",
    "- routing rules (spam vs legit)\n",
    "- parallel tasks (respond + summarize)\n",
    "- human escalation loops\n",
    "\n",
    "Workflows turn agent interactions into a real operational pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### Agent vs. Workflow\n",
    "\n",
    "| AI Agent | Workflow |\n",
    "|----------|----------|\n",
    "| Single reasoning loop | Orchestrates multiple components |\n",
    "| Dynamic tool selection | Predefined execution paths |\n",
    "| Best for: focused tasks | Best for: multi-step processes |\n",
    "\n",
    "### When to Use Workflows\n",
    "\n",
    "| Pattern | Use Case |\n",
    "|---------|----------|\n",
    "| **Sequential** | Steps must run in order (classify \u2192 draft \u2192 review) |\n",
    "| **Branching** | Different paths based on conditions (spam vs. legitimate) |\n",
    "| **Parallel (Fan-out/Fan-in)** | Independent tasks that can run concurrently |\n",
    "| **Group Chat** | Iterative refinement with multiple reviewers |\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Executor** | Unit of work \u2014 agent or custom logic |\n",
    "| **Edge** | Connection between executors with optional conditions |\n",
    "| **WorkflowBuilder** | Constructs the execution graph |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429fcad",
   "metadata": {},
   "source": [
    "# 8. Workflow Pattern #1 \u2014 Sequential Pipeline (InboxOps Assembly Line)\n",
    "\n",
    "![Sequential Workflow](images/sequential-workflow.png)\n",
    "\n",
    "InboxOps introduced a standard pipeline for every inbound email:\n",
    "\n",
    "1) Classify the email  \n",
    "2) Draft a response  \n",
    "3) Review it  \n",
    "\n",
    "This pattern is perfect when order matters.\n",
    "\n",
    "\u2705 Predictable  \n",
    "\u2705 Easy to debug  \n",
    "\u2705 Easy to measure  \n",
    "\u2705 Easy to extend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6274db5",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Executor** | Unit of work (`@executor` or class with `@handler`) |\n",
    "| **WorkflowBuilder** | Connects executors with `add_edge()` |\n",
    "| `ctx.send_message()` | Pass data to next executor |\n",
    "| `ctx.yield_output()` | Return final result |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80541f",
   "metadata": {},
   "source": [
    "## Define Executors\n",
    "\n",
    "Create agent executors for classification, writing, and review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Never\n",
    "from agent_framework import (\n",
    "    WorkflowBuilder, WorkflowContext, WorkflowOutputEvent,\n",
    "    Executor, executor, handler, AgentExecutor, AgentExecutorRequest, AgentExecutorResponse\n",
    ")\n",
    "\n",
    "# === CLASSIFIER AGENT ===\n",
    "classifier_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"Classifier\",\n",
    "        instructions=\"\"\"Classify incoming emails. Return JSON with:\n",
    "- category: \"spam\", \"not_spam\", or \"uncertain\"\n",
    "- confidence: float 0-1\n",
    "- reason: brief explanation\"\"\",\n",
    "        response_format=ClassificationResult,\n",
    "    ),\n",
    "    id=\"classifier\",\n",
    ")\n",
    "\n",
    "# === DRAFT WRITER AGENT ===\n",
    "writer_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"DraftWriter\",\n",
    "        instructions=\"\"\"Draft professional support responses. Return JSON with:\n",
    "- subject: reply subject line\n",
    "- body: reply body\n",
    "- tone: \"formal\", \"friendly\", or \"apologetic\"\n",
    "- needs_review: true if sensitive or complex\"\"\",\n",
    "        response_format=DraftResponse,\n",
    "    ),\n",
    "    id=\"writer\",\n",
    ")\n",
    "\n",
    "# === REVIEWER AGENT ===\n",
    "reviewer_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"Reviewer\",\n",
    "        instructions=\"\"\"Review draft responses for quality. Check:\n",
    "- Professionalism and tone\n",
    "- Accuracy of information\n",
    "- Completeness\n",
    "Return approval decision with notes.\"\"\",\n",
    "    ),\n",
    "    id=\"reviewer\",\n",
    ")\n",
    "\n",
    "print(\"\u2705 Workflow agents defined: classifier, writer, reviewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f78d8",
   "metadata": {},
   "source": [
    "## Build & Run\n",
    "\n",
    "Connect executors with `add_edge()` and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b511c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequential workflow\n",
    "sequential_support_workflow = (\n",
    "    WorkflowBuilder()\n",
    "    .set_start_executor(classifier_agent)\n",
    "    .add_edge(classifier_agent, writer_agent)\n",
    "    .add_edge(writer_agent, reviewer_agent)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Run with legitimate email\n",
    "async def run_sequential_workflow():\n",
    "    email_prompt = f\"\"\"Process this support email:\n",
    "\n",
    "From: {LEGIT_EMAIL.sender}\n",
    "Subject: {LEGIT_EMAIL.subject}\n",
    "Customer ID: {LEGIT_EMAIL.customer_id}\n",
    "\n",
    "{LEGIT_EMAIL.body}\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"\ud83d\udce7 Processing email through workflow: Classify \u2192 Draft \u2192 Review\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    request = AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=email_prompt)],\n",
    "        should_respond=True\n",
    "    )\n",
    "    \n",
    "    from agent_framework._workflows._events import ExecutorCompletedEvent\n",
    "    \n",
    "    async for event in sequential_support_workflow.run_stream(request):\n",
    "        if isinstance(event, ExecutorCompletedEvent) and event.data:\n",
    "            data = event.data[0] if isinstance(event.data, list) else event.data\n",
    "            if hasattr(data, 'agent_response'):\n",
    "                print(f\"\\n\u2705 [{event.executor_id}]:\")\n",
    "                print(f\"   {data.agent_response.text[:300]}...\")\n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            print(f\"\\n\ud83c\udfaf FINAL OUTPUT:\")\n",
    "            if isinstance(event.data, list) and event.data:\n",
    "                final = event.data[0]\n",
    "                if hasattr(final, 'agent_response'):\n",
    "                    print(final.agent_response.text)\n",
    "\n",
    "await run_sequential_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37a59d",
   "metadata": {},
   "source": [
    "# 9. Workflow Pattern #2 \u2014 Branching (InboxOps Triage System)\n",
    "\n",
    "InboxOps doesn't want to treat every message the same.\n",
    "\n",
    "So they built a triage workflow:\n",
    "\n",
    "\ud83d\udeab Spam \u2192 Block and log  \n",
    "\u2705 Legitimate \u2192 Draft a response  \n",
    "\u26a0\ufe0f Uncertain \u2192 Escalate to human review  \n",
    "\n",
    "This prevents wasted effort, reduces risk, and keeps human attention focused where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec9cf6",
   "metadata": {},
   "source": [
    "## Routing Patterns\n",
    "\n",
    "| Pattern | Use Case |\n",
    "|---------|----------|\n",
    "| **Conditional Edge** | Binary if/else |\n",
    "| **Switch-Case** | Multi-way routing |\n",
    "| **Multi-Selection** | Dynamic fan-out |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60db70",
   "metadata": {},
   "source": [
    "## Define Branch Handlers\n",
    "\n",
    "Create handlers for each classification outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38082206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from uuid import uuid4\n",
    "from agent_framework import Case, Default\n",
    "\n",
    "# Internal payload for routing\n",
    "@dataclass\n",
    "class ClassifiedEmail:\n",
    "    email_id: str\n",
    "    category: str  # spam, not_spam, uncertain\n",
    "    confidence: float\n",
    "    reason: str\n",
    "    original_content: str\n",
    "\n",
    "# Shared state keys\n",
    "EMAIL_KEY = \"current_email\"\n",
    "\n",
    "# Helper to extract JSON from markdown code blocks\n",
    "def extract_json(text: str) -> str:\n",
    "    \"\"\"Extract JSON from text, stripping markdown code blocks if present.\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)```', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Transform classification result to routable payload\n",
    "@executor(id=\"extract_classification\")\n",
    "async def extract_classification(response: Any, ctx: WorkflowContext[ClassifiedEmail]) -> None:\n",
    "    \"\"\"Extract classification from agent response for routing.\"\"\"\n",
    "    if isinstance(response, list):\n",
    "        response = response[0]\n",
    "    \n",
    "    # Extract JSON (handles markdown code blocks)\n",
    "    json_text = extract_json(response.agent_response.text)\n",
    "    classification = ClassificationResult.model_validate_json(json_text)\n",
    "    \n",
    "    # Get original email from shared state\n",
    "    original_content = await ctx.get_shared_state(EMAIL_KEY) or \"Unknown\"\n",
    "    \n",
    "    payload = ClassifiedEmail(\n",
    "        email_id=str(uuid4()),\n",
    "        category=classification.category,\n",
    "        confidence=classification.confidence,\n",
    "        reason=classification.reason,\n",
    "        original_content=original_content\n",
    "    )\n",
    "    await ctx.send_message(payload)\n",
    "\n",
    "# Route conditions\n",
    "def is_spam(message: Any) -> bool:\n",
    "    return isinstance(message, ClassifiedEmail) and message.category == \"spam\"\n",
    "\n",
    "def is_not_spam(message: Any) -> bool:\n",
    "    return isinstance(message, ClassifiedEmail) and message.category == \"not_spam\"\n",
    "\n",
    "def is_uncertain(message: Any) -> bool:\n",
    "    return isinstance(message, ClassifiedEmail) and message.category == \"uncertain\"\n",
    "\n",
    "# Terminal handlers\n",
    "@executor(id=\"handle_spam\")\n",
    "async def handle_spam_terminal(email: ClassifiedEmail, ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Handle spam: block and log.\"\"\"\n",
    "    await ctx.yield_output(f\"\ud83d\udeab SPAM BLOCKED: {email.reason} (confidence: {email.confidence:.0%})\")\n",
    "\n",
    "@executor(id=\"handle_not_spam\")\n",
    "async def handle_not_spam_continue(email: ClassifiedEmail, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Handle not_spam: forward to writer.\"\"\"\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Draft a response to: {email.original_content}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "@executor(id=\"finalize_draft\")\n",
    "async def finalize_draft(response: Any, ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Output the final draft.\"\"\"\n",
    "    if isinstance(response, list):\n",
    "        response = response[0]\n",
    "    # Extract JSON (handles markdown code blocks)\n",
    "    json_text = extract_json(response.agent_response.text)\n",
    "    draft = DraftResponse.model_validate_json(json_text)\n",
    "    await ctx.yield_output(f\"\u2709\ufe0f DRAFT READY:\\nSubject: {draft.subject}\\n\\n{draft.body}\")\n",
    "\n",
    "@executor(id=\"handle_uncertain\")\n",
    "async def handle_uncertain_terminal(email: ClassifiedEmail, ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Handle uncertain: flag for human review.\"\"\"\n",
    "    await ctx.yield_output(f\"\u26a0\ufe0f NEEDS HUMAN REVIEW: {email.reason} (confidence: {email.confidence:.0%})\\n\\nOriginal: {email.original_content[:200]}...\")\n",
    "\n",
    "print(\"\u2705 Branching executors defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f06000",
   "metadata": {},
   "source": [
    "## Build Switch-Case Workflow\n",
    "\n",
    "Route based on classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store email and start classification\n",
    "@executor(id=\"start_classification\")\n",
    "async def start_classification(email_text: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Store email and send for classification.\"\"\"\n",
    "    await ctx.set_shared_state(EMAIL_KEY, email_text)\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Classify this email:\\n\\n{email_text}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Build branching workflow\n",
    "branching_workflow = (\n",
    "    WorkflowBuilder()\n",
    "    .set_start_executor(start_classification)\n",
    "    .add_edge(start_classification, classifier_agent)\n",
    "    .add_edge(classifier_agent, extract_classification)\n",
    "    # Switch-case routing\n",
    "    .add_switch_case_edge_group(\n",
    "        extract_classification,\n",
    "        [\n",
    "            Case(condition=is_spam, target=handle_spam_terminal),\n",
    "            Case(condition=is_not_spam, target=handle_not_spam_continue),\n",
    "            Default(target=handle_uncertain_terminal),  # Catches uncertain + unexpected\n",
    "        ],\n",
    "    )\n",
    "    # Continue not_spam path to draft\n",
    "    .add_edge(handle_not_spam_continue, writer_agent)\n",
    "    .add_edge(writer_agent, finalize_draft)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"\u2705 Branching workflow built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6f304",
   "metadata": {},
   "source": [
    "## Test Branching\n",
    "\n",
    "Run all three email types through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59110839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all three paths\n",
    "async def test_branching():\n",
    "    test_cases = [\n",
    "        (\"LEGITIMATE\", LEGIT_EMAIL),\n",
    "        (\"SPAM\", SPAM_EMAIL),\n",
    "        (\"AMBIGUOUS\", AMBIGUOUS_EMAIL),\n",
    "    ]\n",
    "    \n",
    "    for label, email in test_cases:\n",
    "        print(f\"\\n\ud83d\udce7 Testing {label} email...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        email_text = f\"From: {email.sender}\\nSubject: {email.subject}\\n\\n{email.body}\"\n",
    "        \n",
    "        async for event in branching_workflow.run_stream(email_text):\n",
    "            if isinstance(event, WorkflowOutputEvent):\n",
    "                print(event.data)\n",
    "\n",
    "await test_branching()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322b48f",
   "metadata": {},
   "source": [
    "# 9.1. Workflow Pattern #2.5 \u2014 Checkpointing (InboxOps Batch Recovery)\n",
    "\n",
    "## The Problem: Processing 10,000 Emails Overnight\n",
    "\n",
    "InboxOps runs nightly batch jobs to process accumulated emails:\n",
    "- **10,000 emails** need classification and routing\n",
    "- Job takes **3 hours** to complete\n",
    "- **What if it crashes at email 7,500?**\n",
    "\n",
    "Without checkpointing:\n",
    "- \u274c Restart from beginning\n",
    "- \u274c Reprocess 7,500 emails (duplicate work, duplicate costs)\n",
    "- \u274c Delays customer responses\n",
    "\n",
    "## Solution: Workflow Checkpointing\n",
    "\n",
    "Save progress at each step so you can resume from failure point:\n",
    "\n",
    "```python\n",
    "from agent_framework.workflows import FileCheckpointStorage\n",
    "\n",
    "# Enable checkpointing\n",
    "workflow = SequentialWorkflow()\\\n",
    "    .with_checkpointing(\n",
    "        storage=FileCheckpointStorage(checkpoint_dir=\"./checkpoints\")\n",
    "    )\n",
    "\n",
    "# If workflow fails at step 3 of 5:\n",
    "# Resume from step 3 (skip steps 1-2)\n",
    "workflow.resume(checkpoint_id=\"batch_job_001\")\n",
    "```\n",
    "\n",
    "Let's add fault tolerance to InboxOps batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework.workflows import SequentialWorkflow, WorkflowAgent\n",
    "from agent_framework import ChatAgent\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Simulated batch email processing\n",
    "emails_batch = [\n",
    "    {\"id\": f\"EMAIL-{i:04d}\", \"subject\": f\"Customer inquiry #{i}\", \"priority\": \"normal\"}\n",
    "    for i in range(1, 21)  # 20 emails for demo (imagine 10,000 in production)\n",
    "]\n",
    "\n",
    "# Create processing agents\n",
    "classifier_agent = ChatAgent(\n",
    "    name=\"EmailClassifier\",\n",
    "    model_client=model,\n",
    "    instructions=\"Classify emails into: urgent, normal, or low priority. Return only the priority level.\"\n",
    ")\n",
    "\n",
    "router_agent = ChatAgent(\n",
    "    name=\"EmailRouter\",\n",
    "    model_client=model,\n",
    "    instructions=\"Based on email priority, route to: urgent_queue, normal_queue, or low_queue. Return only the queue name.\"\n",
    ")\n",
    "\n",
    "# Simple file-based checkpoint storage (similar to FileCheckpointStorage)\n",
    "class SimpleCheckpointStorage:\n",
    "    def __init__(self, checkpoint_dir: str = \"./checkpoints\"):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(self, workflow_id: str, step: int, data: dict):\n",
    "        \"\"\"Save checkpoint to file\"\"\"\n",
    "        checkpoint_file = os.path.join(self.checkpoint_dir, f\"{workflow_id}.json\")\n",
    "        checkpoint = {\n",
    "            \"workflow_id\": workflow_id,\n",
    "            \"step\": step,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"data\": data\n",
    "        }\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint, f, indent=2)\n",
    "        print(f\"   \ud83d\udcbe Checkpoint saved: step {step}\")\n",
    "    \n",
    "    def load_checkpoint(self, workflow_id: str) -> dict | None:\n",
    "        \"\"\"Load checkpoint from file\"\"\"\n",
    "        checkpoint_file = os.path.join(self.checkpoint_dir, f\"{workflow_id}.json\")\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            with open(checkpoint_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def clear_checkpoint(self, workflow_id: str):\n",
    "        \"\"\"Clear checkpoint after successful completion\"\"\"\n",
    "        checkpoint_file = os.path.join(self.checkpoint_dir, f\"{workflow_id}.json\")\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            os.remove(checkpoint_file)\n",
    "\n",
    "# Create checkpoint storage\n",
    "checkpoint_storage = SimpleCheckpointStorage()\n",
    "workflow_id = \"email_batch_2024\"\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint = checkpoint_storage.load_checkpoint(workflow_id)\n",
    "if checkpoint:\n",
    "    print(f\"\ud83d\udd04 RESUMING FROM CHECKPOINT\")\n",
    "    print(f\"   Workflow: {checkpoint['workflow_id']}\")\n",
    "    print(f\"   Last completed step: {checkpoint['step']}\")\n",
    "    print(f\"   Processed emails: {checkpoint['data']['processed_count']}\\n\")\n",
    "    start_index = checkpoint['data']['processed_count']\n",
    "else:\n",
    "    print(\"\ud83d\ude80 STARTING NEW BATCH PROCESSING\\n\")\n",
    "    start_index = 0\n",
    "\n",
    "# Process emails with checkpointing\n",
    "processed_emails = []\n",
    "\n",
    "print(f\"\ud83d\udce7 Processing {len(emails_batch)} emails (starting from email {start_index + 1})...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(start_index, len(emails_batch)):\n",
    "    email = emails_batch[i]\n",
    "    print(f\"\\n\ud83d\udcec Processing: {email['id']} - {email['subject']}\")\n",
    "    \n",
    "    # Classify email\n",
    "    classification = f\"priority_{email['priority']}\"  # Simplified for demo\n",
    "    print(f\"   \ud83c\udff7\ufe0f  Classified as: {classification}\")\n",
    "    \n",
    "    # Route email\n",
    "    queue = f\"{email['priority']}_queue\"\n",
    "    print(f\"   \ud83d\udcee Routed to: {queue}\")\n",
    "    \n",
    "    processed_emails.append({\n",
    "        **email,\n",
    "        \"classification\": classification,\n",
    "        \"queue\": queue,\n",
    "        \"processed_at\": time.time()\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint every 5 emails\n",
    "    if (i + 1) % 5 == 0:\n",
    "        checkpoint_storage.save_checkpoint(\n",
    "            workflow_id=workflow_id,\n",
    "            step=i + 1,\n",
    "            data={\n",
    "                \"processed_count\": i + 1,\n",
    "                \"last_email_id\": email['id'],\n",
    "                \"processed_emails\": processed_emails\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Simulate processing time\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    # Simulate failure at email 12 (only on first run)\n",
    "    if i == 11 and start_index == 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"\u274c SIMULATED SYSTEM CRASH AT EMAIL 12!\")\n",
    "        print(\"\ud83d\udcbe Checkpoint saved at email 10\")\n",
    "        print(\"\\n\ud83d\udd04 To resume, run this cell again...\")\n",
    "        print(\"=\"*60)\n",
    "        break\n",
    "else:\n",
    "    # Successfully completed\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"\u2705 BATCH PROCESSING COMPLETE!\")\n",
    "    print(f\"   Total processed: {len(processed_emails)} emails\")\n",
    "    print(f\"   Success rate: 100%\")\n",
    "    \n",
    "    # Clear checkpoint\n",
    "    checkpoint_storage.clear_checkpoint(workflow_id)\n",
    "    print(\"   \ud83e\uddf9 Checkpoint cleared\")\n",
    "\n",
    "# Show sample results\n",
    "if processed_emails:\n",
    "    print(\"\\n\ud83d\udcca Sample Results:\")\n",
    "    for email in processed_emails[:3]:\n",
    "        print(f\"   {email['id']}: {email['classification']} \u2192 {email['queue']}\")\n",
    "    if len(processed_emails) > 3:\n",
    "        print(f\"   ... and {len(processed_emails) - 3} more\")\n",
    "\n",
    "print(\"\\n\u2705 Checkpointing enables fault-tolerant batch processing!\")\n",
    "print(\"\ud83d\udca1 In production, use FileCheckpointStorage from agent_framework.workflows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecb767",
   "metadata": {},
   "source": [
    "# 10. Workflow Pattern #3 \u2014 Fan-Out / Fan-In (Parallelization)\n",
    "\n",
    "![Concurrent Workflow](images/concurrent-workflow.png)\n",
    "\n",
    "InboxOps had a performance and productivity challenge:\n",
    "\n",
    "For long emails, support reps want:\n",
    "\u2705 a customer-facing reply  \n",
    "\u2705 an internal summary (for ticket notes)  \n",
    "\n",
    "These tasks are independent.\n",
    "So InboxOps runs them in parallel and merges the results.\n",
    "\n",
    "This reduces total processing time and improves rep productivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863e4e6",
   "metadata": {},
   "source": [
    "## Define Parallel Paths\n",
    "\n",
    "For long emails: respond AND summarize concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary model\n",
    "class EmailSummary(BaseModel):\n",
    "    \"\"\"Concise email summary.\"\"\"\n",
    "    key_points: list[str] = Field(description=\"Main points from the email\")\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Urgency level\")\n",
    "    action_required: str = Field(description=\"Primary action needed\")\n",
    "\n",
    "# Summarizer agent\n",
    "summarizer_agent = AgentExecutor(\n",
    "    chat_client.as_agent(\n",
    "        name=\"Summarizer\",\n",
    "        instructions=\"\"\"Summarize emails concisely. Return JSON with:\n",
    "- key_points: list of main points\n",
    "- urgency: low/medium/high\n",
    "- action_required: primary action needed\"\"\",\n",
    "        response_format=EmailSummary,\n",
    "    ),\n",
    "    id=\"summarizer\",\n",
    ")\n",
    "\n",
    "# Threshold for \"long\" emails\n",
    "LONG_EMAIL_THRESHOLD = 200  # characters\n",
    "\n",
    "@dataclass\n",
    "class EnrichedEmail:\n",
    "    \"\"\"Email with metadata for routing.\"\"\"\n",
    "    email_id: str\n",
    "    content: str\n",
    "    is_long: bool\n",
    "    category: str\n",
    "\n",
    "# Selection function for multi-selection routing\n",
    "def select_parallel_paths(email: EnrichedEmail, target_ids: list[str]) -> list[str]:\n",
    "    \"\"\"Select paths based on email length.\"\"\"\n",
    "    # target_ids order: [respond_path, summarize_path]\n",
    "    respond_id, summarize_id = target_ids\n",
    "    \n",
    "    if email.is_long:\n",
    "        return [respond_id, summarize_id]  # Both paths in parallel\n",
    "    else:\n",
    "        return [respond_id]  # Only respond for short emails\n",
    "\n",
    "# Executors for parallel paths\n",
    "@executor(id=\"prepare_parallel\")\n",
    "async def prepare_parallel(classified: ClassifiedEmail, ctx: WorkflowContext[EnrichedEmail]) -> None:\n",
    "    \"\"\"Prepare email for parallel processing.\"\"\"\n",
    "    enriched = EnrichedEmail(\n",
    "        email_id=classified.email_id,\n",
    "        content=classified.original_content,\n",
    "        is_long=len(classified.original_content) > LONG_EMAIL_THRESHOLD,\n",
    "        category=classified.category\n",
    "    )\n",
    "    await ctx.send_message(enriched)\n",
    "\n",
    "@executor(id=\"respond_path\")\n",
    "async def respond_path(email: EnrichedEmail, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Send to writer for response.\"\"\"\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Draft a response to:\\n{email.content}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "@executor(id=\"summarize_path\")\n",
    "async def summarize_path(email: EnrichedEmail, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Send to summarizer.\"\"\"\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Summarize this email:\\n{email.content}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Aggregator to combine parallel results\n",
    "class ParallelAggregator(Executor):\n",
    "    def __init__(self):\n",
    "        super().__init__(id=\"parallel_aggregator\")\n",
    "    \n",
    "    @handler\n",
    "    async def aggregate(self, results: list[Any], ctx: WorkflowContext[Never, str]) -> None:\n",
    "        \"\"\"Combine response and summary.\"\"\"\n",
    "        output_parts = []\n",
    "        \n",
    "        for result in results:\n",
    "            if isinstance(result, AgentExecutorResponse):\n",
    "                try:\n",
    "                    draft = DraftResponse.model_validate_json(result.agent_response.text)\n",
    "                    output_parts.append(f\"\ud83d\udce7 DRAFT RESPONSE:\\nSubject: {draft.subject}\\n{draft.body}\")\n",
    "                except:\n",
    "                    try:\n",
    "                        summary = EmailSummary.model_validate_json(result.agent_response.text)\n",
    "                        points = \"\\n\".join(f\"  \u2022 {p}\" for p in summary.key_points)\n",
    "                        output_parts.append(f\"\ud83d\udccb SUMMARY:\\n{points}\\nUrgency: {summary.urgency}\\nAction: {summary.action_required}\")\n",
    "                    except:\n",
    "                        output_parts.append(f\"Result: {result.agent_response.text[:200]}...\")\n",
    "        \n",
    "        await ctx.yield_output(\"\\n\\n\" + \"=\"*40 + \"\\n\\n\".join(output_parts))\n",
    "\n",
    "aggregator = ParallelAggregator()\n",
    "\n",
    "print(\"\u2705 Parallel processing executors defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ef984",
   "metadata": {},
   "source": [
    "## Build Fan-Out/Fan-In Workflow\n",
    "\n",
    "Short emails \u2192 respond only. Long emails \u2192 respond + summarize in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e887adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import WorkflowBuilder\n",
    "from agent_framework._workflows._events import ExecutorCompletedEvent\n",
    "from datetime import datetime\n",
    "\n",
    "# Constants\n",
    "LONG_EMAIL_THRESHOLD = 200  # Characters\n",
    "\n",
    "# Start executor - entry point stores email and passes it forward\n",
    "@executor(id=\"fanout_start\")\n",
    "async def fanout_start(email_text: str, ctx: WorkflowContext[str]) -> None:\n",
    "    \"\"\"Entry point: store email length, forward email text.\"\"\"\n",
    "    # Store email length in shared state for selection\n",
    "    await ctx.set_shared_state(\"email_length\", len(email_text))\n",
    "    # Store workflow start time\n",
    "    await ctx.set_shared_state(\"workflow_start_time\", time.time())\n",
    "    await ctx.send_message(email_text)\n",
    "\n",
    "# Selection function that uses shared state\n",
    "def fanout_select_paths(email_text: str, target_ids: list[str]) -> list[str]:\n",
    "    \"\"\"Select paths based on email length (stored in text).\"\"\"\n",
    "    # The email_text is still the raw string at this point\n",
    "    if len(email_text) > LONG_EMAIL_THRESHOLD:\n",
    "        return target_ids  # Both paths for long emails\n",
    "    return [target_ids[0]]  # Only response path for short emails\n",
    "\n",
    "# Response path preparer with timing\n",
    "@executor(id=\"fanout_respond_prep\")\n",
    "async def fanout_respond_prep(email_text: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Prepare email for writer agent.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    start_time = time.time()\n",
    "    elapsed = start_time - workflow_start\n",
    "    print(f\"   \u23f1\ufe0f  [+{elapsed:.2f}s] \ud83d\udcdd RESPONSE PATH started\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"response_start_time\", start_time)\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Draft a response to:\\n{email_text}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Summary path preparer with timing\n",
    "@executor(id=\"fanout_summarize_prep\")\n",
    "async def fanout_summarize_prep(email_text: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "    \"\"\"Prepare email for summarizer agent.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    start_time = time.time()\n",
    "    elapsed = start_time - workflow_start\n",
    "    print(f\"   \u23f1\ufe0f  [+{elapsed:.2f}s] \ud83d\udccb SUMMARY PATH started\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"summary_start_time\", start_time)\n",
    "    await ctx.send_message(AgentExecutorRequest(\n",
    "        messages=[ChatMessage(Role.USER, text=f\"Summarize this email:\\n{email_text}\")],\n",
    "        should_respond=True\n",
    "    ))\n",
    "\n",
    "# Capture completion time immediately after writer finishes\n",
    "@executor(id=\"capture_writer_completion\")\n",
    "async def capture_writer_completion(result: Any, ctx: WorkflowContext[Any]) -> None:\n",
    "    \"\"\"Capture writer completion time.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    response_start = await ctx.get_shared_state(\"response_start_time\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_from_start = end_time - workflow_start\n",
    "    duration = end_time - response_start\n",
    "    print(f\"   \u23f1\ufe0f  [+{elapsed_from_start:.2f}s] \u2705 RESPONSE PATH completed ({duration:.2f}s)\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"response_end_time\", end_time)\n",
    "    await ctx.send_message(result)\n",
    "\n",
    "# Capture completion time immediately after summarizer finishes\n",
    "@executor(id=\"capture_summarizer_completion\")\n",
    "async def capture_summarizer_completion(result: Any, ctx: WorkflowContext[Any]) -> None:\n",
    "    \"\"\"Capture summarizer completion time.\"\"\"\n",
    "    workflow_start = await ctx.get_shared_state(\"workflow_start_time\")\n",
    "    summary_start = await ctx.get_shared_state(\"summary_start_time\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_from_start = end_time - workflow_start\n",
    "    duration = end_time - summary_start\n",
    "    print(f\"   \u23f1\ufe0f  [+{elapsed_from_start:.2f}s] \u2705 SUMMARY PATH completed ({duration:.2f}s)\")\n",
    "    \n",
    "    await ctx.set_shared_state(\"summary_end_time\", end_time)\n",
    "    await ctx.send_message(result)\n",
    "\n",
    "# Aggregator - combines results from parallel paths with timing\n",
    "@executor(id=\"fanout_aggregator\")\n",
    "async def fanout_aggregator(results: list[Any], ctx: WorkflowContext[Never, str]) -> None:\n",
    "    \"\"\"Combine response and summary results with timing information.\"\"\"\n",
    "    response_start = await ctx.get_shared_state(\"response_start_time\")\n",
    "    summary_start = await ctx.get_shared_state(\"summary_start_time\")\n",
    "    response_end = await ctx.get_shared_state(\"response_end_time\")\n",
    "    summary_end = await ctx.get_shared_state(\"summary_end_time\")\n",
    "    \n",
    "    output_parts = []\n",
    "    response_time = None\n",
    "    summary_time = None\n",
    "    \n",
    "    # Calculate durations from stored times\n",
    "    if response_start and response_end:\n",
    "        response_time = response_end - response_start\n",
    "    if summary_start and summary_end:\n",
    "        summary_time = summary_end - summary_start\n",
    "    \n",
    "    for result in results:\n",
    "        if isinstance(result, AgentExecutorResponse):\n",
    "            try:\n",
    "                draft = DraftResponse.model_validate_json(extract_json(result.agent_response.text))\n",
    "                output_parts.append(\n",
    "                    f\"\ud83d\udcec RESPONSE (completed in {response_time:.2f}s):\\n\"\n",
    "                    f\"Subject: {draft.subject}\\n{draft.body}\"\n",
    "                )\n",
    "            except:\n",
    "                try:\n",
    "                    summary = EmailSummary.model_validate_json(extract_json(result.agent_response.text))\n",
    "                    points = \"\\n\".join(f\"  \u2022 {p}\" for p in summary.key_points)\n",
    "                    output_parts.append(\n",
    "                        f\"\ud83d\udccb SUMMARY (completed in {summary_time:.2f}s):\\n\"\n",
    "                        f\"{points}\\n\"\n",
    "                        f\"Urgency: {summary.urgency}\\n\"\n",
    "                        f\"Action: {summary.action_required}\"\n",
    "                    )\n",
    "                except:\n",
    "                    output_parts.append(f\"Result: {result.agent_response.text[:200]}...\")\n",
    "    \n",
    "    # Calculate overlap to show parallelization\n",
    "    if response_time and summary_time:\n",
    "        total_sequential = response_time + summary_time\n",
    "        total_parallel = max(response_time, summary_time)\n",
    "        time_saved = total_sequential - total_parallel\n",
    "        output_parts.append(\n",
    "            f\"\\n\u26a1 PARALLEL EXECUTION BENEFIT:\\n\"\n",
    "            f\"   Sequential time: {total_sequential:.2f}s\\n\"\n",
    "            f\"   Parallel time: {total_parallel:.2f}s\\n\"\n",
    "            f\"   Time saved: {time_saved:.2f}s ({time_saved/total_sequential*100:.1f}%)\"\n",
    "        )\n",
    "    \n",
    "    await ctx.yield_output(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(output_parts))\n",
    "\n",
    "# Build the fan-out workflow\n",
    "# Pattern: start -> [fanout to preparers] -> [agents] -> [capture timing] -> aggregator\n",
    "fanout_workflow = (\n",
    "    WorkflowBuilder()\n",
    "    .set_start_executor(fanout_start)\n",
    "    # Fan-out from start directly to path preparers based on email length\n",
    "    .add_multi_selection_edge_group(\n",
    "        fanout_start,\n",
    "        targets=[fanout_respond_prep, fanout_summarize_prep],\n",
    "        selection_func=fanout_select_paths,\n",
    "    )\n",
    "    # Each preparer sends to its agent\n",
    "    .add_edge(fanout_respond_prep, writer_agent)\n",
    "    .add_edge(fanout_summarize_prep, summarizer_agent)\n",
    "    # Capture completion times immediately after each agent\n",
    "    .add_edge(writer_agent, capture_writer_completion)\n",
    "    .add_edge(summarizer_agent, capture_summarizer_completion)\n",
    "    # Fan-in: collect all results\n",
    "    .add_fan_in_edges([capture_writer_completion, capture_summarizer_completion], fanout_aggregator)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"\u2705 Fan-out/fan-in workflow built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b962d58",
   "metadata": {},
   "source": [
    "## Test Parallel Execution\n",
    "\n",
    "Long emails trigger both response and summary paths concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7492286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with long legitimate email\n",
    "async def test_fanout():\n",
    "    email_text = f\"From: {LEGIT_EMAIL.sender}\\nSubject: {LEGIT_EMAIL.subject}\\n\\n{LEGIT_EMAIL.body}\"\n",
    "    \n",
    "    print(f\"\ud83d\udce7 Testing LONG email ({len(email_text)} chars > {LONG_EMAIL_THRESHOLD} threshold)\")\n",
    "    print(\"Expected: Response AND Summary in parallel\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    async for event in fanout_workflow.run_stream(email_text):\n",
    "        if isinstance(event, WorkflowOutputEvent):\n",
    "            print(event.data)\n",
    "\n",
    "await test_fanout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a7439",
   "metadata": {},
   "source": [
    "# 11. Workflow Pattern #4 \u2014 Group Chat (InboxOps Review Committee)\n",
    "\n",
    "![Group Chat Pattern](images/group-chat.png)\n",
    "\n",
    "InboxOps Enterprise customers have strict requirements.\n",
    "\n",
    "Before sending a response, the draft must pass:\n",
    "1) Security review (PII / compliance)\n",
    "2) Accuracy review (promises and timelines)\n",
    "3) Tone review (final editor)\n",
    "\n",
    "A Group Chat pattern allows:\n",
    "- shared context across reviewers\n",
    "- structured collaboration\n",
    "- a \"final editor\" agent that ships the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38398cec",
   "metadata": {},
   "source": [
    "## Key Differences\n",
    "\n",
    "| Pattern | Coordination | Use Case |\n",
    "|---------|--------------|----------|\n",
    "| **Concurrent** | No coordination | Independent parallel tasks |\n",
    "| **Group Chat** | Orchestrator selects speakers | Iterative refinement, shared context |\n",
    "| **Magentic** | Manager with dynamic planning | Complex open-ended tasks |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105e4d2",
   "metadata": {},
   "source": [
    "## Define Specialists\n",
    "\n",
    "Create agents with distinct review roles. All agents will see the shared conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b41c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework import GroupChatBuilder, GroupChatState, ConcurrentBuilder, MagenticBuilder\n",
    "\n",
    "# Three specialized reviewers - order matters! Last one produces final output.\n",
    "\n",
    "# 1st: Security reviewer - identifies security/compliance issues\n",
    "security_reviewer = ChatAgent(\n",
    "    name=\"SecurityReviewer\",\n",
    "    description=\"Security and compliance specialist - reviews first\",\n",
    "    instructions=\"\"\"You are the FIRST reviewer. Analyze the support response for:\n",
    "- Data exposure risks (customer IDs, case numbers that shouldn't be in emails)\n",
    "- PII handling concerns (names, order details)\n",
    "- Policy compliance issues\n",
    "\n",
    "Be concise. List only the security issues you find. Do NOT rewrite the email - just identify problems for later reviewers to address.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# 2nd: Accuracy reviewer - checks facts and promises\n",
    "accuracy_reviewer = ChatAgent(\n",
    "    name=\"AccuracyReviewer\", \n",
    "    description=\"Factual accuracy specialist - reviews second\",\n",
    "    instructions=\"\"\"You are the SECOND reviewer. Analyze the support response for:\n",
    "- Unrealistic promises or timelines\n",
    "- Unverifiable claims\n",
    "- Compensation appropriateness\n",
    "\n",
    "Consider the security feedback from the previous reviewer. Be concise. List only the accuracy issues. Do NOT rewrite the email - just identify problems for the final reviewer to address.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# 3rd: Tone reviewer - applies all feedback and produces final email\n",
    "tone_reviewer = ChatAgent(\n",
    "    name=\"ToneReviewer\",\n",
    "    description=\"Tone specialist and final editor - produces revised email\",\n",
    "    instructions=\"\"\"You are the FINAL reviewer. Your job is to:\n",
    "1. Consider ALL feedback from SecurityReviewer and AccuracyReviewer\n",
    "2. Review the tone and empathy of the original email\n",
    "3. **PRODUCE A FINAL REVISED EMAIL** that:\n",
    "   - Addresses security concerns (remove/mask sensitive identifiers if needed)\n",
    "   - Fixes accuracy issues (realistic timelines, appropriate promises)\n",
    "   - Maintains professional, empathetic tone\n",
    "   - Is ready to send to the customer\n",
    "\n",
    "End your response with the complete revised email in a clear format.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "print(\"\u2705 Three specialist reviewers defined:\")\n",
    "print(\"   1. SecurityReviewer - identifies security issues\")\n",
    "print(\"   2. AccuracyReviewer - checks facts and promises\")  \n",
    "print(\"   3. ToneReviewer - applies all feedback and produces FINAL email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27452e67",
   "metadata": {},
   "source": [
    "## Build Group Chat with Round-Robin\n",
    "\n",
    "Simple selection: each reviewer speaks in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample draft response to review\n",
    "draft_to_review = \"\"\"\n",
    "Subject: Re: Order #12345 - Delivery Issue\n",
    "\n",
    "Dear Sarah,\n",
    "\n",
    "I'm so sorry to hear about the missing package! This must be incredibly frustrating.\n",
    "\n",
    "I've located your order and can confirm it was marked as delivered on Monday. Here's what I'll do:\n",
    "\n",
    "1. I've opened an investigation with our shipping partner (Case #INV-789)\n",
    "2. As a Premium customer, I'm expediting a replacement shipment TODAY\n",
    "3. The replacement will arrive by Thursday, well before your Friday presentation\n",
    "\n",
    "Your account has also been credited $50 for the inconvenience.\n",
    "\n",
    "If you need anything else, reply directly to this email - I'm here to help!\n",
    "\n",
    "Best regards,\n",
    "Support Team\n",
    "\"\"\"\n",
    "\n",
    "# Round-robin selector: each reviewer speaks in order\n",
    "def round_robin_selector(state: GroupChatState) -> str:\n",
    "    \"\"\"Pick the next speaker based on round index.\"\"\"\n",
    "    participants = list(state.participants.keys())\n",
    "    return participants[state.current_round % len(participants)]\n",
    "\n",
    "# Build group chat with round-robin selection\n",
    "# ORDER MATTERS: Security \u2192 Accuracy \u2192 Tone (final editor)\n",
    "review_group_chat = (\n",
    "    GroupChatBuilder()\n",
    "    .with_orchestrator(selection_func=round_robin_selector, orchestrator_name=\"RoundRobinOrchestrator\")\n",
    "    .participants([security_reviewer, accuracy_reviewer, tone_reviewer])  # Order: Security \u2192 Accuracy \u2192 Tone\n",
    "    .with_termination_condition(lambda msgs: len([m for m in msgs if m.role.value == \"assistant\"]) >= 3)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"\u2705 Group chat built with round-robin selection\")\n",
    "print(\"   Order: SecurityReviewer \u2192 AccuracyReviewer \u2192 ToneReviewer (final)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7588de",
   "metadata": {},
   "source": [
    "## Test Round-Robin Group Chat\n",
    "\n",
    "Each reviewer analyzes the draft in turn, building on previous insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the group chat with round-robin selection\n",
    "from agent_framework._workflows._events import AgentRunUpdateEvent\n",
    "\n",
    "async def test_round_robin_group_chat():\n",
    "    print(\"\ud83d\udcdd DRAFT TO REVIEW:\")\n",
    "    print(draft_to_review)\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n\ud83d\udd04 ROUND-ROBIN GROUP CHAT (each reviewer speaks in turn):\\n\")\n",
    "    \n",
    "    last_executor_id: str | None = None\n",
    "    agent_order = []\n",
    "    \n",
    "    async for event in review_group_chat.run_stream(f\"Review this support response:\\n{draft_to_review}\"):\n",
    "        if isinstance(event, AgentRunUpdateEvent):\n",
    "            eid = event.executor_id\n",
    "            if eid != last_executor_id:\n",
    "                if last_executor_id is not None:\n",
    "                    print(\"\\n\")\n",
    "                agent_order.append(eid)\n",
    "                print(f\"\\n\ud83e\udd16 [{eid}] (Turn #{len(agent_order)}):\", end=\" \", flush=True)\n",
    "                last_executor_id = eid\n",
    "            print(event.data, end=\"\", flush=True)\n",
    "        \n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            print(\"\\n\\n\" + \"=\" * 60)\n",
    "            print(f\"\ud83d\udcca EXECUTION ORDER: {' \u2192 '.join(agent_order)}\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "await test_round_robin_group_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent-based orchestrator for intelligent speaker selection\n",
    "from typing import cast\n",
    "from agent_framework._workflows._events import AgentRunUpdateEvent, WorkflowOutputEvent\n",
    "from agent_framework._types import ChatMessage\n",
    "\n",
    "orchestrator_agent = ChatAgent(\n",
    "    name=\"ReviewOrchestrator\",\n",
    "    description=\"Coordinates multi-agent review process\",\n",
    "    instructions=f\"\"\"You coordinate a team reviewing this support response:\n",
    "\n",
    "{draft_to_review}\n",
    "\n",
    "YOUR TEAM:\n",
    "- SecurityReviewer: Identifies security/PII issues (reviews first)\n",
    "- AccuracyReviewer: Checks facts and promises (reviews second)\n",
    "- ToneReviewer: Final editor who produces the revised email (reviews last)\n",
    "\n",
    "YOUR PROCESS:\n",
    "1. Start with SecurityReviewer to check data safety and PII\n",
    "2. Then AccuracyReviewer to verify claims and timelines\n",
    "3. **Finally, ToneReviewer to produce the FINAL REVISED EMAIL** incorporating all feedback\n",
    "4. If needed, you may ask follow-up questions to any reviewer\n",
    "5. End when ToneReviewer delivers the complete revised email\n",
    "\n",
    "Select speakers intelligently. CRITICAL: ToneReviewer must go last and produce the final email.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# Build group chat with agent-based orchestration\n",
    "# ORDER: Security \u2192 Accuracy \u2192 Tone (final editor)\n",
    "intelligent_review_chat = (\n",
    "    GroupChatBuilder()\n",
    "    .with_orchestrator(agent=orchestrator_agent)\n",
    "    .participants([security_reviewer, accuracy_reviewer, tone_reviewer])\n",
    "    .with_termination_condition(lambda msgs: len([m for m in msgs if m.role.value == \"assistant\"]) >= 5)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Run with detailed logging\n",
    "async def test_agent_orchestrated_group_chat():\n",
    "    print(\"\ud83d\udcdd DRAFT TO REVIEW:\")\n",
    "    print(draft_to_review)\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n\ud83e\udde0 AGENT-ORCHESTRATED GROUP CHAT (intelligent speaker selection):\\n\")\n",
    "    \n",
    "    last_executor_id: str | None = None\n",
    "    agent_calls: dict[str, int] = {}\n",
    "    \n",
    "    async for event in intelligent_review_chat.run_stream(\"Review this support response. Security and Accuracy reviewers identify issues, then ToneReviewer produces the final revised email.\"):\n",
    "        if isinstance(event, AgentRunUpdateEvent):\n",
    "            eid = event.executor_id\n",
    "            if eid != last_executor_id:\n",
    "                if last_executor_id is not None:\n",
    "                    print(\"\\n\")\n",
    "                agent_calls[eid] = agent_calls.get(eid, 0) + 1\n",
    "                print(f\"\\n\ud83e\udd16 [{eid}] (Call #{agent_calls[eid]}):\", end=\" \", flush=True)\n",
    "                last_executor_id = eid\n",
    "            print(event.data, end=\"\", flush=True)\n",
    "        \n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            output_messages = cast(list[ChatMessage], event.data)\n",
    "            \n",
    "            print(\"\\n\\n\" + \"=\" * 60)\n",
    "            print(\"\ud83d\udcca EXECUTION SUMMARY\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"   Total calls: {sum(agent_calls.values())}\")\n",
    "            print(\"\\n   Calls per agent:\")\n",
    "            for agent, count in sorted(agent_calls.items()):\n",
    "                print(f\"      {agent}: {count} call(s)\")\n",
    "            \n",
    "            print(\"\\n   \ud83d\udca1 The orchestrator dynamically selected speakers\")\n",
    "            print(\"      based on what was needed at each step\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"\ud83d\udce7 FINAL REVISED EMAIL (from ToneReviewer)\")\n",
    "            print(\"=\" * 60)\n",
    "            for msg in reversed(output_messages):\n",
    "                if msg.role.value == \"assistant\" and \"ToneReviewer\" in str(msg):\n",
    "                    print(msg.text)\n",
    "                    break\n",
    "\n",
    "await test_agent_orchestrated_group_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4e052",
   "metadata": {},
   "source": [
    "# 12. Workflow Pattern #5 \u2014 Magentic Orchestration (Dynamic Planning)\n",
    "\n",
    "![Magentic Pattern](images/magentic-workflow.png)\n",
    "\n",
    "InboxOps eventually expanded beyond emails.\n",
    "\n",
    "They wanted an AI system that can:\n",
    "- research patterns in customer complaints\n",
    "- identify recurring issues\n",
    "- propose operational improvements\n",
    "- generate executive summaries\n",
    "\n",
    "That's not a fixed pipeline anymore.\n",
    "\n",
    "Magentic orchestration introduces a **manager agent** that:\n",
    "\u2705 plans dynamically  \n",
    "\u2705 delegates to specialists  \n",
    "\u2705 iterates until the task is complete  \n",
    "\n",
    "This is the most powerful orchestration mode for open-ended problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0d883",
   "metadata": {},
   "source": [
    "## Use Case: InboxOps Weekly Support Intelligence Report\n",
    "\n",
    "A complex task requiring:\n",
    "1. **Research Agent** - Gather complaint patterns and data\n",
    "2. **Analyst Agent** - Process and analyze the data\n",
    "3. **Manager** - Dynamic planning and synthesis\n",
    "\n",
    "The manager autonomously decides which agent to call and when based on progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magentic Orchestration: Research + Analysis workflow\n",
    "import json\n",
    "from typing import cast\n",
    "from agent_framework import (\n",
    "    AgentRunUpdateEvent,\n",
    "    MagenticOrchestratorEvent,\n",
    "    MagenticProgressLedger,\n",
    ")\n",
    "\n",
    "# Research Agent - gathers data and patterns from support tickets\n",
    "# Note: In production, this would connect to your ticketing system\n",
    "researcher_agent = ChatAgent(\n",
    "    name=\"ResearcherAgent\",\n",
    "    description=\"Specialist in research and information gathering about support patterns and customer complaints\",\n",
    "    instructions=\"\"\"You are an InboxOps Support Research Specialist. Your job is to:\n",
    "- Gather information about customer complaint patterns\n",
    "- Identify recurring issues and trends\n",
    "- Provide realistic example data based on common e-commerce support scenarios\n",
    "\n",
    "When asked about support data, provide realistic example data for categories like:\n",
    "shipping issues, refund requests, product defects, billing disputes, account access.\n",
    "Be concise and factual. Format data clearly for analysis.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# Analyst Agent - processes and analyzes data\n",
    "# Note: In production, add HostedCodeInterpreterTool for real code execution\n",
    "analyst_agent = ChatAgent(\n",
    "    name=\"AnalystAgent\",\n",
    "    description=\"Data analyst who processes support data and creates operational insights\",\n",
    "    instructions=\"\"\"You are an InboxOps Data Analyst. Your job is to:\n",
    "- Process and analyze support ticket data\n",
    "- Calculate metrics (volume trends, resolution times, escalation rates)\n",
    "- Create clear tables and visualizations descriptions\n",
    "- Identify operational improvement opportunities\n",
    "\n",
    "Show your calculations step by step. Format results in clear tables.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "# Manager Agent - orchestrates the research workflow\n",
    "manager_agent = ChatAgent(\n",
    "    name=\"ResearchManager\",\n",
    "    description=\"Orchestrator that coordinates support intelligence workflows\",\n",
    "    instructions=\"\"\"You manage an InboxOps research team to complete support intelligence reports.\n",
    "\n",
    "YOUR TEAM:\n",
    "- ResearcherAgent: Gathers information about support patterns and customer complaints\n",
    "- AnalystAgent: Processes data, performs calculations, creates operational insights\n",
    "\n",
    "YOUR PROCESS:\n",
    "1. Break down the intelligence request into subtasks\n",
    "2. Delegate to ResearcherAgent to gather relevant support data\n",
    "3. Delegate to AnalystAgent to process and analyze the data\n",
    "4. Continue iterating until you have comprehensive insights\n",
    "5. Synthesize all findings into a final report\n",
    "\n",
    "You dynamically decide who to call based on what's needed. You may call agents multiple times.\"\"\",\n",
    "    chat_client=chat_client,\n",
    ")\n",
    "\n",
    "print(\"\u2705 Magentic agents defined: ResearcherAgent, AnalystAgent, ResearchManager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ddaa3",
   "metadata": {},
   "source": [
    "## Build & Run Magentic Workflow\n",
    "\n",
    "The manager dynamically plans and delegates. Watch how it calls different agents based on the evolving task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919845da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Magentic workflow\n",
    "magentic_research_workflow = (\n",
    "    MagenticBuilder()\n",
    "    .participants([researcher_agent, analyst_agent])\n",
    "    .with_manager(\n",
    "        agent=manager_agent,\n",
    "        max_round_count=10,  # Maximum delegation rounds\n",
    "        max_stall_count=2,   # Replan after 2 stalls\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Research task - InboxOps Weekly Support Intelligence Report\n",
    "research_task = \"\"\"\n",
    "InboxOps wants an internal weekly support intelligence report:\n",
    "1. Identify top 5 complaint categories from incoming emails\n",
    "2. Estimate urgency and business impact for each category\n",
    "3. Calculate resolution time trends and escalation rates\n",
    "4. Suggest operational improvements\n",
    "5. Output a clean summary table + executive summary\n",
    "\"\"\"\n",
    "\n",
    "async def run_magentic_research():\n",
    "    print(\"\ud83d\udd2c INBOXOPS SUPPORT INTELLIGENCE WORKFLOW\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\ud83d\udccb TASK:\\n{research_task}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    last_message_id: str | None = None\n",
    "    agent_calls: dict[str, int] = {}\n",
    "    \n",
    "    async for event in magentic_research_workflow.run_stream(research_task):\n",
    "        # Track streaming from agents\n",
    "        if isinstance(event, AgentRunUpdateEvent):\n",
    "            message_id = event.data.message_id\n",
    "            executor_id = event.executor_id\n",
    "            \n",
    "            if message_id != last_message_id:\n",
    "                if last_message_id is not None:\n",
    "                    print(\"\\n\")\n",
    "                agent_calls[executor_id] = agent_calls.get(executor_id, 0) + 1\n",
    "                print(f\"\\n\ud83e\udd16 [{executor_id}] (Call #{agent_calls[executor_id]}):\", end=\" \", flush=True)\n",
    "                last_message_id = message_id\n",
    "            \n",
    "            print(event.data, end=\"\", flush=True)\n",
    "        \n",
    "        # Track orchestration events\n",
    "        elif isinstance(event, MagenticOrchestratorEvent):\n",
    "            print(f\"\\n\\n{'='*55}\")\n",
    "            print(f\"\ud83d\udccb ORCHESTRATOR: {event.event_type.name}\")\n",
    "            print(f\"{'='*55}\")\n",
    "            \n",
    "            if isinstance(event.data, MagenticProgressLedger):\n",
    "                ledger = event.data.to_dict()\n",
    "                if \"next_speaker\" in ledger:\n",
    "                    next_info = ledger.get('next_speaker', {})\n",
    "                    if isinstance(next_info, dict):\n",
    "                        print(f\"   \u27a1\ufe0f Next: {next_info.get('answer', 'N/A')}\")\n",
    "                        reason = next_info.get('reason', '')\n",
    "                        if reason:\n",
    "                            print(f\"   \ud83d\udcad Why: {reason[:100]}...\")\n",
    "                    else:\n",
    "                        print(f\"   \u27a1\ufe0f Next: {next_info}\")\n",
    "        \n",
    "        # Final output\n",
    "        elif isinstance(event, WorkflowOutputEvent):\n",
    "            output_messages = cast(list[ChatMessage], event.data)\n",
    "            \n",
    "            print(\"\\n\\n\" + \"=\" * 60)\n",
    "            print(\"\ud83d\udcca EXECUTION SUMMARY\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"   Total agent calls: {sum(agent_calls.values())}\")\n",
    "            print(\"\\n   Calls per agent:\")\n",
    "            for agent, count in sorted(agent_calls.items()):\n",
    "                print(f\"      {agent}: {count} call(s)\")\n",
    "            \n",
    "            print(\"\\n   \u2728 Manager dynamically orchestrated:\")\n",
    "            print(f\"      - Broke down complex task into subtasks\")\n",
    "            print(f\"      - Called ResearcherAgent for data gathering\")\n",
    "            print(f\"      - Called AnalystAgent for processing\")\n",
    "            print(f\"      - Synthesized into final report\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"\ud83d\udcd1 FINAL INBOXOPS INTELLIGENCE REPORT\")\n",
    "            print(\"=\" * 60)\n",
    "            for msg in reversed(output_messages):\n",
    "                if msg.role.value == \"assistant\":\n",
    "                    print(msg.text)\n",
    "                    break\n",
    "\n",
    "await run_magentic_research()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c54928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8353be",
   "metadata": {},
   "source": [
    "# 13. V4 \u2014 Evaluation & Testing (InboxOps Quality Metrics)\n",
    "\n",
    "## The Problem: How Do We Know the Agent Works Well?\n",
    "\n",
    "After deploying the InboxOps Support Email Copilot, stakeholders ask:\n",
    "\n",
    "- \ud83d\udcca **What's our response accuracy rate?**\n",
    "- \u23f1\ufe0f **How fast are we resolving tickets?**\n",
    "- \ud83d\ude0a **Are customers satisfied?**\n",
    "- \ud83d\udcb0 **What's the ROI vs. human agents?**\n",
    "\n",
    "**\"Deploy and hope\" is not a strategy.**\n",
    "\n",
    "## Solution: Agent Evaluation Framework\n",
    "\n",
    "Systematically measure agent performance:\n",
    "\n",
    "```python\n",
    "# Evaluation metrics\n",
    "metrics = {\n",
    "    \"accuracy\": measure_correct_responses(),\n",
    "    \"response_time\": measure_avg_latency(),\n",
    "    \"customer_satisfaction\": measure_csat_score(),\n",
    "    \"cost_per_ticket\": calculate_cost(),\n",
    "    \"human_escalation_rate\": measure_escalations()\n",
    "}\n",
    "\n",
    "# Automated testing\n",
    "for test_case in test_dataset:\n",
    "    prediction = agent.run(test_case.input)\n",
    "    score = evaluate(prediction, test_case.expected_output)\n",
    "    results.append(score)\n",
    "```\n",
    "\n",
    "Let's build a comprehensive evaluation system for InboxOps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc438dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Define test cases for InboxOps agents\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"Test case for agent evaluation\"\"\"\n",
    "    id: str\n",
    "    input: str\n",
    "    expected_output: Dict[str, Any]\n",
    "    category: str\n",
    "\n",
    "# Create test dataset\n",
    "test_cases = [\n",
    "    TestCase(\n",
    "        id=\"TEST-001\",\n",
    "        input=\"Where is my order #12345? It's been 3 weeks!\",\n",
    "        expected_output={\n",
    "            \"category\": \"order_status\",\n",
    "            \"priority\": \"high\",\n",
    "            \"sentiment\": \"negative\",\n",
    "            \"requires_tool\": True,\n",
    "            \"tool_name\": \"get_order_status\"\n",
    "        },\n",
    "        category=\"order_inquiry\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"TEST-002\",\n",
    "        input=\"Thank you for the quick refund! Great service!\",\n",
    "        expected_output={\n",
    "            \"category\": \"feedback\",\n",
    "            \"priority\": \"low\",\n",
    "            \"sentiment\": \"positive\",\n",
    "            \"requires_tool\": False\n",
    "        },\n",
    "        category=\"positive_feedback\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"TEST-003\",\n",
    "        input=\"I need to cancel ticket TKT-789 immediately.\",\n",
    "        expected_output={\n",
    "            \"category\": \"ticket_management\",\n",
    "            \"priority\": \"urgent\",\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"requires_tool\": True,\n",
    "            \"tool_name\": \"cancel_ticket\"\n",
    "        },\n",
    "        category=\"urgent_request\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Evaluation metrics\n",
    "class AgentEvaluator:\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    def evaluate_response_accuracy(self, prediction: str, expected: Dict) -> float:\n",
    "        \"\"\"Evaluate if response addresses the correct category\"\"\"\n",
    "        # Simplified accuracy check (in production, use more sophisticated NLP)\n",
    "        score = 0.0\n",
    "        \n",
    "        # Check category keywords\n",
    "        category_keywords = {\n",
    "            \"order_status\": [\"order\", \"shipment\", \"tracking\", \"delivery\"],\n",
    "            \"feedback\": [\"thank\", \"appreciate\", \"great\"],\n",
    "            \"ticket_management\": [\"ticket\", \"cancel\", \"close\"],\n",
    "        }\n",
    "        \n",
    "        expected_category = expected.get(\"category\", \"\")\n",
    "        keywords = category_keywords.get(expected_category, [])\n",
    "        \n",
    "        if any(keyword in prediction.lower() for keyword in keywords):\n",
    "            score += 0.5\n",
    "        \n",
    "        # Check sentiment alignment\n",
    "        sentiment = expected.get(\"sentiment\", \"\")\n",
    "        if sentiment == \"positive\" and any(word in prediction.lower() for word in [\"glad\", \"happy\", \"great\"]):\n",
    "            score += 0.25\n",
    "        elif sentiment == \"negative\" and any(word in prediction.lower() for word in [\"sorry\", \"apologize\", \"unfortunately\"]):\n",
    "            score += 0.25\n",
    "        \n",
    "        # Check priority handling\n",
    "        priority = expected.get(\"priority\", \"\")\n",
    "        if priority == \"urgent\" and any(word in prediction.lower() for word in [\"immediately\", \"right away\", \"asap\"]):\n",
    "            score += 0.25\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def evaluate_response_time(self, latency_ms: float) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate response time performance\"\"\"\n",
    "        # SLA targets for InboxOps\n",
    "        if latency_ms < 1000:\n",
    "            grade = \"excellent\"\n",
    "            score = 1.0\n",
    "        elif latency_ms < 3000:\n",
    "            grade = \"good\"\n",
    "            score = 0.8\n",
    "        elif latency_ms < 5000:\n",
    "            grade = \"acceptable\"\n",
    "            score = 0.6\n",
    "        else:\n",
    "            grade = \"poor\"\n",
    "            score = 0.3\n",
    "        \n",
    "        return {\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"grade\": grade,\n",
    "            \"score\": score,\n",
    "            \"meets_sla\": latency_ms < 5000\n",
    "        }\n",
    "    \n",
    "    def run_evaluation(self, agent: ChatAgent, test_cases: List[TestCase]) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive evaluation on test dataset\"\"\"\n",
    "        results = []\n",
    "        total_latency = 0\n",
    "        \n",
    "        print(\"\ud83e\uddea RUNNING AGENT EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            print(f\"\\n\ud83d\udcdd Test Case: {test_case.id}\")\n",
    "            print(f\"   Input: {test_case.input[:60]}...\")\n",
    "            \n",
    "            # Measure response time\n",
    "            start_time = time.time()\n",
    "            response = agent.run(messages=[{\"role\": \"user\", \"content\": test_case.input}])\n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # Evaluate accuracy\n",
    "            accuracy_score = self.evaluate_response_accuracy(\n",
    "                response.output,\n",
    "                test_case.expected_output\n",
    "            )\n",
    "            \n",
    "            # Evaluate response time\n",
    "            timing_result = self.evaluate_response_time(latency_ms)\n",
    "            \n",
    "            result = {\n",
    "                \"test_id\": test_case.id,\n",
    "                \"category\": test_case.category,\n",
    "                \"accuracy_score\": accuracy_score,\n",
    "                \"latency_ms\": latency_ms,\n",
    "                \"timing_grade\": timing_result[\"grade\"],\n",
    "                \"meets_sla\": timing_result[\"meets_sla\"],\n",
    "                \"response_preview\": response.output[:100] + \"...\" if len(response.output) > 100 else response.output\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            print(f\"   \u2705 Accuracy: {accuracy_score:.2f}\")\n",
    "            print(f\"   \u23f1\ufe0f  Latency: {latency_ms:.0f}ms ({timing_result['grade']})\")\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        avg_accuracy = sum(r[\"accuracy_score\"] for r in results) / len(results)\n",
    "        avg_latency = sum(r[\"latency_ms\"] for r in results) / len(results)\n",
    "        sla_compliance = sum(1 for r in results if r[\"meets_sla\"]) / len(results) * 100\n",
    "        \n",
    "        summary = {\n",
    "            \"total_tests\": len(results),\n",
    "            \"avg_accuracy\": avg_accuracy,\n",
    "            \"avg_latency_ms\": avg_latency,\n",
    "            \"sla_compliance_rate\": sla_compliance,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"detailed_results\": results\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = AgentEvaluator()\n",
    "\n",
    "# Run evaluation on our support agent\n",
    "evaluation_results = evaluator.run_evaluation(support_agent_with_tools, test_cases)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udcca EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Test Cases:      {evaluation_results['total_tests']}\")\n",
    "print(f\"Average Accuracy:      {evaluation_results['avg_accuracy']:.2%}\")\n",
    "print(f\"Average Latency:       {evaluation_results['avg_latency_ms']:.0f}ms\")\n",
    "print(f\"SLA Compliance:        {evaluation_results['sla_compliance_rate']:.1f}%\")\n",
    "print(f\"Timestamp:             {evaluation_results['timestamp']}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 DETAILED RESULTS\")\n",
    "print(\"-\"*60)\n",
    "for result in evaluation_results['detailed_results']:\n",
    "    status = \"\u2705\" if result['accuracy_score'] >= 0.7 else \"\u26a0\ufe0f\"\n",
    "    print(f\"{status} {result['test_id']}: Accuracy={result['accuracy_score']:.2f}, Latency={result['latency_ms']:.0f}ms\")\n",
    "\n",
    "# Calculate ROI metrics\n",
    "print(\"\\n\ud83d\udcb0 ROI ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "human_cost_per_email = 2.50  # $2.50 per email with human agent\n",
    "ai_cost_per_email = 0.02     # $0.02 per email with AI agent\n",
    "monthly_volume = 50000       # 50K emails/month\n",
    "\n",
    "monthly_human_cost = monthly_volume * human_cost_per_email\n",
    "monthly_ai_cost = monthly_volume * ai_cost_per_email\n",
    "monthly_savings = monthly_human_cost - monthly_ai_cost\n",
    "annual_savings = monthly_savings * 12\n",
    "\n",
    "print(f\"Monthly Email Volume:   {monthly_volume:,}\")\n",
    "print(f\"Human Agent Cost:       ${monthly_human_cost:,.2f}/month\")\n",
    "print(f\"AI Agent Cost:          ${monthly_ai_cost:,.2f}/month\")\n",
    "print(f\"Monthly Savings:        ${monthly_savings:,.2f}\")\n",
    "print(f\"Annual Savings:         ${annual_savings:,.2f}\")\n",
    "print(f\"Cost Reduction:         {(1 - ai_cost_per_email/human_cost_per_email)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\u2705 Evaluation complete! Use these metrics to:\")\n",
    "print(\"   \u2022 Track agent performance over time\")\n",
    "print(\"   \u2022 Identify areas for improvement\")\n",
    "print(\"   \u2022 Justify ROI to stakeholders\")\n",
    "print(\"   \u2022 Set SLAs and quality benchmarks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: LLM-as-Judge Evaluation\n",
    "\n",
    "Manual evaluation doesn't scale to hundreds of test cases. **LLM judges** automate quality assessment:\n",
    "\n",
    "- **Correctness** - Is the response factually accurate?\n",
    "- **Helpfulness** - Does it solve the customer's problem?\n",
    "- **Tone** - Is it professional and empathetic?\n",
    "- **Completeness** - Are all questions answered?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM Judge Implementation\n",
    "\n",
    "class LLMJudge:\n",
    "    \"\"\"Use GPT-4 to evaluate agent responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, judge_client):\n",
    "        self.judge = ChatAgent(\n",
    "            chat_client=judge_client,\n",
    "            name=\"QualityJudge\",\n",
    "            instructions=\"\"\"\n",
    "            You are an expert evaluator of customer support responses.\n",
    "            Rate responses on a scale of 1-5 for each criterion.\n",
    "            Provide a brief explanation for your rating.\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    async def evaluate_response(\n",
    "        self,\n",
    "        customer_message: str,\n",
    "        agent_response: str,\n",
    "        expected_answer: str = None\n",
    "    ) -> dict:\n",
    "        \"\"\"Evaluate a single response.\"\"\"\n",
    "        \n",
    "        eval_prompt = f\"\"\"\n",
    "        Evaluate this customer support response:\n",
    "        \n",
    "        Customer Question:\n",
    "        {customer_message}\n",
    "        \n",
    "        Agent Response:\n",
    "        {agent_response}\n",
    "        \n",
    "        {f'Expected Answer: {expected_answer}' if expected_answer else ''}\n",
    "        \n",
    "        Rate the response on these criteria (1-5 scale):\n",
    "        1. Correctness: Is the information accurate?\n",
    "        2. Helpfulness: Does it solve the problem?\n",
    "        3. Tone: Professional and empathetic?\n",
    "        4. Completeness: All questions answered?\n",
    "        \n",
    "        Respond in this JSON format:\n",
    "        {{\n",
    "            \"correctness\": 4,\n",
    "            \"helpfulness\": 5,\n",
    "            \"tone\": 5,\n",
    "            \"completeness\": 4,\n",
    "            \"overall\": 4.5,\n",
    "            \"explanation\": \"Brief explanation here\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        result = await self.judge.run(eval_prompt)\n",
    "        \n",
    "        # Parse JSON from response\n",
    "        import json\n",
    "        import re\n",
    "        \n",
    "        # Extract JSON from markdown code blocks if present\n",
    "        text = result.text\n",
    "        json_match = re.search(r'```(?:json)?\\s*({.*?})\\s*```', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "        else:\n",
    "            json_str = text\n",
    "        \n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            # Fallback if parsing fails\n",
    "            return {\n",
    "                \"error\": \"Failed to parse judge response\",\n",
    "                \"raw\": text[:200]\n",
    "            }\n",
    "\n",
    "# Create judge\n",
    "judge = LLMJudge(client)\n",
    "\n",
    "# Test evaluation\n",
    "print(\"Testing LLM judge...\\n\")\n",
    "\n",
    "test_customer_msg = \"My order #12345 hasn't arrived and it's been 2 weeks.\"\n",
    "test_agent_response = \"\"\"\n",
    "I understand your frustration. Let me check order #12345 for you.\n",
    "I see it shipped on May 1st via USPS. The expected delivery was May 8th.\n",
    "Since it's now May 15th and you haven't received it, I'll file a missing package claim\n",
    "and issue a full refund today. You should see it in 3-5 business days.\n",
    "Would you like a replacement order shipped with expedited shipping at no charge?\n",
    "\"\"\"\n",
    "\n",
    "evaluation = await judge.evaluate_response(\n",
    "    test_customer_msg,\n",
    "    test_agent_response\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(json.dumps(evaluation, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batch Evaluation\n",
    "\n",
    "async def run_batch_evaluation(agent, test_cases: list, judge: LLMJudge):\n",
    "    \"\"\"Evaluate agent on multiple test cases.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating agent on {len(test_cases)} test cases...\\n\")\n",
    "    \n",
    "    for i, case in enumerate(test_cases):\n",
    "        print(f\"[{i+1}/{len(test_cases)}] Testing: {case['input'][:50]}...\")\n",
    "        \n",
    "        # Get agent response\n",
    "        try:\n",
    "            response = await agent.run(case['input'])\n",
    "            agent_text = response.text\n",
    "        except Exception as e:\n",
    "            agent_text = f\"ERROR: {str(e)}\"\n",
    "        \n",
    "        # Evaluate with LLM judge\n",
    "        eval_result = await judge.evaluate_response(\n",
    "            case['input'],\n",
    "            agent_text,\n",
    "            case.get('expected')\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'test_case': case,\n",
    "            'agent_response': agent_text,\n",
    "            'evaluation': eval_result\n",
    "        })\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    if results:\n",
    "        avg_correctness = sum(r['evaluation'].get('correctness', 0) for r in results) / len(results)\n",
    "        avg_helpfulness = sum(r['evaluation'].get('helpfulness', 0) for r in results) / len(results)\n",
    "        avg_tone = sum(r['evaluation'].get('tone', 0) for r in results) / len(results)\n",
    "        avg_completeness = sum(r['evaluation'].get('completeness', 0) for r in results) / len(results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"\ud83d\udcca INBOXOPS QUALITY REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total Test Cases: {len(results)}\")\n",
    "        print(f\"\\nAverage Scores (out of 5):\")\n",
    "        print(f\"  Correctness:  {avg_correctness:.2f} {'\u2b50' * int(avg_correctness)}\")\n",
    "        print(f\"  Helpfulness:  {avg_helpfulness:.2f} {'\u2b50' * int(avg_helpfulness)}\")\n",
    "        print(f\"  Tone:         {avg_tone:.2f} {'\u2b50' * int(avg_tone)}\")\n",
    "        print(f\"  Completeness: {avg_completeness:.2f} {'\u2b50' * int(avg_completeness)}\")\n",
    "        print(f\"\\nOverall Quality: {(avg_correctness + avg_helpfulness + avg_tone + avg_completeness) / 4:.2f}/5.0\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define test cases\n",
    "inboxops_test_cases = [\n",
    "    {\n",
    "        'input': 'How do I track my order #98765?',\n",
    "        'expected': 'Provide tracking link and estimated delivery date'\n",
    "    },\n",
    "    {\n",
    "        'input': 'I want to return a defective product.',\n",
    "        'expected': 'Explain return process, provide label, and timeline'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Your website is down!',\n",
    "        'expected': 'Acknowledge issue, provide status, offer alternatives'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "# eval_results = await run_batch_evaluation(support_agent, inboxops_test_cases, judge)\n",
    "\n",
    "print(\"\\n\u2705 Batch evaluation framework ready!\")\n",
    "print(\"Uncomment the line above to run full evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: A/B Testing Framework\n",
    "\n",
    "Compare different agent configurations to optimize for cost, speed, and quality:\n",
    "\n",
    "- **Model comparison** - GPT-4 vs GPT-4o vs GPT-3.5\n",
    "- **Prompt engineering** - Test different instructions\n",
    "- **Temperature tuning** - Creativity vs consistency\n",
    "- **Tool configurations** - Which tools improve quality?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A/B Testing Harness\n",
    "\n",
    "class ABTestingHarness:\n",
    "    \"\"\"Compare two agent configurations.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_a, agent_b, test_cases, judge):\n",
    "        self.agent_a = agent_a\n",
    "        self.agent_b = agent_b\n",
    "        self.test_cases = test_cases\n",
    "        self.judge = judge\n",
    "    \n",
    "    async def run_comparison(self):\n",
    "        \"\"\"Run both agents on all test cases and compare.\"\"\"\n",
    "        results_a = []\n",
    "        results_b = []\n",
    "        \n",
    "        print(f\"Running A/B test on {len(self.test_cases)} cases...\\n\")\n",
    "        \n",
    "        for i, case in enumerate(self.test_cases):\n",
    "            print(f\"[{i+1}/{len(self.test_cases)}] Testing: {case['input'][:40]}...\")\n",
    "            \n",
    "            # Test Agent A\n",
    "            try:\n",
    "                result_a = await self.agent_a.run(case['input'])\n",
    "                eval_a = await self.judge.evaluate_response(\n",
    "                    case['input'],\n",
    "                    result_a.text\n",
    "                )\n",
    "                results_a.append({\n",
    "                    'response': result_a.text,\n",
    "                    'tokens': getattr(result_a.usage, 'total_tokens', 0) if hasattr(result_a, 'usage') else 0,\n",
    "                    'evaluation': eval_a\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results_a.append({'error': str(e)})\n",
    "            \n",
    "            # Test Agent B\n",
    "            try:\n",
    "                result_b = await self.agent_b.run(case['input'])\n",
    "                eval_b = await self.judge.evaluate_response(\n",
    "                    case['input'],\n",
    "                    result_b.text\n",
    "                )\n",
    "                results_b.append({\n",
    "                    'response': result_b.text,\n",
    "                    'tokens': getattr(result_b.usage, 'total_tokens', 0) if hasattr(result_b, 'usage') else 0,\n",
    "                    'evaluation': eval_b\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results_b.append({'error': str(e)})\n",
    "        \n",
    "        return self._generate_report(results_a, results_b)\n",
    "    \n",
    "    def _generate_report(self, results_a, results_b):\n",
    "        \"\"\"Generate comparison report.\"\"\"\n",
    "        \n",
    "        # Calculate metrics\n",
    "        def calc_avg(results, metric):\n",
    "            values = [r['evaluation'].get(metric, 0) for r in results if 'evaluation' in r]\n",
    "            return sum(values) / len(values) if values else 0\n",
    "        \n",
    "        def calc_tokens(results):\n",
    "            values = [r.get('tokens', 0) for r in results if 'tokens' in r]\n",
    "            return sum(values)\n",
    "        \n",
    "        report = {\n",
    "            'agent_a': {\n",
    "                'correctness': calc_avg(results_a, 'correctness'),\n",
    "                'helpfulness': calc_avg(results_a, 'helpfulness'),\n",
    "                'tone': calc_avg(results_a, 'tone'),\n",
    "                'completeness': calc_avg(results_a, 'completeness'),\n",
    "                'total_tokens': calc_tokens(results_a)\n",
    "            },\n",
    "            'agent_b': {\n",
    "                'correctness': calc_avg(results_b, 'correctness'),\n",
    "                'helpfulness': calc_avg(results_b, 'helpfulness'),\n",
    "                'tone': calc_avg(results_b, 'tone'),\n",
    "                'completeness': calc_avg(results_b, 'completeness'),\n",
    "                'total_tokens': calc_tokens(results_b)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Calculate improvements\n",
    "        report['comparison'] = {\n",
    "            'quality_delta': (\n",
    "                (report['agent_b']['correctness'] + report['agent_b']['helpfulness']) / 2 -\n",
    "                (report['agent_a']['correctness'] + report['agent_a']['helpfulness']) / 2\n",
    "            ),\n",
    "            'token_savings_pct': (\n",
    "                (report['agent_a']['total_tokens'] - report['agent_b']['total_tokens']) /\n",
    "                report['agent_a']['total_tokens'] * 100\n",
    "                if report['agent_a']['total_tokens'] > 0 else 0\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self._print_report(report)\n",
    "        return report\n",
    "    \n",
    "    def _print_report(self, report):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"\ud83d\udcca A/B TEST RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nAgent A (e.g., GPT-4):\")\n",
    "        print(f\"  Correctness:  {report['agent_a']['correctness']:.2f}/5\")\n",
    "        print(f\"  Helpfulness:  {report['agent_a']['helpfulness']:.2f}/5\")\n",
    "        print(f\"  Total Tokens: {report['agent_a']['total_tokens']:,}\")\n",
    "        \n",
    "        print(f\"\\nAgent B (e.g., GPT-4o):\")\n",
    "        print(f\"  Correctness:  {report['agent_b']['correctness']:.2f}/5\")\n",
    "        print(f\"  Helpfulness:  {report['agent_b']['helpfulness']:.2f}/5\")\n",
    "        print(f\"  Total Tokens: {report['agent_b']['total_tokens']:,}\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcc8 Comparison:\")\n",
    "        quality_delta = report['comparison']['quality_delta']\n",
    "        token_savings = report['comparison']['token_savings_pct']\n",
    "        \n",
    "        print(f\"  Quality Delta: {quality_delta:+.2f} {'\u2705 Better' if quality_delta > 0 else '\u26a0\ufe0f Worse'}\")\n",
    "        print(f\"  Token Savings: {token_savings:+.1f}% {'\ud83d\udcb0 Cheaper' if token_savings > 0 else '\ud83d\udcb8 More expensive'}\")\n",
    "        \n",
    "        if quality_delta > 0 and token_savings > 0:\n",
    "            print(f\"\\n\u2705 WINNER: Agent B (Better quality AND cheaper!)\")\n",
    "        elif abs(quality_delta) < 0.2 and token_savings > 0:\n",
    "            print(f\"\\n\u2705 WINNER: Agent B (Similar quality, much cheaper)\")\n",
    "        else:\n",
    "            print(f\"\\n\u2696\ufe0f  Trade-off: Choose based on priorities\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "\n",
    "# Example: Compare two configurations\n",
    "# ab_test = ABTestingHarness(\n",
    "#     agent_a=support_agent_gpt4,\n",
    "#     agent_b=support_agent_gpt4o,\n",
    "#     test_cases=inboxops_test_cases,\n",
    "#     judge=judge\n",
    "# )\n",
    "# comparison = await ab_test.run_comparison()\n",
    "\n",
    "print(\"\\n\u2705 A/B testing framework ready!\")\n",
    "print(\"Use this to optimize cost vs quality tradeoffs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfaf V4 Evaluation Summary\n",
    "\n",
    "InboxOps now has enterprise-grade quality assurance:\n",
    "\n",
    "| Technique | Purpose | When to Use |\n",
    "|-----------|---------|-------------|\n",
    "| **Test Cases** | Regression testing | Every code change |\n",
    "| **LLM Judge** | Quality assessment at scale | Weekly quality audits |\n",
    "| **A/B Testing** | Optimize cost/quality tradeoffs | Before prod deployments |\n",
    "\n",
    "### Key Metrics to Track\n",
    "\n",
    "1. **Quality Metrics**\n",
    "   - Correctness: Are responses accurate?\n",
    "   - Helpfulness: Do they solve problems?\n",
    "   - Tone: Professional and empathetic?\n",
    "   - Completeness: All questions answered?\n",
    "\n",
    "2. **Cost Metrics**\n",
    "   - Tokens per conversation\n",
    "   - Model API costs\n",
    "   - Tool execution costs\n",
    "\n",
    "3. **Performance Metrics**\n",
    "   - Response time (p50, p95, p99)\n",
    "   - Tool call latency\n",
    "   - Concurrent load handling\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Set up continuous evaluation pipeline\n",
    "- Define quality thresholds (e.g., >4.0/5 on all metrics)\n",
    "- Implement automated alerts for quality regressions\n",
    "- Run A/B tests before deploying model changes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V5 \u2014 Durable Agents: Serverless Deployment at Scale\n",
    "\n",
    "## The Problem: Infrastructure Management\n",
    "\n",
    "InboxOps faces production deployment challenges:\n",
    "\n",
    "- **Auto-scaling needed** - Black Friday traffic spikes 100x\n",
    "- **Long-running workflows** - Human approvals can take days/weeks\n",
    "- **State persistence** - Conversations must survive server restarts\n",
    "- **Cost efficiency** - Pay-per-execution, not per-hour\n",
    "\n",
    "Traditional deployments can't handle:\n",
    "- \u274c 30-second HTTP timeouts (approval workflows need days)\n",
    "- \u274c Lost conversation state on server restart\n",
    "- \u274c Manual scaling during traffic spikes\n",
    "- \u274c 24/7 compute costs while waiting for human input\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: Durable Task Extension + Azure Functions\n",
    "\n",
    "The **Durable Task extension** for Agent Framework provides:\n",
    "\n",
    "1. **Serverless Hosting** - Auto-scales from 0 to 10,000+ instances\n",
    "2. **Stateful Threads** - Conversation history persisted in Cosmos DB\n",
    "3. **Deterministic Orchestrations** - Multi-agent workflows that survive failures\n",
    "4. **Zero-Cost Waiting** - No compute charges while waiting for human input\n",
    "5. **Built-in Observability** - Durable Task Scheduler dashboard\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture: How It Works\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                     Azure Functions                         \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n",
    "\u2502  \u2502   HTTP       \u2502      \u2502  Durable     \u2502                   \u2502\n",
    "\u2502  \u2502   Trigger    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Agent       \u2502                   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n",
    "\u2502                              \u2502                              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                               \u2502\n",
    "                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                \u2502              \u2502              \u2502\n",
    "                \u25bc              \u25bc              \u25bc\n",
    "          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "          \u2502 Cosmos  \u2502    \u2502  Azure  \u2502   \u2502 Durable  \u2502\n",
    "          \u2502   DB    \u2502    \u2502 Storage \u2502   \u2502   Task   \u2502\n",
    "          \u2502 (state) \u2502    \u2502 (queue) \u2502   \u2502Scheduler \u2502\n",
    "          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installation & Setup\n",
    "\n",
    "# Install the durable task extension\n",
    "# !pip install agent-framework-azurefunctions --pre\n",
    "# !pip install azure-identity\n",
    "\n",
    "# Note: Full durable agents require Azure Functions deployment.\n",
    "# This example shows the programming model for reference.\n",
    "\n",
    "print(\"\"\"\\n",
    "\ud83d\udce6 Durable Agent Deployment Stack:\n",
    "\n",
    "1. agent-framework-azurefunctions  # Core extension\n",
    "2. Azure Functions (Flex Consumption)  # Serverless hosting\n",
    "3. Cosmos DB  # Persistent state storage\n",
    "4. Azure Storage  # Queue & orchestration\n",
    "5. Durable Task Scheduler  # Observability dashboard\n",
    "\n",
    "\ud83d\udcb0 Cost Model:\n",
    "- Only pay for execution time\n",
    "- $0.00 while waiting for human approval\n",
    "- Auto-scale from 0 to thousands of instances\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: Durable Agent Definition\n",
    "\n",
    "# This is the programming model (requires Azure Functions to run)\n",
    "\n",
    "sample_code = '''\n",
    "# function_app.py\n",
    "from agent_framework_azurefunctions import AIAgentApp\n",
    "from agent_framework import ChatAgent\n",
    "from agent_framework_azure_ai import AzureAIAgentClient\n",
    "\n",
    "app = AIAgentApp()\n",
    "\n",
    "@app.agent(name=\"durable-support-agent\")\n",
    "def create_support_agent():\n",
    "    \"\"\"Create a durable agent with persistent state.\"\"\"\n",
    "    client = AzureAIAgentClient.from_connection_string(\n",
    "        connection_string=os.environ[\"AZURE_AI_CONNECTION_STRING\"]\n",
    "    )\n",
    "    \n",
    "    return ChatAgent(\n",
    "        chat_client=client,\n",
    "        name=\"DurableInboxOps\",\n",
    "        instructions=\"You are InboxOps customer support.\",\n",
    "        tools=[create_ticket, lookup_customer, send_email]\n",
    "    )\n",
    "\n",
    "# Deploy with:\n",
    "# func azure functionapp publish <your-function-app>\n",
    "\n",
    "# The agent automatically gets:\n",
    "# 1. HTTP endpoint: https://<your-app>.azurewebsites.net/api/agents/durable-support-agent\n",
    "# 2. Persistent state in Cosmos DB\n",
    "# 3. Auto-scaling (0 to N instances)\n",
    "# 4. Built-in observability dashboard\n",
    "'''\n",
    "\n",
    "print(sample_code)\n",
    "\n",
    "print(\"\\n\u2705 Agent Features After Deployment:\")\n",
    "print(\"\"\"\n",
    "- Stateful Threads: Conversation history survives restarts\n",
    "- HTTP API: RESTful endpoints for client integration\n",
    "- Auto-Scale: 0\u21921000+ instances based on load\n",
    "- Observability: Real-time dashboard of all conversations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deterministic Multi-Agent Orchestration\n",
    "\n",
    "# Durable orchestrations coordinate multiple agents reliably\n",
    "\n",
    "orchestration_code = '''\n",
    "@app.orchestration(\"escalation-workflow\")\n",
    "async def escalation_orchestration(context, input_data):\n",
    "    \"\"\"Durable workflow for escalating complex support tickets.\n",
    "    \n",
    "    This orchestration:\n",
    "    - Survives server restarts\n",
    "    - Waits indefinitely for human input (days/weeks)\n",
    "    - No compute cost while waiting\n",
    "    - Automatically resumes when event arrives\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Initial support agent handles the request\n",
    "    support_agent = await context.get_agent(\"support-agent\")\n",
    "    initial_response = await support_agent.run(input_data[\"message\"])\n",
    "    \n",
    "    # Step 2: Check if escalation is needed\n",
    "    if \"complex\" in initial_response.text.lower():\n",
    "        \n",
    "        # Step 3: Wait for human approval (CAN TAKE DAYS!)\n",
    "        # Orchestration is serialized to Cosmos DB here\n",
    "        # Compute resources are released (cost = $0)\n",
    "        approval = await context.wait_for_external_event(\"approval_decision\")\n",
    "        \n",
    "        # Step 4: When approval arrives, orchestration resumes\n",
    "        if approval[\"approved\"]:\n",
    "            specialist = await context.get_agent(\"specialist-agent\")\n",
    "            final_response = await specialist.run(input_data[\"message\"])\n",
    "            return {\n",
    "                \"status\": \"escalated\",\n",
    "                \"response\": final_response.text\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"denied\",\n",
    "                \"reason\": approval[\"reason\"]\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"resolved\",\n",
    "        \"response\": initial_response.text\n",
    "    }\n",
    "\n",
    "# To send approval event from external system:\n",
    "# POST https://<your-app>.azurewebsites.net/runtime/webhooks/durabletask/instances/{instanceId}/raiseEvent/approval_decision\n",
    "# Body: {\"approved\": true, \"reason\": \"Authorized by manager\"}\n",
    "'''\n",
    "\n",
    "print(orchestration_code)\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Key Benefits:\")\n",
    "print(\"\"\"\n",
    "1. Checkpointing: State saved after each step\n",
    "2. Fault Tolerance: Survives failures, automatically retries\n",
    "3. Event-Driven: Wait for external events (webhooks, human input)\n",
    "4. Cost-Efficient: No charges while waiting\n",
    "5. Observable: Full execution history in dashboard\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallel Orchestration Pattern\n",
    "\n",
    "parallel_code = '''\n",
    "@app.orchestration(\"quality-assessment\")\n",
    "async def quality_assessment_orchestration(context, email_content):\n",
    "    \"\"\"Run multiple specialist agents in parallel.\"\"\"\n",
    "    \n",
    "    # Get all specialist agents\n",
    "    sentiment_agent = await context.get_agent(\"sentiment-analyzer\")\n",
    "    priority_agent = await context.get_agent(\"priority-classifier\")\n",
    "    spam_agent = await context.get_agent(\"spam-detector\")\n",
    "    \n",
    "    # Execute all agents concurrently\n",
    "    tasks = [\n",
    "        sentiment_agent.run(email_content),\n",
    "        priority_agent.run(email_content),\n",
    "        spam_agent.run(email_content)\n",
    "    ]\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Aggregate results\n",
    "    return {\n",
    "        \"sentiment\": results[0].text,\n",
    "        \"priority\": results[1].text,\n",
    "        \"is_spam\": results[2].text\n",
    "    }\n",
    "    \n",
    "# Parallel execution with automatic checkpointing:\n",
    "# - If one agent fails, others continue\n",
    "# - Completed work is not repeated after restart\n",
    "# - Full observability of each parallel branch\n",
    "'''\n",
    "\n",
    "print(parallel_code)\n",
    "print(\"\\n\u2705 Parallel Pattern Benefits:\")\n",
    "print(\"- Reduced latency (concurrent execution)\")\n",
    "print(\"- Fault isolation (one failure doesn't affect others)\")\n",
    "print(\"- Automatic retry of failed branches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Observability Dashboard\n",
    "\n",
    "print(\"\"\"\n",
    "\ud83d\udcca DURABLE TASK SCHEDULER DASHBOARD\n",
    "\n",
    "The built-in dashboard provides:\n",
    "\n",
    "1. Agent Session Insights:\n",
    "   - Complete chat history for each conversation\n",
    "   - Tool calls and their results\n",
    "   - Token usage and costs\n",
    "   - User messages and agent responses\n",
    "\n",
    "2. Orchestration Insights:\n",
    "   - Visual execution flow diagram\n",
    "   - Step-by-step execution timeline\n",
    "   - Parallel branch visualization\n",
    "   - Waiting states (pending approvals)\n",
    "\n",
    "3. Performance Metrics:\n",
    "   - Agent response times\n",
    "   - Orchestration duration\n",
    "   - Active vs queued instances\n",
    "   - Success/failure rates\n",
    "\n",
    "4. Debugging Capabilities:\n",
    "   - Structured agent outputs\n",
    "   - Tool invocation traces\n",
    "   - External event monitoring\n",
    "   - Replay and time-travel debugging\n",
    "\n",
    "Access: https://<your-app>.azurewebsites.net/durabletask\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2705 V5 Complete: InboxOps can now deploy at enterprise scale!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Deploy to Azure Functions\")\n",
    "print(\"2. Configure Cosmos DB for state\")\n",
    "print(\"3. Enable Durable Task Scheduler dashboard\")\n",
    "print(\"4. Monitor production traffic in real-time\")\n",
    "print(\"\\nLearn more: https://learn.microsoft.com/agent-framework/durable-agents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}